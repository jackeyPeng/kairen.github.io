<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>KaiRen&#39;s Blog</title>
  <icon>https://www.gravatar.com/avatar/9156bf89edf2ea0f74c01bae4a478fff</icon>
  <subtitle>KaiRen&#39;s Blog</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://kairen.github.io/"/>
  <updated>2017-12-14T02:08:41.000Z</updated>
  <id>https://kairen.github.io/</id>
  
  <author>
    <name>Kyle Bai</name>
    <email>kyle.b@inwinstack.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Deploy OpenStack on Kubernetes using OpenStack-helm</title>
    <link href="https://kairen.github.io/2017/11/29/openstack/openstack-helm/"/>
    <id>https://kairen.github.io/2017/11/29/openstack/openstack-helm/</id>
    <published>2017-11-29T08:23:01.000Z</published>
    <updated>2017-12-14T02:08:41.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/openstack/openstack-helm" target="_blank" rel="noopener">OpenStack Helm</a> 是一個提供部署建置的專案，其目的是為了推動 OpenStack 生產環境的解決方案，而這種部署方式採用容器化方式，並執行於 Kubernetes 系統上來提供 OpenStack 服務的管理與排程等使用。</p><p><img src="https://i.imgur.com/8sMjowM.png" alt=""></p><a id="more"></a><p>而本篇文章將說明如何建置多節點的 OpenStack Helm 環境來進行功能驗證。</p><h2 id="節點與安裝版本"><a href="#節點與安裝版本" class="headerlink" title="節點與安裝版本"></a>節點與安裝版本</h2><p>以下為各節點的硬體資訊。</p><table><thead><tr><th>IP Address</th><th>Role</th><th>CPU</th><th>Memory</th></tr></thead><tbody><tr><td>172.22.132.10</td><td>vip</td><td>-</td><td>-</td></tr><tr><td>172.22.132.101</td><td>master1</td><td>4</td><td>16G</td></tr><tr><td>172.22.132.22</td><td>node1</td><td>4</td><td>16G</td></tr><tr><td>172.22.132.24</td><td>node2</td><td>4</td><td>16G</td></tr><tr><td>172.22.132.28</td><td>node3</td><td>4</td><td>16G</td></tr></tbody></table><p>使用 Kernel、作業系統與軟體版本：</p><table><thead><tr><th></th><th>資訊描述</th></tr></thead><tbody><tr><td>作業系統版本</td><td>16.04.3 LTS (Xenial Xerus)</td></tr><tr><td>Kernel 版本</td><td>4.4.0-101-generic</td></tr><tr><td>Kubernetes</td><td>v1.8.4</td></tr><tr><td>Docker</td><td>Docker 17.09.0-ce</td></tr><tr><td>Calico</td><td>v2.6.2</td></tr><tr><td>Etcd</td><td>v3.2.9</td></tr><tr><td>Ceph</td><td>v10.2.10</td></tr><tr><td>Helm</td><td>v2.7.0</td></tr></tbody></table><h2 id="Kubernetes-叢集"><a href="#Kubernetes-叢集" class="headerlink" title="Kubernetes 叢集"></a>Kubernetes 叢集</h2><p>本節說明如何建立 Kubernetes Cluster，這邊採用 <a href="https://github.com/kairen/kube-ansible" target="_blank" rel="noopener">kube-ansible</a> 工具來建立。</p><h3 id="初始化與設定基本需求"><a href="#初始化與設定基本需求" class="headerlink" title="初始化與設定基本需求"></a>初始化與設定基本需求</h3><p>安裝前需要確認以下幾個項目：</p><ul><li>所有節點的網路之間可以互相溝通。</li><li><code>部署節點</code>對其他節點不需要 SSH 密碼即可登入。</li><li>所有節點都擁有 Sudoer 權限，並且不需要輸入密碼。</li><li>所有節點需要安裝<code>Python</code>。</li><li>所有節點需要設定<code>/etc/host</code>解析到所有主機。</li><li><code>部署節點</code>需要安裝 <strong>Ansible &gt;= 2.4.0</strong>。</li></ul><pre><code class="shell"># Ubuntu install$ sudo apt-get install -y software-properties-common$ sudo apt-add-repository -y ppa:ansible/ansible$ sudo apt-get update &amp;&amp; sudo apt-get install -y ansible git make# CentOS install$ sudo yum install -y epel-release$ sudo yum -y install ansible cowsay</code></pre><h3 id="安裝與設定-Kube-ansible"><a href="#安裝與設定-Kube-ansible" class="headerlink" title="安裝與設定 Kube-ansible"></a>安裝與設定 Kube-ansible</h3><p>首先取得最新穩定版本的 Kubernetes Ansible:</p><pre><code class="shell">$ git clone https://github.com/kairen/kube-ansible.git$ cd kube-ansible</code></pre><p>然後新增<code>inventory</code>檔案來描述要部屬的主機角色:</p><pre><code>[etcds]172.22.132.101 ansible_user=ubuntu[masters]172.22.132.101 ansible_user=ubuntu[nodes]172.22.132.22 ansible_user=ubuntu172.22.132.24 ansible_user=ubuntu172.22.132.28 ansible_user=ubuntu[kube-cluster:children]mastersnodes[kube-addon:children]masters</code></pre><p>接著編輯<code>group_vars/all.yml</code>檔案來添加與修改以下內容：</p><pre><code class="yaml"># Kubenrtes version, only support 1.8.0+.kube_version: 1.8.4# CNI plugin# Support: flannel, calico, canal, weave or router.network: calicopod_network_cidr: 10.244.0.0/16# CNI opts: flannel(--iface=enp0s8), calico(interface=enp0s8), canal(enp0s8).cni_iface: &quot;&quot;# Kubernetes cluster network.cluster_subnet: 10.96.0kubernetes_service_ip: &quot;{{ cluster_subnet }}.1&quot;service_ip_range: &quot;{{ cluster_subnet }}.0/12&quot;service_node_port_range: 30000-32767api_secure_port: 5443# Highly Available configuration.haproxy: truekeepalived: true # set `lb_vip_address` as keepalived vip, if this enable.keepalived_vip_interface: &quot;{{ ansible_default_ipv4.interface }}&quot;lb_vip_address: 172.22.132.10lb_secure_port: 6443lb_api_url: &quot;https://{{ lb_vip_address }}:{{ lb_secure_port }}&quot;etcd_iface: &quot;&quot;insecure_registrys:- &quot;172.22.132.253:5000&quot; # 有需要的話ceph_cluster: true</code></pre><blockquote><ul><li>這邊<code>insecure_registrys</code>為 deploy 節點的 Docker registry ip 與 port。</li><li>Extra addons 部分針對需求開啟，預設不會開啟。</li><li>若想把 Etcd, VIP 與 Network plugin 綁定在指定網路的話，請修改<code>etcd_iface</code>, <code>keepalived_vip_interface</code> 與 <code>cni_iface</code>。其中<code>cni_iface</code>需要針對不同 Plugin 來改變。</li><li>若想要修改部署版本的 Packages 的話，請編輯<code>roles/commons/packages/defaults/main.yml</code>來修改版本。</li></ul></blockquote><p>接著由於 OpenStack-helm 使用的 Kubernetes Controller Manager 不同，因此要修改<code>roles/commons/container-images/defaults/main.yml</code>的 Image 來源如下：</p><pre><code class="yaml">...  manager:  name: kube-controller-manager  repos: kairen/  tag: &quot;v{{ kube_version }}&quot;...</code></pre><p>完後成修改 storage roles 設定版本並進行安裝。</p><p>首先編輯<code>roles/storage/ceph/defaults/main.yml</code>修改版本為以下：</p><pre><code class="yaml">ceph_version: jewel</code></pre><p>接著編輯<code>roles/storage/ceph/tasks/main.yml</code>修改成以下內容：</p><pre><code class="yaml">---- name: Install Ceph dependency packages  include_tasks: install-ceph.yml# - name: Create and copy generator config file#   include_tasks: gen-config.yml#   delegate_to: &quot;{{ groups['masters'][0] }}&quot;#   run_once: true## - name: Deploy Ceph components on Kubernetes#   include_tasks: ceph-on-k8s.yml#   delegate_to: &quot;{{ groups['masters'][0] }}&quot;#   run_once: true# - name: Label all storage nodes#   shell: &quot;kubectl label nodes node-type=storage&quot;#   delegate_to: &quot;{{ groups['masters'][0] }}&quot;#   run_once: true#   ignore_errors: true</code></pre><h3 id="部屬-Kubernetes-叢集"><a href="#部屬-Kubernetes-叢集" class="headerlink" title="部屬 Kubernetes 叢集"></a>部屬 Kubernetes 叢集</h3><p>確認<code>group_vars/all.yml</code>與其他設定都完成後，就透過 ansible ping 來檢查叢集狀態：</p><pre><code class="shell">$ ansible -i inventory all -m ping...172.22.132.101 | SUCCESS =&gt; {    &quot;changed&quot;: false,    &quot;failed&quot;: false,    &quot;ping&quot;: &quot;pong&quot;}...</code></pre><p>接著就可以透過以下指令進行部署叢集：</p><pre><code class="shell">$ ansible-playbook cluster.yml...TASK [cni : Apply calico network daemonset] *********************************************************************************************************************************changed: [172.22.132.101 -&gt; 172.22.132.101]PLAY RECAP ******************************************************************************************************************************************************************172.22.132.101             : ok=155  changed=58   unreachable=0    failed=0172.22.132.22              : ok=117  changed=28   unreachable=0    failed=0172.22.132.24              : ok=50   changed=18   unreachable=0    failed=0172.22.132.28              : ok=51   changed=19   unreachable=0    failed=0</code></pre><p>完成後，進入<code>master</code>節點執行以下指令確認叢集：</p><pre><code class="shell">$ kubectl get nodeNAME           STATUS    ROLES     AGE       VERSIONkube-master1   Ready     master    1h        v1.8.4kube-node1     Ready     &lt;none&gt;    1h        v1.8.4kube-node2     Ready     &lt;none&gt;    1h        v1.8.4kube-node3     Ready     &lt;none&gt;    1h        v1.8.4$ kubectl -n kube-system get poNAME                                       READY     STATUS    RESTARTS   AGEcalico-node-js6qp                          2/2       Running   2          1hcalico-node-kx9xn                          2/2       Running   2          1hcalico-node-lxrjl                          2/2       Running   2          1hcalico-node-vwn5f                          2/2       Running   2          1hcalico-policy-controller-d549764f6-9kn9l   1/1       Running   1          1hhaproxy-kube-master1                       1/1       Running   1          1hkeepalived-kube-master1                    1/1       Running   1          1hkube-apiserver-kube-master1                1/1       Running   1          1hkube-controller-manager-kube-master1       1/1       Running   1          1hkube-dns-7bd4879dc9-kxmx6                  3/3       Running   3          1hkube-proxy-7tqkm                           1/1       Running   1          1hkube-proxy-glzmm                           1/1       Running   1          1hkube-proxy-krqxs                           1/1       Running   1          1hkube-proxy-x9zdb                           1/1       Running   1          1hkube-scheduler-kube-master1                1/1       Running   1          1h</code></pre><p>檢查 kube-dns 是否連 host 都能夠解析:</p><pre><code class="shell">$ nslookup kubernetesServer:        10.96.0.10Address:    10.96.0.10#53Non-authoritative answer:Name:    kubernetes.default.svc.cluster.localAddress: 10.96.0.1</code></pre><p>接著安裝 Ceph 套件：</p><pre><code class="sh">$ ansible-playbook storage.yml</code></pre><h2 id="OpenStack-helm-叢集"><a href="#OpenStack-helm-叢集" class="headerlink" title="OpenStack-helm 叢集"></a>OpenStack-helm 叢集</h2><p>本節說明如何建立 OpenStack on Kubernetes 使用 Helm，部署是使用 <a href="https://github.com/openstack/openstack-helm" target="_blank" rel="noopener">openstack-helm</a>。過程將透過 OpenStack-helm 來在 Kubernetes 建置 OpenStack 叢集。以下所有操作都在<code>kube-master1</code>上進行。</p><h3 id="Helm-init"><a href="#Helm-init" class="headerlink" title="Helm init"></a>Helm init</h3><p>在開始前需要先將 Helm 進行初始化，以提供後續使用，然而這邊由於使用到 RBAC 的關係，因此需建立一個 Service account 來提供給 Helm 使用：</p><pre><code class="shell">$ kubectl -n kube-system create sa tiller$ kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller$ helm init --service-account tiller</code></pre><blockquote><p>由於 <code>kube-ansible</code> 本身包含 Helm 工具, 因此不需要自己安裝，只需要依據上面指令進行 init 即可。</p></blockquote><p>新增一個檔案<code>openrc</code>來提供環境變數：</p><pre><code class="shell">export HELM_HOST=$(kubectl describe svc/tiller-deploy -n kube-system | awk &#39;/Endpoints/{print $2}&#39;)export OSD_CLUSTER_NETWORK=172.22.132.0/24export OSD_PUBLIC_NETWORK=172.22.132.0/24export WORK_DIR=localexport CEPH_RGW_KEYSTONE_ENABLED=true</code></pre><blockquote><ul><li><code>OSD_CLUSTER_NETWORK</code>與<code>OSD_PUBLIC_NETWORK</code>都是使用實體機器網路，這邊 daemonset 會使用 hostNetwork。</li><li><code>CEPH_RGW_KEYSTONE_ENABLED</code> 在 Kubernetes 版本有點不穩，可依需求關閉。</li></ul></blockquote><p>完成後，透過 source 指令引入:</p><pre><code class="shell">$ source openrc$ helm versionClient: &amp;version.Version{SemVer:&quot;v2.7.0&quot;, GitCommit:&quot;08c1144f5eb3e3b636d9775617287cc26e53dba4&quot;, GitTreeState:&quot;clean&quot;}Server: &amp;version.Version{SemVer:&quot;v2.7.0&quot;, GitCommit:&quot;08c1144f5eb3e3b636d9775617287cc26e53dba4&quot;, GitTreeState:&quot;clean&quot;}</code></pre><h3 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h3><p>首先透過 Kubernetes label 來標示每個節點的角色：</p><pre><code class="shell">kubectl label nodes openstack-control-plane=enabled --allkubectl label nodes ceph-mon=enabled --allkubectl label nodes ceph-osd=enabled --allkubectl label nodes ceph-mds=enabled --allkubectl label nodes ceph-rgw=enabled --allkubectl label nodes openvswitch=enabled --allkubectl label nodes openstack-compute-node=enabled --all</code></pre><blockquote><p>這邊為了避免過度的節點污染，因此不讓 masters 充當任何角色：</p><pre><code class="shell">kubectl label nodes kube-master1 openstack-control-plane-kubectl label nodes kube-master1 ceph-mon-kubectl label nodes kube-master1 ceph-osd-kubectl label nodes kube-master1 ceph-mds-kubectl label nodes kube-master1 ceph-rgw-kubectl label nodes kube-master1 openvswitch-kubectl label nodes kube-master1 openstack-compute-node-</code></pre></blockquote><p>由於使用 Kubernetes RBAC，而目前 openstack-helm 有 bug，不會正確建立 Service account 的 ClusterRoleBindings，因此要手動建立(這邊偷懶一下直接使用 Admin roles)：</p><pre><code class="shell">$ cat &lt;&lt;EOF | kubectl create -f -apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata:  name: ceph-sa-adminroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: cluster-adminsubjects:  - apiGroup: rbac.authorization.k8s.io    kind: User    name: system:serviceaccount:ceph:defaultEOF$ cat &lt;&lt;EOF | kubectl create -f -apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata:  name: openstack-sa-adminroleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: cluster-adminsubjects:  - apiGroup: rbac.authorization.k8s.io    kind: User    name: system:serviceaccount:openstack:defaultEOF</code></pre><blockquote><p>若沒有建立的話，會有類似以下的錯誤資訊：</p><pre><code>Error from server (Forbidden): error when creating &quot;STDIN&quot;: secrets is forbidden: User &quot;system:serviceaccount:ceph:default&quot; cannot create secrets in the namespace &quot;ceph&quot;</code></pre></blockquote><p>下載最新版本 openstack-helm 專案：</p><pre><code class="shell">$ git clone https://github.com/openstack/openstack-helm.git$ cd openstack-helm</code></pre><p>現在須建立 openstack-helm chart 來提供部署使用：</p><pre><code class="shell">$ helm serve &amp;$ helm repo add local http://localhost:8879/charts$ make# output...1 chart(s) linted, no failuresif [ -d congress ]; then helm package congress; fiSuccessfully packaged chart and saved it to: /root/openstack-helm/congress-0.1.0.tgzmake[1]: Leaving directory &#39;/root/openstack-helm&#39;</code></pre><h3 id="Ceph-Chart"><a href="#Ceph-Chart" class="headerlink" title="Ceph Chart"></a>Ceph Chart</h3><p>在部署 OpenStack 前，需要先部署 Ceph 叢集，這邊透過以下指令建置：</p><pre><code class="shell">$ helm install --namespace=ceph ${WORK_DIR}/ceph --name=ceph \  --set endpoints.identity.namespace=openstack \  --set endpoints.object_store.namespace=ceph \  --set endpoints.ceph_mon.namespace=ceph \  --set ceph.rgw_keystone_auth=${CEPH_RGW_KEYSTONE_ENABLED} \  --set network.public=${OSD_PUBLIC_NETWORK} \  --set network.cluster=${OSD_CLUSTER_NETWORK} \  --set deployment.storage_secrets=true \  --set deployment.ceph=true \  --set deployment.rbd_provisioner=true \  --set deployment.client_secrets=false \  --set deployment.rgw_keystone_user_and_endpoints=false \  --set bootstrap.enabled=true</code></pre><blockquote><ul><li><code>CEPH_RGW_KEYSTONE_ENABLED</code>是否啟動 Ceph RGW Keystone。</li><li><code>OSD_PUBLIC_NETWORK</code>與<code>OSD_PUBLIC_NETWORK</code>為 Ceph 叢集網路。</li></ul></blockquote><p>成功安裝 Ceph chart 後，就可以透過 kubectl 來查看結果：</p><pre><code class="shell">$ kubectl -n ceph get poNAME                                   READY     STATUS    RESTARTS   AGEceph-mds-57798cc8f6-r898r              1/1       Running   2          10minceph-mon-96p9r                         1/1       Running   0          10minceph-mon-check-bd8875f87-whvhd         1/1       Running   0          10minceph-mon-qkj95                         1/1       Running   0          10minceph-mon-zx7tw                         1/1       Running   0          10minceph-osd-5fvfl                         1/1       Running   0          10minceph-osd-kvw9b                         1/1       Running   0          10minceph-osd-wcf5j                         1/1       Running   0          10minceph-rbd-provisioner-599ff9575-mdqnf   1/1       Running   0          10minceph-rbd-provisioner-599ff9575-vpcr6   1/1       Running   0          10minceph-rgw-7c8c5d4f6f-8fq9c              1/1       Running   3          10min</code></pre><p>確認 Ceph 叢集建立正確：</p><pre><code class="shell">$ MON_POD=$(kubectl get pods \  --namespace=ceph \  --selector=&quot;application=ceph&quot; \  --selector=&quot;component=mon&quot; \  --no-headers | awk &#39;{ print $1; exit }&#39;)$ kubectl exec -n ceph ${MON_POD} -- ceph -s    cluster 02ad8724-dee0-4f55-829f-3cc24e2c7571     health HEALTH_WARN            too many PGs per OSD (856 &gt; max 300)     monmap e2: 3 mons at {kube-node1=172.22.132.22:6789/0,kube-node2=172.22.132.24:6789/0,kube-node3=172.22.132.28:6789/0}            election epoch 8, quorum 0,1,2 kube-node1,kube-node2,kube-node3      fsmap e5: 1/1/1 up {0=mds-ceph-mds-57798cc8f6-r898r=up:active}     osdmap e21: 3 osds: 3 up, 3 in            flags sortbitwise,require_jewel_osds      pgmap v6053: 856 pgs, 10 pools, 3656 bytes data, 191 objects            43091 MB used, 2133 GB / 2291 GB avail                 856 active+clean</code></pre><blockquote><p>Warn 這邊忽略，OSD 機器太少….。</p></blockquote><p>接著為了讓 Ceph 可以在其他 Kubernetes namespace 中存取 PVC，這邊要產生 client secret key 於 openstack namespace 中來提供給 OpenStack 元件使用，這邊執行以下 Chart 來產生：</p><pre><code class="shell">$ helm install --namespace=openstack ${WORK_DIR}/ceph --name=ceph-openstack-config \  --set endpoints.identity.namespace=openstack \  --set endpoints.object_store.namespace=ceph \  --set endpoints.ceph_mon.namespace=ceph \  --set ceph.rgw_keystone_auth=${CEPH_RGW_KEYSTONE_ENABLED} \  --set network.public=${OSD_PUBLIC_NETWORK} \  --set network.cluster=${OSD_CLUSTER_NETWORK} \  --set deployment.storage_secrets=false \  --set deployment.ceph=false \  --set deployment.rbd_provisioner=false \  --set deployment.client_secrets=true \  --set deployment.rgw_keystone_user_and_endpoints=false</code></pre><p>檢查 pod 與 secret 是否建立成功：</p><pre><code class="shell">$ kubectl -n openstack get secret,po -aNAME                          TYPE                                  DATA      AGEsecrets/default-token-q2r87   kubernetes.io/service-account-token   3         2msecrets/pvc-ceph-client-key   kubernetes.io/rbd                     1         2mNAME                                           READY     STATUS      RESTARTS   AGEpo/ceph-namespace-client-key-generator-w84n4   0/1       Completed   0          2m</code></pre><h3 id="OpenStack-Chart"><a href="#OpenStack-Chart" class="headerlink" title="OpenStack Chart"></a>OpenStack Chart</h3><p>確認沒問題後，就可以開始部署 OpenStack chart 了。首先先安裝 Mariadb cluster:</p><pre><code class="shell">$ helm install --name=mariadb ./mariadb --namespace=openstack</code></pre><blockquote><p>這邊跑超久…34mins…，原因可能是 Storage 效能問題。</p></blockquote><p>這邊正確執行後，會依序依據 StatefulSet 建立起 Pod 組成 Cluster：</p><pre><code class="shell">$ kubectl -n openstack get poNAME        READY     STATUS    RESTARTS   AGEmariadb-0   1/1       Running   0          37mmariadb-1   1/1       Running   0          4mmariadb-2   1/1       Running   0          2m</code></pre><p>當 Mariadb cluster 完成後，就可以部署一些需要的服務，如 RabbitMQ, OVS 等：</p><pre><code class="shell">helm install --name=memcached ./memcached --namespace=openstackhelm install --name=etcd-rabbitmq ./etcd --namespace=openstackhelm install --name=rabbitmq ./rabbitmq --namespace=openstackhelm install --name=ingress ./ingress --namespace=openstackhelm install --name=libvirt ./libvirt --namespace=openstackhelm install --name=openvswitch ./openvswitch --namespace=openstack</code></pre><p>上述指令若正確執行的話，會分別建立起以下服務：</p><pre><code class="shell">$ kubectl -n openstack get poNAME                                   READY     STATUS    RESTARTS   AGEetcd-5c9bc8c97f-jpm2k                  1/1       Running   0          4mingress-api-jhjjv                      1/1       Running   0          4mingress-api-nx5qm                      1/1       Running   0          4mingress-api-vr8xf                      1/1       Running   0          4mingress-error-pages-86b9db69cc-mmq4p   1/1       Running   0          4mlibvirt-94xq5                          1/1       Running   0          4mlibvirt-lzfzs                          1/1       Running   0          4mlibvirt-vswxb                          1/1       Running   0          4mmariadb-0                              1/1       Running   0          42mmariadb-1                              1/1       Running   0          9mmariadb-2                              1/1       Running   0          7mmemcached-746fcc894-cwhpr              1/1       Running   0          4mopenvswitch-db-7fjr2                   1/1       Running   0          4mopenvswitch-db-gtmcr                   1/1       Running   0          4mopenvswitch-db-hqmbt                   1/1       Running   0          4mopenvswitch-vswitchd-gptp9             1/1       Running   0          4mopenvswitch-vswitchd-s4cwd             1/1       Running   0          4mopenvswitch-vswitchd-tvxlg             1/1       Running   0          4mrabbitmq-6fdb8879df-6vmz8              1/1       Running   0          4mrabbitmq-6fdb8879df-875zz              1/1       Running   0          4mrabbitmq-6fdb8879df-h5wj6              1/1       Running   0          4m</code></pre><p>一旦所有基礎服務與元件都建立完成後，就可以開始部署 OpenStack 的專案 Chart，首先建立 Keystone 來提供身份認證服務：</p><pre><code class="shell">$ helm install --namespace=openstack --name=keystone ./keystone \  --set pod.replicas.api=1$ kubectl -n openstack get po -l application=keystoneNAME                            READY     STATUS     RESTARTS   AGEkeystone-api-74c774d448-dkqmj   0/1       Init:0/1   0          4mkeystone-bootstrap-xpdtl        0/1       Init:0/1   0          4mkeystone-db-sync-2bxtp          1/1       Running    0          4m        0          29s</code></pre><blockquote><p>這邊由於叢集規模問題，副本數都為一份。</p></blockquote><p>這時候會先建立 Keystone database tables，完成後將啟動 API pod，如以下結果：</p><pre><code class="shell">$ kubectl -n openstack get po -l application=keystoneNAME                            READY     STATUS    RESTARTS   AGEkeystone-api-74c774d448-dkqmj   1/1       Running   0          11m</code></pre><p>如果安裝支援 RGW 的 Keystone endpoint 的話，可以使用以下方式建立：</p><pre><code class="shell">$ helm install --namespace=openstack ${WORK_DIR}/ceph --name=radosgw-openstack \  --set endpoints.identity.namespace=openstack \  --set endpoints.object_store.namespace=ceph \  --set endpoints.ceph_mon.namespace=ceph \  --set ceph.rgw_keystone_auth=${CEPH_RGW_KEYSTONE_ENABLED} \  --set network.public=${OSD_PUBLIC_NETWORK} \  --set network.cluster=${OSD_CLUSTER_NETWORK} \  --set deployment.storage_secrets=false \  --set deployment.ceph=false \  --set deployment.rbd_provisioner=false \  --set deployment.client_secrets=false \  --set deployment.rgw_keystone_user_and_endpoints=true$ kubectl -n openstack get po -a -l application=cephNAME                                        READY     STATUS      RESTARTS   AGEceph-ks-endpoints-vfg4l                     0/3       Completed   0          1mceph-ks-service-tr9xt                       0/1       Completed   0          1mceph-ks-user-z5tlt                          0/1       Completed   0          1m</code></pre><p>完成後，安裝 Horizon chart 來提供 OpenStack dashbaord：</p><pre><code class="shell">$ helm install --namespace=openstack --name=horizon ./horizon \  --set network.enable_node_port=true \  --set network.node_port=31000$ kubectl -n openstack get po -l application=horizonNAME                       READY     STATUS    RESTARTS   AGEhorizon-7c54878549-45668   1/1       Running   0          3m</code></pre><p>接著安裝 Glance chart 來提供 OpenStack image service。目前 Glance 支援幾個 backend storage:</p><ul><li><strong>pvc</strong>: 一個簡單的 Kubernetes PVCs 檔案後端。</li><li><strong>rbd</strong>: 使用 Ceph RBD 來儲存 images。</li><li><strong>radosgw</strong>: 使用 Ceph RGW 來儲存 images。</li><li><strong>swift</strong>: 另用 OpenStack switf 所提供的物件儲存服務來儲存 images.</li></ul><p>這邊可以利用以下方式來部署不同的儲存後端：</p><pre><code class="shell">$ export GLANCE_BACKEND=radosgw$ helm install --namespace=openstack --name=glance ./glance \  --set pod.replicas.api=1 \  --set pod.replicas.registry=1 \  --set storage=${GLANCE_BACKEND}$ kubectl -n openstack get po -l application=glanceNAME                               READY     STATUS    RESTARTS   AGEglance-api-6cd8b856d6-lhzfs        1/1       Running   0          14mglance-registry-599f8b857b-gt4c6   1/1       Running   0          14m</code></pre><p>接著安裝 Neutron chart 來提供 OpenStack 虛擬化網路服務：</p><pre><code class="shell">$ helm install --namespace=openstack --name=neutron ./neutron \  --set pod.replicas.server=1$ kubectl -n openstack get po -l application=neutronNAME                              READY     STATUS    RESTARTS   AGEneutron-dhcp-agent-2z49d          1/1       Running   0          9hneutron-dhcp-agent-d2kn8          1/1       Running   0          9hneutron-dhcp-agent-mrstl          1/1       Running   0          9hneutron-l3-agent-9f9mw            1/1       Running   0          9hneutron-l3-agent-cshzw            1/1       Running   0          9hneutron-l3-agent-j5vb9            1/1       Running   0          9hneutron-metadata-agent-6bfb2      1/1       Running   0          9hneutron-metadata-agent-kxk9c      1/1       Running   0          9hneutron-metadata-agent-w8cnl      1/1       Running   0          9hneutron-ovs-agent-j2549           1/1       Running   0          9hneutron-ovs-agent-plj9t           1/1       Running   0          9hneutron-ovs-agent-xlx7z           1/1       Running   0          9hneutron-server-6f45d74b87-6wmck   1/1       Running   0          9h</code></pre><p>接著安裝 Nova chart 來提供 OpenStack 虛擬機運算服務:</p><pre><code class="shell">$ helm install --namespace=openstack --name=nova ./nova \  --set pod.replicas.api_metadata=1 \  --set pod.replicas.osapi=1 \  --set pod.replicas.conductor=1 \  --set pod.replicas.consoleauth=1 \  --set pod.replicas.scheduler=1 \  --set pod.replicas.novncproxy=1$ kubectl -n openstack get po -l application=novaNAME                                 READY     STATUS    RESTARTS   AGEnova-api-metadata-84fdc84fd7-ldzrh   1/1       Running   1          9hnova-api-osapi-57f599c6d6-pqrjv      1/1       Running   0          9hnova-compute-8rvm9                   2/2       Running   0          9hnova-compute-cbk7h                   2/2       Running   0          9hnova-compute-tf2jb                   2/2       Running   0          9hnova-conductor-7f5bc76d79-bxwnb      1/1       Running   0          9hnova-consoleauth-6946b5884f-nss6n    1/1       Running   0          9hnova-novncproxy-d789dccff-7ft9q      1/1       Running   0          9hnova-placement-api-f7c79578f-hj2g9   1/1       Running   0          9hnova-scheduler-778866f555-mmksg      1/1       Running   0          9h</code></pre><p>接著安裝 Cinfer chart 來提供 OpenStack 區塊儲存服務:</p><pre><code class="shell">$ helm install --namespace=openstack --name=cinder ./cinder \  --set pod.replicas.api=1$ kubectl -n openstack get po -l application=cinderNAME                                READY     STATUS    RESTARTS   AGEcinder-api-5cc89f5467-ssm8k         1/1       Running   0          32mcinder-backup-67c4d8dfdb-zfsq4      1/1       Running   0          32mcinder-scheduler-65f9dd49bf-6htwg   1/1       Running   0          32mcinder-volume-69bfb67b4-bmst2       1/1       Running   0          32m</code></pre><p>(option)都完成後，將 Horizon 服務透過 NodePort 方式曝露出來(如果上面 Horizon chart 沒反應的話)，執行以下指令編輯：</p><pre><code class="shell">$ kubectl -n openstack edit svc horizon-int# 修改 type:  type: NodePort</code></pre><p>最後連接 <a href="http://172.22.132.10:31000" target="_blank" rel="noopener">Horizon Dashboard</a>，預設使用者為<code>admin/password</code>。</p><p><img src="https://i.imgur.com/8yunUPy.png" alt=""></p><p>其他 Chart 可以利用以下方式來安裝，如 Heat chart：</p><pre><code class="shell">$ helm install --namespace=openstack --name=heat ./heat$ kubectl -n openstack get po -l application=heatNAME                              READY     STATUS    RESTARTS   AGEheat-api-5cf45d9d44-qrt69         1/1       Running   0          13mheat-cfn-79dbf55789-bq4wh         1/1       Running   0          13mheat-cloudwatch-bcc4647f4-4c4ln   1/1       Running   0          13mheat-engine-55cfcc86f8-cct4m      1/1       Running   0          13m</code></pre><h2 id="測試-OpenStack-功能"><a href="#測試-OpenStack-功能" class="headerlink" title="測試 OpenStack 功能"></a>測試 OpenStack 功能</h2><p>在<code>kube-master1</code>安裝 openstack client:</p><pre><code class="shell">$ sudo pip install python-openstackclient</code></pre><p>建立<code>adminrc</code>來提供 client 環境變數：</p><pre><code class="shell">export OS_PROJECT_DOMAIN_NAME=defaultexport OS_USER_DOMAIN_NAME=defaultexport OS_PROJECT_NAME=adminexport OS_USERNAME=adminexport OS_PASSWORD=passwordexport OS_AUTH_URL=http://keystone.openstack.svc.cluster.local:80/v3export OS_IDENTITY_API_VERSION=3export OS_IMAGE_API_VERSION=2</code></pre><p>引入環境變數，並透過 openstack client 測試：</p><pre><code class="shell">$ source adminrc$ openstack user list+----------------------------------+-----------+| ID                               | Name      |+----------------------------------+-----------+| 42f0d2e7823e413cb469f9cce731398a | glance    || 556a2744811f450098f64b37d34192d4 | nova      || a97ec73724aa4445b2d575be54f23240 | cinder    || b28a5dcfd18948419e14acba7ecf6f63 | swift     || d1f312b6bb7c460eb7d8d78c8bf350fc | admin     || dc326aace22c4314a0100865fe4f57c2 | neutron   || ec5d6d3c529847b29a1c9187599c8a6b | placement |+----------------------------------+-----------+</code></pre><p>接著需要設定對外網路來提供給 VM 存取，在有<code>neutron-l3-agent</code>節點上，新增一個腳本<code>setup-gateway.sh</code>：</p><pre><code class="shell">#!/bin/bashset -x# Assign IP address to br-exOSH_BR_EX_ADDR=&quot;172.24.4.1/24&quot;OSH_EXT_SUBNET=&quot;172.24.4.0/24&quot;sudo ip addr add ${OSH_BR_EX_ADDR} dev br-exsudo ip link set br-ex up# Setup masquerading on default route dev to public subnetDEFAULT_ROUTE_DEV=&quot;enp3s0&quot;sudo iptables -t nat -A POSTROUTING -o ${DEFAULT_ROUTE_DEV} -s ${OSH_EXT_SUBNET} -j MASQUERADE</code></pre><blockquote><ul><li>網卡記得修改<code>DEFAULT_ROUTE_DEV</code>。</li><li>這邊因為沒有額外提供其他張網卡，所以先用 bridge 處理。</li></ul></blockquote><p>然後透過執行該腳本建立一個 bridge 網路：</p><pre><code class="shell">$ chmod u+x setup-gateway.sh$ ./setup-gateway.sh</code></pre><p>確認完成後，接著建立 Neutron ext net，透過以下指令進行建立：</p><pre><code class="shell">$ openstack network create \   --share --external \   --provider-physical-network external \   --provider-network-type flat ext-net$ openstack subnet create --network ext-net \    --allocation-pool start=172.24.4.10,end=172.24.4.100 \    --dns-nameserver 8.8.8.8 --gateway 172.24.4.1 \    --subnet-range 172.24.4.0/24 \    --no-dhcp ext-subnet$ openstack router create router1$ neutron router-gateway-set router1 ext-net</code></pre><p>直接進入 Dashboard 新增 Self-service Network:<br><img src="https://i.imgur.com/lqMrgqs.png" alt=""></p><p>加入到 router1:<br><img src="https://i.imgur.com/4aNnF3O.png" alt=""></p><p>完成後，就可以建立 instance，這邊都透過 Dashboard 來操作：<br><img src="https://i.imgur.com/fCYkxSC.png" alt=""></p><p>透過 SSH 進入 instance：<br><img src="https://i.imgur.com/Ijylo9X.png" alt=""></p><h2 id="Refers"><a href="#Refers" class="headerlink" title="Refers"></a>Refers</h2><ul><li><a href="https://github.com/portdirect/sydney-workshop" target="_blank" rel="noopener">sydney-workshop</a></li><li><a href="https://docs.openstack.org/openstack-helm/latest/install/multinode.html" target="_blank" rel="noopener">Multi Node</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://github.com/openstack/openstack-helm&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;OpenStack Helm&lt;/a&gt; 是一個提供部署建置的專案，其目的是為了推動 OpenStack 生產環境的解決方案，而這種部署方式採用容器化方式，並執行於 Kubernetes 系統上來提供 OpenStack 服務的管理與排程等使用。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/8sMjowM.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="OpenStack" scheme="https://kairen.github.io/categories/OpenStack/"/>
    
    
      <category term="Helm" scheme="https://kairen.github.io/tags/Helm/"/>
    
      <category term="Kubernetes" scheme="https://kairen.github.io/tags/Kubernetes/"/>
    
      <category term="Openstack" scheme="https://kairen.github.io/tags/Openstack/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes v1.8.x 全手動苦工安裝教學(TL;DR)</title>
    <link href="https://kairen.github.io/2017/10/27/kubernetes/deploy/manual-v1.8/"/>
    <id>https://kairen.github.io/2017/10/27/kubernetes/deploy/manual-v1.8/</id>
    <published>2017-10-27T09:08:54.000Z</published>
    <updated>2017-12-14T02:45:17.000Z</updated>
    
    <content type="html"><![CDATA[<p>Kubernetes 提供了許多雲端平台與作業系統的安裝方式，本章將以<code>全手動安裝方式</code>來部署，主要是學習與了解 Kubernetes 建置流程。若想要瞭解更多平台的部署可以參考 <a href="https://kubernetes.io/docs/getting-started-guides/" target="_blank" rel="noopener">Picking the Right Solution</a>來選擇自己最喜歡的方式。</p><p>本次安裝版本為：</p><ul><li>Kubernetes v1.8.2</li><li>Etcd v3.2.9</li><li>Calico v2.6.2</li><li>Docker v17.10.0-ce</li></ul><a id="more"></a><h2 id="預先準備資訊"><a href="#預先準備資訊" class="headerlink" title="預先準備資訊"></a>預先準備資訊</h2><p>本教學將以下列節點數與規格來進行部署 Kubernetes 叢集，作業系統可採用<code>Ubuntu 16.x</code>與<code>CentOS 7.x</code>：</p><table><thead><tr><th>IP Address</th><th>Role</th><th>CPU</th><th>Memory</th></tr></thead><tbody><tr><td>172.16.35.12</td><td>master1</td><td>1</td><td>2G</td></tr><tr><td>172.16.35.10</td><td>node1</td><td>1</td><td>2G</td></tr><tr><td>172.16.35.11</td><td>node2</td><td>1</td><td>2G</td></tr></tbody></table><blockquote><ul><li>這邊 master 為主要控制節點也是<code>部署節點</code>，node 為應用程式工作節點。</li><li>所有操作全部用<code>root</code>使用者進行(方便用)，以 SRE 來說不推薦。</li><li>可以下載 <a href="https://kairen.github.io/files/manual-v1.8/Vagrantfile">Vagrantfile</a> 來建立 Virtual box 虛擬機叢集。</li></ul></blockquote><p>首先安裝前要確認以下幾項都已將準備完成：</p><ul><li>所有節點彼此網路互通，並且<code>master1</code> SSH 登入其他節點為 passwdless。</li><li>所有防火牆與 SELinux 已關閉。如 CentOS：</li></ul><pre><code class="sh">$ systemctl stop firewalld &amp;&amp; systemctl disable firewalld$ setenforce 0$ vim /etc/selinux/configSELINUX=disabled</code></pre><ul><li>所有節點需要設定<code>/etc/host</code>解析到所有主機。</li></ul><pre><code>...172.16.35.10 node1172.16.35.11 node2172.16.35.12 master1</code></pre><ul><li>所有節點需要安裝<code>Docker</code>或<code>rtk</code>引擎。這邊採用<code>Docker</code>來當作容器引擎，安裝方式如下：</li></ul><pre><code class="sh">$ curl -fsSL &quot;https://get.docker.com/&quot; | sh</code></pre><blockquote><p>不管是在 <code>Ubuntu</code> 或 <code>CentOS</code> 都只需要執行該指令就會自動安裝最新版 Docker。<br>CentOS 安裝完成後，需要再執行以下指令：</p><pre><code class="sh">$ systemctl enable docker &amp;&amp; systemctl start docker</code></pre></blockquote><p>編輯<code>/lib/systemd/system/docker.service</code>，在<code>ExecStart=..</code>上面加入：</p><pre><code>ExecStartPost=/sbin/iptables -A FORWARD -s 0.0.0.0/0 -j ACCEPT</code></pre><blockquote><p>完成後，重新啟動 docker 服務：</p><pre><code class="sh">$ systemctl daemon-reload &amp;&amp; systemctl restart docker</code></pre></blockquote><ul><li>所有節點需要設定<code>/etc/sysctl.d/k8s.conf</code>的系統參數。</li></ul><pre><code class="sh">$ cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.ipv4.ip_forward = 1net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOF$ sysctl -p /etc/sysctl.d/k8s.conf</code></pre><ul><li>在<code>master1</code>需要安裝<code>CFSSL</code>工具，這將會用來建立 TLS certificates。</li></ul><pre><code class="sh">$ export CFSSL_URL=&quot;https://pkg.cfssl.org/R1.2&quot;$ wget &quot;${CFSSL_URL}/cfssl_linux-amd64&quot; -O /usr/local/bin/cfssl$ wget &quot;${CFSSL_URL}/cfssljson_linux-amd64&quot; -O /usr/local/bin/cfssljson$ chmod +x /usr/local/bin/cfssl /usr/local/bin/cfssljson</code></pre><h2 id="Etcd"><a href="#Etcd" class="headerlink" title="Etcd"></a>Etcd</h2><p>在開始安裝 Kubernetes 之前，需要先將一些必要系統建置完成，其中 Etcd 就是 Kubernetes 最重要的一環，Kubernetes 會將大部分資訊儲存於 Etcd 上，來提供給其他節點索取，以確保整個叢集運作與溝通正常。</p><h3 id="建立叢集-CA-與-Certificates"><a href="#建立叢集-CA-與-Certificates" class="headerlink" title="建立叢集 CA 與 Certificates"></a>建立叢集 CA 與 Certificates</h3><p>在這部分，將會需要產生 client 與 server 的各元件 certificates，並且替 Kubernetes admin user 產生 client 證書。</p><p>建立<code>/etc/etcd/ssl</code>資料夾，然後進入目錄完成以下操作。</p><pre><code class="sh">$ mkdir -p /etc/etcd/ssl &amp;&amp; cd /etc/etcd/ssl$ export PKI_URL=&quot;https://kairen.github.io/files/manual-v1.8/pki&quot;</code></pre><p>下載<code>ca-config.json</code>與<code>etcd-ca-csr.json</code>檔案，並產生 CA 金鑰：</p><pre><code class="sh">$ wget &quot;${PKI_URL}/ca-config.json&quot; &quot;${PKI_URL}/etcd-ca-csr.json&quot;$ cfssl gencert -initca etcd-ca-csr.json | cfssljson -bare etcd-ca$ ls etcd-ca*.pemetcd-ca-key.pem  etcd-ca.pem</code></pre><p>下載<code>etcd-csr.json</code>檔案，並產生 etcd certificate 證書：</p><pre><code class="sh">$ wget &quot;${PKI_URL}/etcd-csr.json&quot;$ cfssl gencert \  -ca=etcd-ca.pem \  -ca-key=etcd-ca-key.pem \  -config=ca-config.json \  -profile=kubernetes \  etcd-csr.json | cfssljson -bare etcd$ ls etcd*.pemetcd-ca-key.pem  etcd-ca.pem  etcd-key.pem  etcd.pem</code></pre><blockquote><p>若節點 IP 不同，需要修改<code>etcd-csr.json</code>的<code>hosts</code>。</p></blockquote><p>完成後刪除不必要檔案：</p><pre><code class="sh">$ rm -rf *.json</code></pre><p>確認<code>/etc/etcd/ssl</code>有以下檔案：</p><pre><code class="sh">$ ls /etc/etcd/ssletcd-ca.csr  etcd-ca-key.pem  etcd-ca.pem  etcd.csr  etcd-key.pem  etcd.pem</code></pre><h3 id="Etcd-安裝與設定"><a href="#Etcd-安裝與設定" class="headerlink" title="Etcd 安裝與設定"></a>Etcd 安裝與設定</h3><p>首先在<code>master1</code>節點下載 Etcd，並解壓縮放到 /opt 底下與安裝：</p><pre><code class="sh">$ export ETCD_URL=&quot;https://github.com/coreos/etcd/releases/download&quot;$ cd &amp;&amp; wget -qO- --show-progress &quot;${ETCD_URL}/v3.2.9/etcd-v3.2.9-linux-amd64.tar.gz&quot; | tar -zx$ mv etcd-v3.2.9-linux-amd64/etcd* /usr/local/bin/ &amp;&amp; rm -rf etcd-v3.2.9-linux-amd64</code></pre><p>完成後新建 Etcd Group 與 User，並建立 Etcd 設定檔目錄：</p><pre><code class="sh">$ groupadd etcd &amp;&amp; useradd -c &quot;Etcd user&quot; -g etcd -s /sbin/nologin -r etcd</code></pre><p>下載<code>etcd</code>相關檔案，我們將來管理 Etcd：</p><pre><code class="sh">$ export ETCD_CONF_URL=&quot;https://kairen.github.io/files/manual-v1.8/master&quot;$ wget &quot;${ETCD_CONF_URL}/etcd.conf&quot; -O /etc/etcd/etcd.conf$ wget &quot;${ETCD_CONF_URL}/etcd.service&quot; -O /lib/systemd/system/etcd.service</code></pre><blockquote><p>若與該教學 IP 不同的話，請用自己 IP 取代<code>172.16.35.12</code>。</p></blockquote><p>建立 var 存放資訊，然後啟動 Etcd 服務:</p><pre><code class="sh">$ mkdir -p /var/lib/etcd &amp;&amp; chown etcd:etcd -R /var/lib/etcd /etc/etcd$ systemctl enable etcd.service &amp;&amp; systemctl start etcd.service</code></pre><p>透過簡單指令驗證：</p><pre><code class="sh">$ export CA=&quot;/etc/etcd/ssl&quot;$ ETCDCTL_API=3 etcdctl \    --cacert=${CA}/etcd-ca.pem \    --cert=${CA}/etcd.pem \    --key=${CA}/etcd-key.pem \    --endpoints=&quot;https://172.16.35.12:2379&quot; \    endpoint health# outputhttps://172.16.35.12:2379 is healthy: successfully committed proposal: took = 641.36µs</code></pre><h2 id="Kubernetes-Master"><a href="#Kubernetes-Master" class="headerlink" title="Kubernetes Master"></a>Kubernetes Master</h2><p>Master 是 Kubernetes 的大總管，主要建置<code>apiserver</code>、<code>Controller manager</code>與<code>Scheduler</code>來元件管理所有 Node。本步驟將下載 Kubernetes 並安裝至 <code>master1</code>上，然後產生相關 TLS Cert 與 CA 金鑰，提供給叢集元件認證使用。</p><h3 id="下載-Kubernetes-元件"><a href="#下載-Kubernetes-元件" class="headerlink" title="下載 Kubernetes 元件"></a>下載 Kubernetes 元件</h3><p>首先透過網路取得所有需要的執行檔案：</p><pre><code class="sh"># Download Kubernetes$ export KUBE_URL=&quot;https://storage.googleapis.com/kubernetes-release/release/v1.8.2/bin/linux/amd64&quot;$ wget &quot;${KUBE_URL}/kubelet&quot; -O /usr/local/bin/kubelet$ wget &quot;${KUBE_URL}/kubectl&quot; -O /usr/local/bin/kubectl$ chmod +x /usr/local/bin/kubelet /usr/local/bin/kubectl# Download CNI$ mkdir -p /opt/cni/bin &amp;&amp; cd /opt/cni/bin$ export CNI_URL=&quot;https://github.com/containernetworking/plugins/releases/download&quot;$ wget -qO- --show-progress &quot;${CNI_URL}/v0.6.0/cni-plugins-amd64-v0.6.0.tgz&quot; | tar -zx</code></pre><h3 id="建立叢集-CA-與-Certificates-1"><a href="#建立叢集-CA-與-Certificates-1" class="headerlink" title="建立叢集 CA 與 Certificates"></a>建立叢集 CA 與 Certificates</h3><p>在這部分，將會需要產生 client 與 server 的各元件 certificates，並且替 Kubernetes admin user 產生 client 證書。</p><p>建立<code>pki</code>資料夾，然後進入目錄完成以下操作。</p><pre><code class="sh">$ mkdir -p /etc/kubernetes/pki &amp;&amp; cd /etc/kubernetes/pki$ export PKI_URL=&quot;https://kairen.github.io/files/manual-v1.8/pki&quot;$ export KUBE_APISERVER=&quot;https://172.16.35.12:6443&quot;</code></pre><p>下載<code>ca-config.json</code>與<code>ca-csr.json</code>檔案，並產生 CA 金鑰：</p><pre><code class="sh">$ wget &quot;${PKI_URL}/ca-config.json&quot; &quot;${PKI_URL}/ca-csr.json&quot;$ cfssl gencert -initca ca-csr.json | cfssljson -bare ca$ ls ca*.pemca-key.pem  ca.pem</code></pre><h4 id="API-server-certificate"><a href="#API-server-certificate" class="headerlink" title="API server certificate"></a>API server certificate</h4><p>下載<code>apiserver-csr.json</code>檔案，並產生 kube-apiserver certificate 證書：</p><pre><code class="sh">$ wget &quot;${PKI_URL}/apiserver-csr.json&quot;$ cfssl gencert \  -ca=ca.pem \  -ca-key=ca-key.pem \  -config=ca-config.json \  -hostname=10.96.0.1,172.16.35.12,127.0.0.1,kubernetes.default \  -profile=kubernetes \  apiserver-csr.json | cfssljson -bare apiserver$ ls apiserver*.pemapiserver-key.pem  apiserver.pem</code></pre><blockquote><p>若節點 IP 不同，需要修改<code>-hostname</code>。</p></blockquote><h4 id="Front-proxy-certificate"><a href="#Front-proxy-certificate" class="headerlink" title="Front proxy certificate"></a>Front proxy certificate</h4><p>下載<code>front-proxy-ca-csr.json</code>檔案，並產生 Front proxy CA 金鑰，Front proxy 主要是用在 API aggregator 上:</p><pre><code class="sh">$ wget &quot;${PKI_URL}/front-proxy-ca-csr.json&quot;$ cfssl gencert \  -initca front-proxy-ca-csr.json | cfssljson -bare front-proxy-ca$ ls front-proxy-ca*.pemfront-proxy-ca-key.pem  front-proxy-ca.pem</code></pre><p>下載<code>front-proxy-client-csr.json</code>檔案，並產生 front-proxy-client 證書：</p><pre><code class="sh">$ wget &quot;${PKI_URL}/front-proxy-client-csr.json&quot;$ cfssl gencert \  -ca=front-proxy-ca.pem \  -ca-key=front-proxy-ca-key.pem \  -config=ca-config.json \  -profile=kubernetes \  front-proxy-client-csr.json | cfssljson -bare front-proxy-client$ ls front-proxy-client*.pemfront-proxy-client-key.pem  front-proxy-client.pem</code></pre><h4 id="Bootstrap-Token"><a href="#Bootstrap-Token" class="headerlink" title="Bootstrap Token"></a>Bootstrap Token</h4><p>由於透過手動建立 CA 方式太過繁雜，只適合少量機器，因為每次簽證時都需要綁定 Node IP，隨機器增加會帶來很多困擾，因此這邊使用 TLS Bootstrapping 方式進行授權，由 apiserver 自動給符合條件的 Node 發送證書來授權加入叢集。</p><p>主要做法是 kubelet 啟動時，向 kube-apiserver 傳送 TLS Bootstrapping 請求，而 kube-apiserver 驗證 kubelet 請求的 token 是否與設定的一樣，若一樣就自動產生 kubelet 證書與金鑰。具體作法可以參考 <a href="https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/" target="_blank" rel="noopener">TLS bootstrapping</a>。</p><p>首先建立一個變數來產生<code>BOOTSTRAP_TOKEN</code>，並建立 <code>bootstrap.conf</code> 的 kubeconfig 檔：</p><pre><code class="sh">$ export BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d &#39; &#39;)$ cat &lt;&lt;EOF &gt; /etc/kubernetes/token.csv${BOOTSTRAP_TOKEN},kubelet-bootstrap,10001,&quot;system:kubelet-bootstrap&quot;EOF# bootstrap set-cluster$ kubectl config set-cluster kubernetes \    --certificate-authority=ca.pem \    --embed-certs=true \    --server=${KUBE_APISERVER} \    --kubeconfig=../bootstrap.conf# bootstrap set-credentials$ kubectl config set-credentials kubelet-bootstrap \    --token=${BOOTSTRAP_TOKEN} \    --kubeconfig=../bootstrap.conf# bootstrap set-context$ kubectl config set-context default \    --cluster=kubernetes \    --user=kubelet-bootstrap \   --kubeconfig=../bootstrap.conf# bootstrap set default context$ kubectl config use-context default --kubeconfig=../bootstrap.conf</code></pre><blockquote><p>若想要用 CA 方式來認證，可以參考 <a href="https://gist.github.com/kairen/60ad8545b79e8e7aa9bdc8a2893df7a0" target="_blank" rel="noopener">Kubelet certificate</a>。</p></blockquote><h4 id="Admin-certificate"><a href="#Admin-certificate" class="headerlink" title="Admin certificate"></a>Admin certificate</h4><p>下載<code>admin-csr.json</code>檔案，並產生 admin certificate 證書：</p><pre><code class="sh">$ wget &quot;${PKI_URL}/admin-csr.json&quot;$ cfssl gencert \  -ca=ca.pem \  -ca-key=ca-key.pem \  -config=ca-config.json \  -profile=kubernetes \  admin-csr.json | cfssljson -bare admin$ ls admin*.pemadmin-key.pem  admin.pem</code></pre><p>接著透過以下指令產生名稱為 <code>admin.conf</code> 的 kubeconfig 檔：</p><pre><code class="sh"># admin set-cluster$ kubectl config set-cluster kubernetes \    --certificate-authority=ca.pem \    --embed-certs=true \    --server=${KUBE_APISERVER} \    --kubeconfig=../admin.conf# admin set-credentials$ kubectl config set-credentials kubernetes-admin \    --client-certificate=admin.pem \    --client-key=admin-key.pem \    --embed-certs=true \    --kubeconfig=../admin.conf# admin set-context$ kubectl config set-context kubernetes-admin@kubernetes \    --cluster=kubernetes \    --user=kubernetes-admin \    --kubeconfig=../admin.conf# admin set default context$ kubectl config use-context kubernetes-admin@kubernetes \    --kubeconfig=../admin.conf</code></pre><h4 id="Controller-manager-certificate"><a href="#Controller-manager-certificate" class="headerlink" title="Controller manager certificate"></a>Controller manager certificate</h4><p>下載<code>manager-csr.json</code>檔案，並產生 kube-controller-manager certificate 證書：</p><pre><code class="sh">$ wget &quot;${PKI_URL}/manager-csr.json&quot;$ cfssl gencert \  -ca=ca.pem \  -ca-key=ca-key.pem \  -config=ca-config.json \  -profile=kubernetes \  manager-csr.json | cfssljson -bare controller-manager$ ls controller-manager*.pem</code></pre><blockquote><p>若節點 IP 不同，需要修改<code>manager-csr.json</code>的<code>hosts</code>。</p></blockquote><p>接著透過以下指令產生名稱為<code>controller-manager.conf</code>的 kubeconfig 檔：</p><pre><code class="sh"># controller-manager set-cluster$ kubectl config set-cluster kubernetes \    --certificate-authority=ca.pem \    --embed-certs=true \    --server=${KUBE_APISERVER} \    --kubeconfig=../controller-manager.conf# controller-manager set-credentials$ kubectl config set-credentials system:kube-controller-manager \    --client-certificate=controller-manager.pem \    --client-key=controller-manager-key.pem \    --embed-certs=true \    --kubeconfig=../controller-manager.conf# controller-manager set-context$ kubectl config set-context system:kube-controller-manager@kubernetes \    --cluster=kubernetes \    --user=system:kube-controller-manager \    --kubeconfig=../controller-manager.conf# controller-manager set default context$ kubectl config use-context system:kube-controller-manager@kubernetes \    --kubeconfig=../controller-manager.conf</code></pre><h4 id="Scheduler-certificate"><a href="#Scheduler-certificate" class="headerlink" title="Scheduler certificate"></a>Scheduler certificate</h4><p>下載<code>scheduler-csr.json</code>檔案，並產生 kube-scheduler certificate 證書：</p><pre><code class="sh">$ wget &quot;${PKI_URL}/scheduler-csr.json&quot;$ cfssl gencert \  -ca=ca.pem \  -ca-key=ca-key.pem \  -config=ca-config.json \  -profile=kubernetes \  scheduler-csr.json | cfssljson -bare scheduler$ ls scheduler*.pemscheduler-key.pem  scheduler.pem</code></pre><blockquote><p>若節點 IP 不同，需要修改<code>scheduler-csr.json</code>的<code>hosts</code>。</p></blockquote><p>接著透過以下指令產生名稱為 <code>scheduler.conf</code> 的 kubeconfig 檔：</p><pre><code class="sh"># scheduler set-cluster$ kubectl config set-cluster kubernetes \    --certificate-authority=ca.pem \    --embed-certs=true \    --server=${KUBE_APISERVER} \    --kubeconfig=../scheduler.conf# scheduler set-credentials$ kubectl config set-credentials system:kube-scheduler \    --client-certificate=scheduler.pem \    --client-key=scheduler-key.pem \    --embed-certs=true \    --kubeconfig=../scheduler.conf# scheduler set-context$ kubectl config set-context system:kube-scheduler@kubernetes \    --cluster=kubernetes \    --user=system:kube-scheduler \    --kubeconfig=../scheduler.conf# scheduler set default context$ kubectl config use-context system:kube-scheduler@kubernetes \    --kubeconfig=../scheduler.conf</code></pre><h4 id="Kubelet-master-certificate"><a href="#Kubelet-master-certificate" class="headerlink" title="Kubelet master certificate"></a>Kubelet master certificate</h4><p>下載<code>kubelet-csr.json</code>檔案，並產生 master node certificate 證書：</p><pre><code class="sh">$ wget &quot;${PKI_URL}/kubelet-csr.json&quot;$ sed -i &#39;s/$NODE/master1/g&#39; kubelet-csr.json$ cfssl gencert \  -ca=ca.pem \  -ca-key=ca-key.pem \  -config=ca-config.json \  -hostname=master1,172.16.35.12 \  -profile=kubernetes \  kubelet-csr.json | cfssljson -bare kubelet$ ls kubelet*.pemkubelet-key.pem  kubelet.pem</code></pre><blockquote><p>這邊<code>$NODE</code>需要隨節點名稱不同而改變。</p></blockquote><p>接著透過以下指令產生名稱為 <code>kubelet.conf</code> 的 kubeconfig 檔：</p><pre><code class="sh"># kubelet set-cluster$ kubectl config set-cluster kubernetes \    --certificate-authority=ca.pem \    --embed-certs=true \    --server=${KUBE_APISERVER} \    --kubeconfig=../kubelet.conf# kubelet set-credentials$ kubectl config set-credentials system:node:master1 \    --client-certificate=kubelet.pem \    --client-key=kubelet-key.pem \    --embed-certs=true \    --kubeconfig=../kubelet.conf# kubelet set-context$ kubectl config set-context system:node:master1@kubernetes \    --cluster=kubernetes \    --user=system:node:master1 \    --kubeconfig=../kubelet.conf# kubelet set default context$ kubectl config use-context system:node:master1@kubernetes \    --kubeconfig=../kubelet.conf</code></pre><h4 id="Service-account-key"><a href="#Service-account-key" class="headerlink" title="Service account key"></a>Service account key</h4><p>Service account 不是透過 CA 進行認證，因此不要透過 CA 來做 Service account key 的檢查，這邊建立一組 Private 與 Public 金鑰提供給 Service account key 使用：</p><pre><code class="sh">$ openssl genrsa -out sa.key 2048$ openssl rsa -in sa.key -pubout -out sa.pub$ ls sa.*sa.key  sa.pub</code></pre><p>完成後刪除不必要檔案：</p><pre><code class="sh">$ rm -rf *.json *.csr</code></pre><p>確認<code>/etc/kubernetes</code>與<code>/etc/kubernetes/pki</code>有以下檔案：</p><pre><code class="sh">$ ls /etc/kubernetes/admin.conf  bootstrap.conf  controller-manager.conf  kubelet.conf  pki  scheduler.conf  token.csv$ ls /etc/kubernetes/pkiadmin-key.pem  apiserver-key.pem  ca-key.pem  controller-manager-key.pem  front-proxy-ca-key.pem  front-proxy-client-key.pem  kubelet-key.pem  sa.key  scheduler-key.pemadmin.pem      apiserver.pem      ca.pem      controller-manager.pem      front-proxy-ca.pem      front-proxy-client.pem      kubelet.pem      sa.pub  scheduler.pem</code></pre><h3 id="安裝-Kubernetes-核心元件"><a href="#安裝-Kubernetes-核心元件" class="headerlink" title="安裝 Kubernetes 核心元件"></a>安裝 Kubernetes 核心元件</h3><p>首先下載 Kubernetes 核心元件 YAML 檔案，這邊我們不透過 Binary 方案來建立 Master 核心元件，而是利用 Kubernetes Static Pod 來達成，因此需下載所有核心元件的<code>Static Pod</code>檔案到<code>/etc/kubernetes/manifests</code>目錄：</p><pre><code class="sh">$ export CORE_URL=&quot;https://kairen.github.io/files/manual-v1.8/master&quot;$ mkdir -p /etc/kubernetes/manifests &amp;&amp; cd /etc/kubernetes/manifests$ for FILE in apiserver manager scheduler; do    wget &quot;${CORE_URL}/${FILE}.yml.conf&quot; -O ${FILE}.yml  done</code></pre><blockquote><p>若<code>IP</code>與教學設定不同的話，請記得修改<code>apiserver.yml</code>、<code>manager.yml</code>、<code>scheduler.yml</code>。<br>apiserver 中的 <code>NodeRestriction</code> 請參考 <a href="https://kubernetes.io/docs/admin/authorization/node/" target="_blank" rel="noopener">Using Node Authorization</a>。</p></blockquote><p>產生一個用來加密 Etcd 的 Key：</p><pre><code class="sh">$ head -c 32 /dev/urandom | base64SUpbL4juUYyvxj3/gonV5xVEx8j769/99TSAf8YT/sQ=</code></pre><p>在<code>/etc/kubernetes/</code>目錄下，建立<code>encryption.yml</code>的加密 YAML 檔案：</p><pre><code class="sh">$ cat &lt;&lt;EOF &gt; /etc/kubernetes/encryption.ymlkind: EncryptionConfigapiVersion: v1resources:  - resources:      - secrets    providers:      - aescbc:          keys:            - name: key1              secret: SUpbL4juUYyvxj3/gonV5xVEx8j769/99TSAf8YT/sQ=      - identity: {}EOF</code></pre><blockquote><p>Etcd 資料加密可參考這篇 <a href="https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/" target="_blank" rel="noopener">Encrypting data at rest</a>。</p></blockquote><p>在<code>/etc/kubernetes/</code>目錄下，建立<code>audit-policy.yml</code>的進階稽核策略 YAML 檔：</p><pre><code class="sh">$ cat &lt;&lt;EOF &gt; /etc/kubernetes/audit-policy.ymlapiVersion: audit.k8s.io/v1beta1kind: Policyrules:- level: MetadataEOF</code></pre><blockquote><p>Audit Policy 請參考這篇 <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/" target="_blank" rel="noopener">Auditing</a>。</p></blockquote><p>下載<code>kubelet.service</code>相關檔案來管理 kubelet：</p><pre><code class="sh">$ export KUBELET_URL=&quot;https://kairen.github.io/files/manual-v1.8/master&quot;$ mkdir -p /etc/systemd/system/kubelet.service.d$ wget &quot;${KUBELET_URL}/kubelet.service&quot; -O /lib/systemd/system/kubelet.service$ wget &quot;${KUBELET_URL}/10-kubelet.conf&quot; -O /etc/systemd/system/kubelet.service.d/10-kubelet.conf</code></pre><p>最後建立 var 存放資訊，然後啟動 kubelet 服務:</p><pre><code class="sh">$ mkdir -p /var/lib/kubelet /var/log/kubernetes$ systemctl enable kubelet.service &amp;&amp; systemctl start kubelet.service</code></pre><p>完成後會需要一段時間來下載映像檔與啟動元件，可以利用該指令來監看：</p><pre><code class="sh">$ watch netstat -ntlptcp        0      0 127.0.0.1:10248         0.0.0.0:*               LISTEN      23012/kubelettcp        0      0 127.0.0.1:10251         0.0.0.0:*               LISTEN      22305/kube-scheduletcp        0      0 127.0.0.1:10252         0.0.0.0:*               LISTEN      22529/kube-controlltcp6       0      0 :::6443                 :::*                    LISTEN      22956/kube-apiserve</code></pre><blockquote><p>若看到以上資訊表示服務正常啟動，若發生問題可以用<code>docker cli</code>來查看。</p></blockquote><p>完成後，複製 admin kubeconfig 檔案，並透過簡單指令驗證：</p><pre><code class="sh">$ cp /etc/kubernetes/admin.conf ~/.kube/config$ kubectl get csNAME                 STATUS    MESSAGE              ERRORetcd-0               Healthy   {&quot;health&quot;: &quot;true&quot;}scheduler            Healthy   okcontroller-manager   Healthy   ok$ kubectl get nodeNAME      STATUS     ROLES     AGE       VERSIONmaster1   NotReady   master    4m        v1.8.2$ kubectl -n kube-system get poNAME                              READY     STATUS    RESTARTS   AGEkube-apiserver-master1            1/1       Running   0          4mkube-controller-manager-master1   1/1       Running   0          4mkube-scheduler-master1            1/1       Running   0          4m</code></pre><p>確認服務能夠執行 logs 等指令：</p><pre><code class="sh">$ kubectl -n kube-system logs -f kube-scheduler-master1Error from server (Forbidden): Forbidden (user=kube-apiserver, verb=get, resource=nodes, subresource=proxy) ( pods/log kube-apiserver-master1)</code></pre><blockquote><p>這邊會發現出現 403 Forbidden 問題，這是因為 <code>kube-apiserver</code> user 並沒有 nodes 的資源權限，屬於正常。</p></blockquote><p>由於上述權限問題，我們必需建立一個 <code>apiserver-to-kubelet-rbac.yml</code> 來定義權限，以供我們執行 logs、exec 等指令：</p><pre><code class="sh">$ cd /etc/kubernetes/$ export URL=&quot;https://kairen.github.io/files/manual-v1.8/master&quot;$ wget &quot;${URL}/apiserver-to-kubelet-rbac.yml.conf&quot; -O apiserver-to-kubelet-rbac.yml$ kubectl apply -f apiserver-to-kubelet-rbac.yml# 測試 logs$ kubectl -n kube-system logs -f kube-scheduler-master1...I1031 03:22:42.527697       1 leaderelection.go:184] successfully acquired lease kube-system/kube-scheduler</code></pre><h2 id="Kubernetes-Node"><a href="#Kubernetes-Node" class="headerlink" title="Kubernetes Node"></a>Kubernetes Node</h2><p>Node 是主要執行容器實例的節點，可視為工作節點。在這步驟我們會下載 Kubernetes binary 檔，並建立 node 的 certificate 來提供給節點註冊認證用。Kubernetes 使用<code>Node Authorizer</code>來提供<a href="https://kubernetes.io/docs/admin/authorization/node/" target="_blank" rel="noopener">Authorization mode</a>，這種授權模式會替 Kubelet 生成 API request。</p><p>在開始前，我們先在<code>master1</code>將需要的 ca 與 cert 複製到 Node 節點上：</p><pre><code class="sh">$ for NODE in node1 node2; do    ssh ${NODE} &quot;mkdir -p /etc/kubernetes/pki/&quot;    ssh ${NODE} &quot;mkdir -p /etc/etcd/ssl&quot;    # Etcd ca and cert    for FILE in etcd-ca.pem etcd.pem etcd-key.pem; do      scp /etc/etcd/ssl/${FILE} ${NODE}:/etc/etcd/ssl/${FILE}    done    # Kubernetes ca and cert    for FILE in pki/ca.pem pki/ca-key.pem bootstrap.conf; do      scp /etc/kubernetes/${FILE} ${NODE}:/etc/kubernetes/${FILE}    done  done</code></pre><h3 id="下載-Kubernetes-元件-1"><a href="#下載-Kubernetes-元件-1" class="headerlink" title="下載 Kubernetes 元件"></a>下載 Kubernetes 元件</h3><p>首先透過網路取得所有需要的執行檔案：</p><pre><code class="sh"># Download Kubernetes$ export KUBE_URL=&quot;https://storage.googleapis.com/kubernetes-release/release/v1.8.2/bin/linux/amd64&quot;$ wget &quot;${KUBE_URL}/kubelet&quot; -O /usr/local/bin/kubelet$ chmod +x /usr/local/bin/kubelet# Download CNI$ mkdir -p /opt/cni/bin &amp;&amp; cd /opt/cni/bin$ export CNI_URL=&quot;https://github.com/containernetworking/plugins/releases/download&quot;$ wget -qO- --show-progress &quot;${CNI_URL}/v0.6.0/cni-plugins-amd64-v0.6.0.tgz&quot; | tar -zx</code></pre><h3 id="設定-Kubernetes-node"><a href="#設定-Kubernetes-node" class="headerlink" title="設定 Kubernetes node"></a>設定 Kubernetes node</h3><p>接著下載 Kubernetes 相關檔案，包含 drop-in file、systemd service 檔案等：</p><pre><code class="sh">$ export KUBELET_URL=&quot;https://kairen.github.io/files/manual-v1.8/node&quot;$ mkdir -p /etc/systemd/system/kubelet.service.d$ wget &quot;${KUBELET_URL}/kubelet.service&quot; -O /lib/systemd/system/kubelet.service$ wget &quot;${KUBELET_URL}/10-kubelet.conf&quot; -O /etc/systemd/system/kubelet.service.d/10-kubelet.conf</code></pre><p>接著在所有<code>node</code>建立 var 存放資訊，然後啟動 kubelet 服務:</p><pre><code class="sh">$ mkdir -p /var/lib/kubelet /var/log/kubernetes /etc/kubernetes/manifests$ systemctl enable kubelet.service &amp;&amp; systemctl start kubelet.service</code></pre><blockquote><p>P.S. 重複一樣動作來完成其他節點。</p></blockquote><h3 id="授權-Kubernetes-Node"><a href="#授權-Kubernetes-Node" class="headerlink" title="授權 Kubernetes Node"></a>授權 Kubernetes Node</h3><p>當所有節點都完成後，在<code>master</code>節點，因為我們採用 TLS Bootstrapping，所需要建立一個 ClusterRoleBinding：</p><pre><code class="sh">$ kubectl create clusterrolebinding kubelet-bootstrap \    --clusterrole=system:node-bootstrapper \    --user=kubelet-bootstrap</code></pre><p>在<code>master</code>透過簡單指令驗證，會看到節點處於<code>pending</code>：</p><pre><code class="sh">$ kubectl get csrNAME                                                   AGE       REQUESTOR           CONDITIONnode-csr-YWf97ZrLCTlr2hmXsNLfjVLwaLfZRsu52FRKOYjpcBE   2s        kubelet-bootstrap   Pendingnode-csr-eq4q6ffOwT4yqYQNU6sT7mphPOQdFN6yulMVZeu6pkE   2s        kubelet-bootstrap   Pending</code></pre><p>透過 kubectl 來允許節點加入叢集：</p><pre><code class="sh">$ kubectl get csr | awk &#39;/Pending/ {print $1}&#39; | xargs kubectl certificate approvecertificatesigningrequest &quot;node-csr-YWf97ZrLCTlr2hmXsNLfjVLwaLfZRsu52FRKOYjpcBE&quot; approvedcertificatesigningrequest &quot;node-csr-eq4q6ffOwT4yqYQNU6sT7mphPOQdFN6yulMVZeu6pkE&quot; approved$ kubectl get csrNAME                                                   AGE       REQUESTOR           CONDITIONnode-csr-YWf97ZrLCTlr2hmXsNLfjVLwaLfZRsu52FRKOYjpcBE   30s       kubelet-bootstrap   Approved,Issuednode-csr-eq4q6ffOwT4yqYQNU6sT7mphPOQdFN6yulMVZeu6pkE   30s       kubelet-bootstrap   Approved,Issued$ kubectl get noNAME      STATUS     ROLES     AGE       VERSIONmaster1   NotReady   master    15m       v1.8.2node1     NotReady   &lt;none&gt;    8m        v1.8.2node2     NotReady   &lt;none&gt;    6s        v1.8.2</code></pre><h2 id="Kubernetes-Core-Addons-部署"><a href="#Kubernetes-Core-Addons-部署" class="headerlink" title="Kubernetes Core Addons 部署"></a>Kubernetes Core Addons 部署</h2><p>當完成上面所有步驟後，接著我們需要安裝一些插件，而這些有部分是非常重要跟好用的，如<code>Kube-dns</code>與<code>Kube-proxy</code>等。</p><h3 id="Kube-proxy-addon"><a href="#Kube-proxy-addon" class="headerlink" title="Kube-proxy addon"></a>Kube-proxy addon</h3><p><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/kube-proxy" target="_blank" rel="noopener">Kube-proxy</a> 是實現 Service 的關鍵元件，kube-proxy 會在每台節點上執行，然後監聽 API Server 的 Service 與 Endpoint 資源物件的改變，然後來依據變化執行 iptables 來實現網路的轉發。這邊我們會需要建議一個 DaemonSet 來執行，並且建立一些需要的 certificate。</p><p>首先在<code>master1</code>下載<code>kube-proxy-csr.json</code>檔案，並產生 kube-proxy certificate 證書：</p><pre><code class="sh">$ export PKI_URL=&quot;https://kairen.github.io/files/manual-v1.8/pki&quot;$ cd /etc/kubernetes/pki$ wget &quot;${PKI_URL}/kube-proxy-csr.json&quot; &quot;${PKI_URL}/ca-config.json&quot;$ cfssl gencert \  -ca=ca.pem \  -ca-key=ca-key.pem \  -config=ca-config.json \  -profile=kubernetes \  kube-proxy-csr.json | cfssljson -bare kube-proxy$ ls kube-proxy*.pemkube-proxy-key.pem  kube-proxy.pem</code></pre><p>接著透過以下指令產生名稱為 <code>kube-proxy.conf</code> 的 kubeconfig 檔：</p><pre><code class="sh"># kube-proxy set-cluster$ kubectl config set-cluster kubernetes \    --certificate-authority=ca.pem \    --embed-certs=true \    --server=&quot;https://172.16.35.12:6443&quot; \    --kubeconfig=../kube-proxy.conf# kube-proxy set-credentials$ kubectl config set-credentials system:kube-proxy \    --client-key=kube-proxy-key.pem \    --client-certificate=kube-proxy.pem \    --embed-certs=true \    --kubeconfig=../kube-proxy.conf# kube-proxy set-context$ kubectl config set-context system:kube-proxy@kubernetes \    --cluster=kubernetes \    --user=system:kube-proxy \    --kubeconfig=../kube-proxy.conf# kube-proxy set default context$ kubectl config use-context system:kube-proxy@kubernetes \    --kubeconfig=../kube-proxy.conf</code></pre><p>完成後刪除不必要檔案：</p><pre><code class="sh">$ rm -rf *.json</code></pre><p>確認<code>/etc/kubernetes</code>有以下檔案：</p><pre><code class="sh">$ ls /etc/kubernetes/admin.conf        bootstrap.conf           encryption.yml  kube-proxy.conf  pki             token.csvaudit-policy.yml  controller-manager.conf  kubelet.conf    manifests        scheduler.conf</code></pre><p>在<code>master1</code>將<code>kube-proxy</code>相關檔案複製到 Node 節點上：</p><pre><code class="sh">$ for NODE in node1 node2; do    for FILE in pki/kube-proxy.pem pki/kube-proxy-key.pem kube-proxy.conf; do      scp /etc/kubernetes/${FILE} ${NODE}:/etc/kubernetes/${FILE}    done  done</code></pre><p>完成後，在<code>master1</code>透過 kubectl 來建立 kube-proxy daemon：</p><pre><code class="sh">$ export ADDON_URL=&quot;https://kairen.github.io/files/manual-v1.8/addon&quot;$ mkdir -p /etc/kubernetes/addons &amp;&amp; cd /etc/kubernetes/addons$ wget &quot;${ADDON_URL}/kube-proxy.yml.conf&quot; -O kube-proxy.yml$ kubectl apply -f kube-proxy.yml$ kubectl -n kube-system get po -l k8s-app=kube-proxyNAME               READY     STATUS    RESTARTS   AGEkube-proxy-bpp7q   1/1       Running   0          47skube-proxy-cztvh   1/1       Running   0          47skube-proxy-q7mm4   1/1       Running   0          47s</code></pre><h3 id="Kube-dns-addon"><a href="#Kube-dns-addon" class="headerlink" title="Kube-dns addon"></a>Kube-dns addon</h3><p><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns" target="_blank" rel="noopener">Kube DNS</a> 是 Kubernetes 叢集內部 Pod 之間互相溝通的重要 Addon，它允許 Pod 可以透過 Domain Name 方式來連接 Service，其主要由 Kube DNS 與 Sky DNS 組合而成，透過 Kube DNS 監聽 Service 與 Endpoint 變化，來提供給 Sky DNS 資訊，已更新解析位址。</p><p>安裝只需要在<code>master1</code>透過 kubectl 來建立 kube-dns deployment 即可：</p><pre><code class="sh">$ export ADDON_URL=&quot;https://kairen.github.io/files/manual-v1.8/addon&quot;$ wget &quot;${ADDON_URL}/kube-dns.yml.conf&quot; -O kube-dns.yml$ kubectl apply -f kube-dns.yml$ kubectl -n kube-system get po -l k8s-app=kube-dnsNAME                        READY     STATUS    RESTARTS   AGEkube-dns-6cb549f55f-h4zr5   0/3       Pending   0          40s</code></pre><h2 id="Calico-Network-安裝與設定"><a href="#Calico-Network-安裝與設定" class="headerlink" title="Calico Network 安裝與設定"></a>Calico Network 安裝與設定</h2><p>Calico 是一款純 Layer 3 的資料中心網路方案(不需要 Overlay 網路)，Calico 好處是他已與各種雲原生平台有良好的整合，而 Calico 在每一個節點利用 Linux Kernel 實現高效的 vRouter 來負責資料的轉發，而當資料中心複雜度增加時，可以用 BGP route reflector 來達成。</p><p>首先在<code>master1</code>透過 kubectl 建立 Calico policy controller：</p><pre><code class="sh">$ export CALICO_CONF_URL=&quot;https://kairen.github.io/files/manual-v1.8/network&quot;$ wget &quot;${CALICO_CONF_URL}/calico-controller.yml.conf&quot; -O calico-controller.yml$ kubectl apply -f calico-controller.yml$ kubectl -n kube-system get po -l k8s-app=calico-policyNAME                                        READY     STATUS    RESTARTS   AGEcalico-policy-controller-5ff8b4549d-tctmm   0/1       Pending   0          5s</code></pre><blockquote><p>若節點 IP 不同，需要修改<code>calico-controller.yml</code>的<code>ETCD_ENDPOINTS</code>。</p></blockquote><p>在<code>master1</code>下載 Calico CLI 工具：</p><pre><code class="sh">$ wget https://github.com/projectcalico/calicoctl/releases/download/v1.6.1/calicoctl$ chmod +x calicoctl &amp;&amp; mv calicoctl /usr/local/bin/</code></pre><p>然後在<code>所有</code>節點下載 Calico，並執行以下步驟：</p><pre><code class="sh">$ export CALICO_URL=&quot;https://github.com/projectcalico/cni-plugin/releases/download/v1.11.0&quot;$ wget -N -P /opt/cni/bin ${CALICO_URL}/calico$ wget -N -P /opt/cni/bin ${CALICO_URL}/calico-ipam$ chmod +x /opt/cni/bin/calico /opt/cni/bin/calico-ipam</code></pre><p>接著在<code>所有</code>節點下載 CNI plugins設定檔，以及 calico-node.service：</p><pre><code class="sh">$ mkdir -p /etc/cni/net.d$ export CALICO_CONF_URL=&quot;https://kairen.github.io/files/manual-v1.8/network&quot;$ wget &quot;${CALICO_CONF_URL}/10-calico.conf&quot; -O /etc/cni/net.d/10-calico.conf$ wget &quot;${CALICO_CONF_URL}/calico-node.service&quot; -O /lib/systemd/system/calico-node.service</code></pre><blockquote><blockquote><p>若節點 IP 不同，需要修改<code>10-calico.conf</code>的<code>etcd_endpoints</code>。</p><ul><li>若部署的機器是使用虛擬機，如 Virtualbox 等的話，請修改<code>calico-node.service</code>檔案，並在<code>IP_AUTODETECTION_METHOD</code>(包含 IP6)部分指定綁定的網卡，以避免預設綁定到 NAT 網路上。</li></ul></blockquote></blockquote><p>之後在<code>所有</code>節點啟動 Calico-node:</p><pre><code class="sh">$ systemctl enable calico-node.service &amp;&amp; systemctl start calico-node.service</code></pre><p>在<code>master1</code>查看 Calico nodes:</p><pre><code class="sh">$ cat &lt;&lt;EOF &gt; ~/calico-rcexport ETCD_ENDPOINTS=&quot;https://172.16.35.12:2379&quot;export ETCD_CA_CERT_FILE=&quot;/etc/etcd/ssl/etcd-ca.pem&quot;export ETCD_CERT_FILE=&quot;/etc/etcd/ssl/etcd.pem&quot;export ETCD_KEY_FILE=&quot;/etc/etcd/ssl/etcd-key.pem&quot;EOF$ . ~/calico-rc$ calicoctl get node -o wideNAME      ASN       IPV4              IPV6master1   (64512)   172.16.35.12/24node1     (64512)   172.16.35.10/24node2     (64512)   172.16.35.11/24</code></pre><p>查看 pending 的 pod 是否已執行：</p><pre><code class="sh">$ kubectl -n kube-system get poNAME                                        READY     STATUS    RESTARTS   AGEcalico-policy-controller-5ff8b4549d-tctmm   1/1       Running   0          4mkube-apiserver-master1                      1/1       Running   0          20mkube-controller-manager-master1             1/1       Running   0          20mkube-dns-6cb549f55f-h4zr5                   3/3       Running   0          5mkube-proxy-fnrkb                            1/1       Running   0          6mkube-proxy-l72bq                            1/1       Running   0          6mkube-proxy-m6rfw                            1/1       Running   0          6mkube-scheduler-master1                      1/1       Running   0          20m</code></pre><p>最後若想省事，可以直接用 <a href="https://docs.projectcalico.org/v2.6/getting-started/kubernetes/installation/hosted/hosted" target="_blank" rel="noopener">Standard Hosted</a> 方式安裝。</p><h2 id="Kubernetes-Extra-Addons-部署"><a href="#Kubernetes-Extra-Addons-部署" class="headerlink" title="Kubernetes Extra Addons 部署"></a>Kubernetes Extra Addons 部署</h2><p>本節說明如何部署一些官方常用的 Addons，如 Dashboard、Heapster 等。</p><h3 id="Dashboard-addon"><a href="#Dashboard-addon" class="headerlink" title="Dashboard addon"></a>Dashboard addon</h3><p><a href="https://github.com/kubernetes/dashboard" target="_blank" rel="noopener">Dashboard</a> 是 Kubernetes 社區官方開發的儀表板，有了儀表板後管理者就能夠透過 Web-based 方式來管理 Kubernetes 叢集，除了提升管理方便，也讓資源視覺化，讓人更直覺看見系統資訊的呈現結果。</p><p>首先我們要建立<code>kubernetes-dashboard-certs</code>，來提供給 Dashboard TLS 使用：</p><pre><code class="sh">$ mkdir -p /etc/kubernetes/addons/certs &amp;&amp; cd /etc/kubernetes/addons$ openssl genrsa -des3 -passout pass:x -out certs/dashboard.pass.key 2048$ openssl rsa -passin pass:x -in certs/dashboard.pass.key -out certs/dashboard.key$ openssl req -new -key certs/dashboard.key -out certs/dashboard.csr -subj &#39;/CN=kube-dashboard&#39;$ openssl x509 -req -sha256 -days 365 -in certs/dashboard.csr -signkey certs/dashboard.key -out certs/dashboard.crt$ rm certs/dashboard.pass.key$ kubectl create secret generic kubernetes-dashboard-certs\    --from-file=certs -n kube-system</code></pre><p>接著在<code>master1</code>透過 kubectl 來建立 kubernetes dashboard 即可：</p><pre><code class="sh">$ export ADDON_URL=&quot;https://kairen.github.io/files/manual-v1.8/addon&quot;$ wget ${ADDON_URL}/kube-dashboard.yml.conf -O kube-dashboard.yml$ kubectl apply -f kube-dashboard.yml$ kubectl -n kube-system get po,svc -l k8s-app=kubernetes-dashboardNAME                                      READY     STATUS    RESTARTS   AGEpo/kubernetes-dashboard-747c4f7cf-md5m8   1/1       Running   0          56sNAME                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGEsvc/kubernetes-dashboard   ClusterIP   10.98.120.209   &lt;none&gt;        443/TCP   56s</code></pre><blockquote><p>P.S. 這邊會額外建立一個名稱為<code>anonymous-open-door</code> Cluster Role Binding，這僅作為方便測試時使用，在一般情況下不要開啟，不然就會直接被存取所有 API。</p></blockquote><p>完成後，就可以透過瀏覽器存取 <a href="https://172.16.35.12:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/" target="_blank" rel="noopener">Dashboard</a>。</p><h3 id="Heapster-addon"><a href="#Heapster-addon" class="headerlink" title="Heapster addon"></a>Heapster addon</h3><p><a href="https://github.com/kubernetes/heapster" target="_blank" rel="noopener">Heapster</a> 是 Kubernetes 社區維護的容器叢集監控與效能分析工具。Heapster 會從 Kubernetes apiserver 取得所有 Node 資訊，然後再透過這些 Node 來取得 kubelet 上的資料，最後再將所有收集到資料送到 Heapster 的後台儲存 InfluxDB，最後利用 Grafana 來抓取 InfluxDB 的資料源來進行視覺化。</p><p>在<code>master1</code>透過 kubectl 來建立 kubernetes monitor  即可：</p><pre><code class="sh">$ export ADDON_URL=&quot;https://kairen.github.io/files/manual-v1.8/addon&quot;$ wget ${ADDON_URL}/kube-monitor.yml.conf -O kube-monitor.yml$ kubectl apply -f kube-monitor.yml$ kubectl -n kube-system get po,svcNAME                                           READY     STATUS    RESTARTS   AGE...po/heapster-74fb5c8cdc-62xzc                   4/4       Running   0          7mpo/influxdb-grafana-55bd7df44-nw4nc            2/2       Running   0          7mNAME                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE...svc/heapster               ClusterIP   10.100.242.225   &lt;none&gt;        80/TCP              7msvc/monitoring-grafana     ClusterIP   10.101.106.180   &lt;none&gt;        80/TCP              7msvc/monitoring-influxdb    ClusterIP   10.109.245.142   &lt;none&gt;        8083/TCP,8086/TCP   7m···</code></pre><p>完成後，就可以透過瀏覽器存取 <a href="https://172.16.35.12:6443/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana" target="_blank" rel="noopener">Grafana Dashboard</a>。</p><h2 id="簡單部署-Nginx-服務"><a href="#簡單部署-Nginx-服務" class="headerlink" title="簡單部署 Nginx 服務"></a>簡單部署 Nginx 服務</h2><p>Kubernetes 可以選擇使用指令直接建立應用程式與服務，或者撰寫 YAML 與 JSON 檔案來描述部署應用程式的配置，以下將建立一個簡單的 Nginx 服務：</p><pre><code class="sh">$ kubectl run nginx --image=nginx --port=80$ kubectl expose deploy nginx --port=80 --type=LoadBalancer --external-ip=172.16.35.12$ kubectl get svc,poNAME             TYPE           CLUSTER-IP      EXTERNAL-IP    PORT(S)        AGEsvc/kubernetes   ClusterIP      10.96.0.1       &lt;none&gt;         443/TCP        1hsvc/nginx        LoadBalancer   10.97.121.243   172.16.35.12   80:30344/TCP   22sNAME                        READY     STATUS    RESTARTS   AGEpo/nginx-7cbc4b4d9c-7796l   1/1       Running   0          28s       192.160.57.181   ,172.16.35.12   80:32054/TCP   21s</code></pre><blockquote><p>這邊<code>type</code>可以選擇 NodePort 與 LoadBalancer，在本地裸機部署，兩者差異在於<code>NodePort</code>只映射 Host port 到 Container port，而<code>LoadBalancer</code>則繼承<code>NodePort</code>額外多出映射 Host target port 到 Container port。</p></blockquote><p>確認沒問題後即可在瀏覽器存取 <a href="http://172.16.35.12。" target="_blank" rel="noopener">http://172.16.35.12。</a></p><h3 id="擴展服務數量"><a href="#擴展服務數量" class="headerlink" title="擴展服務數量"></a>擴展服務數量</h3><p>若叢集<code>node</code>節點增加了，而想讓 Nginx 服務提供可靠性的話，可以透過以下方式來擴展服務的副本：</p><pre><code class="sh">$ kubectl scale deploy nginx --replicas=2$ kubectl get pods -o wideNAME                    READY     STATUS    RESTARTS   AGE       IP             NODEnginx-158599303-0h9lr   1/1       Running   0          25s       10.244.100.5   node2nginx-158599303-k7cbt   1/1       Running   0          1m        10.244.24.3    node1</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Kubernetes 提供了許多雲端平台與作業系統的安裝方式，本章將以&lt;code&gt;全手動安裝方式&lt;/code&gt;來部署，主要是學習與了解 Kubernetes 建置流程。若想要瞭解更多平台的部署可以參考 &lt;a href=&quot;https://kubernetes.io/docs/getting-started-guides/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Picking the Right Solution&lt;/a&gt;來選擇自己最喜歡的方式。&lt;/p&gt;
&lt;p&gt;本次安裝版本為：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes v1.8.2&lt;/li&gt;
&lt;li&gt;Etcd v3.2.9&lt;/li&gt;
&lt;li&gt;Calico v2.6.2&lt;/li&gt;
&lt;li&gt;Docker v17.10.0-ce&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://kairen.github.io/categories/Kubernetes/"/>
    
    
      <category term="Docker" scheme="https://kairen.github.io/tags/Docker/"/>
    
      <category term="Kubernetes" scheme="https://kairen.github.io/tags/Kubernetes/"/>
    
      <category term="Calico" scheme="https://kairen.github.io/tags/Calico/"/>
    
  </entry>
  
  <entry>
    <title>利用 Kuryr 整合 OpenStack 與 Kubernetes 網路</title>
    <link href="https://kairen.github.io/2017/08/29/openstack/kuryr-kubernetes/"/>
    <id>https://kairen.github.io/2017/08/29/openstack/kuryr-kubernetes/</id>
    <published>2017-08-29T08:23:01.000Z</published>
    <updated>2017-10-26T14:15:10.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/openstack/kuryr-kubernetes" target="_blank" rel="noopener">Kubernetes Kuryr</a> 是 OpenStack Neutron 的子專案，其主要目標是透過該專案來整合 OpenStack 與 Kubernetes 的網路。該專案在 Kubernetes 中實作了原生 Neutron-based 的網路，因此使用 Kuryr-Kubernetes 可以讓你的 OpenStack VM 與 Kubernetes Pods 能夠選擇在同一個子網路上運作，並且能夠使用 Neutron 的 L3 與 Security Group 來對網路進行路由，以及阻擋特定來源 Port。</p><p><img src="https://i.imgur.com/2XfP3vb.png" alt=""></p><a id="more"></a><p>Kuryr-Kubernetes 整合有兩個主要組成部分：</p><ol><li><strong>Kuryr Controller</strong>:<br>Controller 主要目的是監控 Kubernetes API 的來獲取 Kubernetes 資源的變化，然後依據 Kubernetes 資源的需求來執行子資源的分配和資源管理。</li><li><strong>Kuryr CNI</strong>：主要是依據 Kuryr Controller 分配的資源來綁定網路至 Pods 上。</li></ol><p>本篇我們將說明如何利用<code>DevStack</code>與<code>Kubespray</code>建立一個簡單的測試環境。</p><h2 id="環境資源與事前準備"><a href="#環境資源與事前準備" class="headerlink" title="環境資源與事前準備"></a>環境資源與事前準備</h2><p>準備兩台實體機器，這邊測試的作業系統為<code>CentOS 7.x</code>，該環境將在扁平的網路下進行。</p><table><thead><tr><th>IP Address 1</th><th>Role</th></tr></thead><tbody><tr><td>172.24.0.34</td><td>controller, k8s-master</td></tr><tr><td>172.24.0.80</td><td>compute, k8s-node1</td></tr></tbody></table><p>更新每台節點的 CentOS 7.x packages:</p><pre><code class="shell=">$ sudo yum --enablerepo=cr update -y</code></pre><p>然後關閉 firewalld 以及 SELinux 來避免實現發生問題：</p><pre><code class="shell=">$ sudo setenforce 0$ sudo systemctl disable firewalld &amp;&amp; sudo systemctl stop firewalld</code></pre><h2 id="OpenStack-Controller-安裝"><a href="#OpenStack-Controller-安裝" class="headerlink" title="OpenStack Controller 安裝"></a>OpenStack Controller 安裝</h2><p>首先進入<code>172.24.0.34（controller）</code>，並且執行以下指令。</p><p>然後執行以下指令來建立 DevStack 專用使用者：</p><pre><code class="shell=">$ sudo useradd -s /bin/bash -d /opt/stack -m stack$ echo &quot;stack ALL=(ALL) NOPASSWD: ALL&quot; | sudo tee /etc/sudoers.d/stack</code></pre><blockquote><p>選用 DevStack 是因為現在都是用 Systemd 來管理服務，不用再用 screen 了，雖然都很方便。</p></blockquote><p>接著切換至該使用者環境來建立 OpenStack：</p><pre><code class="shell=">$ sudo su - stack</code></pre><p>下載 DevStack 安裝套件：</p><pre><code class="shell=">$ git clone https://git.openstack.org/openstack-dev/devstack$ cd devstack</code></pre><p>新增<code>local.conf</code>檔案，來描述部署資訊：</p><pre><code>[[local|localrc]]HOST_IP=172.24.0.34GIT_BASE=https://github.comADMIN_PASSWORD=passwdDATABASE_PASSWORD=passwdRABBIT_PASSWORD=passwdSERVICE_PASSWORD=passwdSERVICE_TOKEN=passwdMULTI_HOST=1</code></pre><blockquote><p>[color=#fc9fca]Tips:<br>修改 HOST_IP 為自己的 IP 位置。</p></blockquote><p>完成後，執行以下指令開始部署：</p><pre><code class="shell=">$ ./stack.sh</code></pre><h2 id="Openstack-Compute-安裝"><a href="#Openstack-Compute-安裝" class="headerlink" title="Openstack Compute 安裝"></a>Openstack Compute 安裝</h2><p>進入到<code>172.24.0.80（compute）</code>，並且執行以下指令。</p><p>然後執行以下指令來建立 DevStack 專用使用者：</p><pre><code class="shell=">$ sudo useradd -s /bin/bash -d /opt/stack -m stack$ echo &quot;stack ALL=(ALL) NOPASSWD: ALL&quot; | sudo tee /etc/sudoers.d/stack</code></pre><blockquote><p>選用 DevStack 是因為現在都是用 Systemd 來管理服務，不用再用 screen 了，雖然都很方便。</p></blockquote><p>接著切換至該使用者環境來建立 OpenStack：</p><pre><code class="shell=">$ sudo su - stack</code></pre><p>下載 DevStack 安裝套件：</p><pre><code class="shell=">$ git clone https://git.openstack.org/openstack-dev/devstack$ cd devstack</code></pre><p>新增<code>local.conf</code>檔案，來描述部署資訊：</p><pre><code>[[local|localrc]]HOST_IP=172.24.0.80GIT_BASE=https://github.comMULTI_HOST=1LOGFILE=/opt/stack/logs/stack.sh.logADMIN_PASSWORD=passwdDATABASE_PASSWORD=passwdRABBIT_PASSWORD=passwdSERVICE_PASSWORD=passwdDATABASE_TYPE=mysqlSERVICE_HOST=172.24.0.34MYSQL_HOST=$SERVICE_HOSTRABBIT_HOST=$SERVICE_HOSTGLANCE_HOSTPORT=$SERVICE_HOST:9292ENABLED_SERVICES=n-cpu,q-agt,n-api-meta,c-vol,placement-clientNOVA_VNC_ENABLED=TrueNOVNCPROXY_URL=&quot;http://$SERVICE_HOST:6080/vnc_auto.html&quot;VNCSERVER_LISTEN=$HOST_IPVNCSERVER_PROXYCLIENT_ADDRESS=$VNCSERVER_LISTEN</code></pre><blockquote><p>Tips:<br>修改 HOST_IP 為自己的主機位置。<br>修改 SERVICE_HOST 為 Master 的IP位置。</p></blockquote><p>完成後，執行以下指令開始部署：</p><pre><code class="shell=">$ ./stack.sh</code></pre><h2 id="建立-Kubernetes-叢集環境"><a href="#建立-Kubernetes-叢集環境" class="headerlink" title="建立 Kubernetes 叢集環境"></a>建立 Kubernetes 叢集環境</h2><p>首先確認所有節點之間不需要 SSH 密碼即可登入，接著進入到<code>172.24.0.34（k8s-master）</code>並且執行以下指令。</p><p>接著安裝所需要的套件：</p><pre><code class="shell=">$ sudo yum -y install software-properties-common ansible git gcc python-pip python-devel libffi-devel openssl-devel$ sudo pip install -U kubespray</code></pre><p>完成後，新增 kubespray 設定檔：</p><pre><code class="shell=">$ cat &lt;&lt;EOF &gt;  ~/.kubespray.ymlkubespray_git_repo: &quot;https://github.com/kubernetes-incubator/kubespray.git&quot;# Logging optionsloglevel: &quot;info&quot;EOF</code></pre><p>然後利用 kubespray-cli 快速產生環境的<code>inventory</code>檔，並修改部分內容：</p><pre><code class="shell=">$ sudo -i$ kubespray prepare --masters master --etcds master --nodes node1</code></pre><p>編輯<code>/root/.kubespray/inventory/inventory.cfg</code>，修改以下內容：</p><pre><code>[all]master  ansible_host=172.24.0.34 ansible_user=root ip=172.24.0.34node1    ansible_host=172.24.0.80 ansible_user=root ip=172.24.0.80[kube-master]master[kube-node]masternode1[etcd]master[k8s-cluster:children]kube-node1kube-master</code></pre><p>完成後，即可利用 kubespray-cli 指令來進行部署：</p><pre><code class="shell=">$ kubespray deploy --verbose -u root -k .ssh/id_rsa -n calico</code></pre><p>經過一段時間後就會部署完成，這時候檢查節點是否正常：</p><pre><code class="shell=">$ kubectl get noNAME      STATUS         AGE       VERSIONmaster    Ready,master   2m        v1.7.4node1     Ready          2m        v1.7.4</code></pre><p>接著為了方便讓 Kuryr Controller 簡單取得 K8s API Server，這邊修改<code>/etc/kubernetes/manifests/kube-apiserver.yml</code>檔案，加入以下內容：</p><pre><code>- &quot;--insecure-bind-address=0.0.0.0&quot;- &quot;--insecure-port=8080&quot;</code></pre><blockquote><p>Tips:<br>將 insecure 綁定到 0.0.0.0 之上，以及開啟 8080 Port。</p></blockquote><h2 id="安裝-Openstack-Kuryr"><a href="#安裝-Openstack-Kuryr" class="headerlink" title="安裝 Openstack Kuryr"></a>安裝 Openstack Kuryr</h2><p>進入到<code>172.24.0.34（controller）</code>，並且執行以下指令。</p><p>首先在節點安裝所需要的套件：</p><pre><code class="shell=">$ sudo yum -y install  gcc libffi-devel python-devel openssl-devel install python-pip</code></pre><p>然後下載 kuryr-kubernetes 並進行安裝：</p><pre><code class="shell=">$ git clone http://git.openstack.org/openstack/kuryr-kubernetes$ pip install -e kuryr-kubernetes</code></pre><p>新增<code>kuryr.conf</code>至<code>/etc/kuryr</code>目錄：</p><pre><code class="shell=">$ cd kuryr-kubernetes$ ./tools/generate_config_file_samples.sh$ sudo mkdir -p /etc/kuryr/$ sudo cp etc/kuryr.conf.sample /etc/kuryr/kuryr.conf</code></pre><p>接著使用 OpenStack Dashboard 建立相關專案，在瀏覽器輸入<a href="http://172.24.0.34" target="_blank" rel="noopener">Dashboard</a>，並執行以下步驟。</p><ol><li>新增 K8s project。</li><li>修改 K8s project member 加入到 service project。</li><li>在該 Project 中新增 Security Groups，參考 <a href="https://docs.openstack.org/kuryr-kubernetes/latest/installation/manual.html" target="_blank" rel="noopener">kuryr-kubernetes manually</a>。</li><li>在該 Project 中新增 pod_subnet 子網路。</li><li>在該 Project 中新增 service_subnet 子網路。</li></ol><p>完成後，修改<code>/etc/kuryr/kuryr.conf</code>檔案，加入以下內容：</p><pre><code>[DEFAULT]use_stderr = truebindir = /usr/local/libexec/kuryr[kubernetes]api_root = http://172.24.0.34:8080[neutron]auth_url = http://172.24.0.34/identityusername = adminuser_domain_name = Defaultpassword = adminproject_name = serviceproject_domain_name = Defaultauth_type = password[neutron_defaults]ovs_bridge = br-intpod_security_groups = {id_of_secuirity_group_for_pods}pod_subnet = {id_of_subnet_for_pods}project = {id_of_project}service_subnet = {id_of_subnet_for_k8s_services}</code></pre><p>完成後執行 kuryr-k8s-controller：</p><pre><code class="shell=">$ kuryr-k8s-controller --config-file /etc/kuryr/kuryr.conf</code></pre><h2 id="安裝-Kuryr-CNI"><a href="#安裝-Kuryr-CNI" class="headerlink" title="安裝 Kuryr-CNI"></a>安裝 Kuryr-CNI</h2><p>進入到<code>172.24.0.80（node1）</code>並且執行以下指令。</p><p>首先在節點安裝所需要的套件：</p><pre><code class="shell=">$ sudo yum -y install  gcc libffi-devel python-devel openssl-devel python-pip</code></pre><p>然後安裝 Kuryr-CNI 來提供給 kubelet 使用：</p><pre><code class="shell=">$ git clone http://git.openstack.org/openstack/kuryr-kubernetes$ sudo pip install -e kuryr-kubernetes</code></pre><p>新增<code>kuryr.conf</code>至<code>/etc/kuryr</code>目錄：</p><pre><code class="shell=">$ cd kuryr-kubernetes$ ./tools/generate_config_file_samples.sh$ sudo mkdir -p /etc/kuryr/$ sudo cp etc/kuryr.conf.sample /etc/kuryr/kuryr.conf</code></pre><p>修改<code>/etc/kuryr/kuryr.conf</code>檔案，加入以下內容：</p><pre><code>[DEFAULT]use_stderr = truebindir = /usr/local/libexec/kuryr[kubernetes]api_root = http://172.24.0.34:8080</code></pre><p>建立 CNI bin 與 Conf 目錄：</p><pre><code class="shell=">$ sudo mkdir -p /opt/cni/bin$ sudo ln -s $(which kuryr-cni) /opt/cni/bin/$ sudo mkdir -p /etc/cni/net.d/</code></pre><p>新增<code>/etc/cni/net.d/10-kuryr.conf</code> CNI 設定檔：</p><pre><code>{    &quot;cniVersion&quot;: &quot;0.3.0&quot;,    &quot;name&quot;: &quot;kuryr&quot;,    &quot;type&quot;: &quot;kuryr-cni&quot;,    &quot;kuryr_conf&quot;: &quot;/etc/kuryr/kuryr.conf&quot;,    &quot;debug&quot;: true}</code></pre><p>完成後，更新 oslo 與 vif python 函式庫：</p><pre><code class="shell=">$ sudo pip install &#39;oslo.privsep&gt;=1.20.0&#39; &#39;os-vif&gt;=1.5.0&#39;</code></pre><p>最後重新啟動相關服務：</p><pre><code>sudo systemctl daemon-reload &amp;&amp; systemctl restart kubelet.service</code></pre><h2 id="測試結果"><a href="#測試結果" class="headerlink" title="測試結果"></a>測試結果</h2><p>我們這邊開一個 Pod 與 OpenStack VM 來進行溝通：<br><img src="https://i.imgur.com/UYXdKud.png" alt=""></p><p><img src="https://i.imgur.com/dwoEytW.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://github.com/openstack/kuryr-kubernetes&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Kubernetes Kuryr&lt;/a&gt; 是 OpenStack Neutron 的子專案，其主要目標是透過該專案來整合 OpenStack 與 Kubernetes 的網路。該專案在 Kubernetes 中實作了原生 Neutron-based 的網路，因此使用 Kuryr-Kubernetes 可以讓你的 OpenStack VM 與 Kubernetes Pods 能夠選擇在同一個子網路上運作，並且能夠使用 Neutron 的 L3 與 Security Group 來對網路進行路由，以及阻擋特定來源 Port。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/2XfP3vb.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="OpenStack" scheme="https://kairen.github.io/categories/OpenStack/"/>
    
    
      <category term="Docker" scheme="https://kairen.github.io/tags/Docker/"/>
    
      <category term="OpenStack" scheme="https://kairen.github.io/tags/OpenStack/"/>
    
      <category term="Kubernetes" scheme="https://kairen.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>利用 OpenStack Ironic 提供裸機部署服務</title>
    <link href="https://kairen.github.io/2017/08/16/openstack/ironic-dev/"/>
    <id>https://kairen.github.io/2017/08/16/openstack/ironic-dev/</id>
    <published>2017-08-16T08:23:01.000Z</published>
    <updated>2017-08-24T16:42:08.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://docs.openstack.org/ironic/latest/user/index.html" target="_blank" rel="noopener">Ironic</a> 是 OpenStack 專案之一，主要目的是提供裸機機器部署服務(Bare-metal service)。它能夠單獨或整合 OpenStack 其他服務被使用，而可整合服務包含 Keystone、Nova、Neutron、Glance 與 Swift 等核心服務。當使用 Compute 與 Network 服務對 Bare-metal 進行適當的配置時，OpenStack 可以透過 Compute API 同時部署虛擬機(Virtual machines)與裸機(Bare machines)。</p><p>本篇為了精簡安裝過程，故這邊不採用手動安裝教學(會在 Gitbook 書上更新)，因此採用 <a href="https://docs.openstack.org/devstack/latest/" target="_blank" rel="noopener">DevStack</a> 來部署服務，再手動設定一些步驟。</p><p>本環境安裝資訊：</p><ul><li>OpenStack Pike</li><li>DevStack Pike</li><li>Pike Pike Pike ….</li></ul><a id="more"></a><p><img src="/images/openstack/openstack-ironic.png" alt=""></p><blockquote><p>P.S. 這邊因為我的 Manage net 已經有 MAAS 的服務，所以才用其他張網卡進行部署。</p></blockquote><h2 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h2><p>本次安裝作業系統採用<code>Ubuntu 16.04 Server</code>，測試環境為實體主機：</p><table><thead><tr><th>Role</th><th>CPU</th><th>Memory</th></tr></thead><tbody><tr><td>controller</td><td>4</td><td>16G</td></tr><tr><td>bare-node1</td><td>4</td><td>16G</td></tr></tbody></table><blockquote><p>這邊 controller 為主要控制節點，將安裝大部分 OpenStack 服務。而 bare-node 為被用來做裸機部署的機器。</p></blockquote><p>網卡若是實體主機，請設定為固定 IP，如以下：</p><pre><code>auto eth0iface eth0 inet static           address 172.20.3.93/24           gateway    172.20.3.1           dns-nameservers 8.8.8.8</code></pre><blockquote><p>若想修改主機的網卡名稱，可以編輯<code>/etc/udev/rules.d/70-persistent-net.rules</code>。</p></blockquote><p>其中<code>controller</code>的<code>eth2</code>需設定為以下：</p><pre><code>auto &lt;ethx&gt;iface &lt;ethx&gt; inet manual        up ip link set dev $IFACE up        down ip link set dev $IFACE down</code></pre><h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>安裝前需要確認叢集滿足以下幾點：</p><ul><li>確認所有節點網路可以溝通。</li><li>Bare-node IPMI 設定完成。包含 Address、User 與 Password。</li><li>修改 Controller 的 <code>/etc/apt/sources.list</code>，使用<code>tw.archive.ubuntu.com</code>。</li></ul><h2 id="安裝-OpenStack-服務"><a href="#安裝-OpenStack-服務" class="headerlink" title="安裝 OpenStack 服務"></a>安裝 OpenStack 服務</h2><p>這邊採用 DevStack 來部署測試環境，首先透過以下指令取得 DevStack：</p><pre><code class="sh">$ sudo useradd -s /bin/bash -d /opt/stack -m stack$ echo &quot;stack ALL=(ALL) NOPASSWD: ALL&quot; | sudo tee /etc/sudoers.d/stack$ sudo su - stack$ git clone https://git.openstack.org/openstack-dev/devstack$ cd devstack</code></pre><p>接著撰寫 local.conf 來描述部署過程所需的服務：</p><pre><code class="sh">$ wget https://kairen.github.io/files/devstack/ironic-local.conf -O local.conf$ sed -i &#39;s/HOST_IP=.*/HOST_IP=172.22.132.93/g&#39; local.conf</code></pre><blockquote><p><code>HOST_IP</code>請更換為自己環境 IP。有其他 Driver 請記得加入。</p></blockquote><p>完成後執行部署腳本進行建置：</p><pre><code class="sh">$ ./stack.sh</code></pre><blockquote><p>大約經過 15 min 就可以完成整個環境安裝。</p></blockquote><p>測試 OpenStack 環境：</p><pre><code class="sh">$ source openrc admin$ openstack user list+----------------------------------+----------------+| ID                               | Name           |+----------------------------------+----------------+| 3ba4e813270e4e98ad781f4103284e0d | demo           || 40c6014bc18f407fbfbc22aadedb1ca0 | placement      || 567156ad1c7b4ccdbcd4ea02e7c44ce3 | alt_demo       || 7a22ce5036614993a707dd976c505ccd | swift          || 8d392f051afe45008289abca4dadf3ca | swiftusertest1 || a6e616af3bf04611bc23625e71a22e64 | swiftusertest4 || a835f1674648427396a7c6ac7e5eef06 | neutron        || b2bf73ef2eaa425c93e4f552e9266056 | swiftusertest2 || b7de1af8522b495c8a9fb743eb6e7f59 | nova           || cada5913a03e4f2794066902144264d3 | admin          || f03e39680b234474b139d00c3fbca989 | swiftusertest3 || f0a4033463f64c00858ff05525545b6d | glance-swift   || f2a1b186e7e84b10ae7e8f810e5c2412 | glance         || ff31787d136f4fba96c19af419b8559c | ironic         |+----------------------------------+----------------+</code></pre><p>測試 ironic 是否正常運行：</p><pre><code class="sh">$ ironic driver-list+---------------------+----------------+| Supported driver(s) | Active host(s) |+---------------------+----------------+| agent_ipmitool      | ironic-dev     || fake                | ironic-dev     || ipmi                | ironic-dev     || pxe_ipmitool        | ironic-dev     |+---------------------+----------------+</code></pre><h3 id="建立-Bare-metal-網路"><a href="#建立-Bare-metal-網路" class="headerlink" title="建立 Bare metal 網路"></a>建立 Bare metal 網路</h3><p>首先我們需要設定一個網路來提供 DHCP, PXE 與其他需求使用，這部分會說明如何建立一個 Flat network 來提供裸機配置用。詳細可參考 <a href="https://docs.openstack.org/ironic/latest/install/configure-networking.html" target="_blank" rel="noopener">Configure the Networking service for bare metal provisioning</a>。</p><p>首先編輯<code>/etc/neutron/plugins/ml2/ml2_conf.ini</code>修改以下內容：</p><pre><code>[ml2_type_flat]flat_networks = public, physnet1[ovs]datapath_type = systembridge_mappings = public:br-ex, physnet1:br-eth2tunnel_bridge = br-tunlocal_ip = 172.22.132.93</code></pre><p>接著建立 bridge 來處理實體網路與 OpenStack 之間的溝通：</p><pre><code class="sh">$ sudo ovs-vsctl add-br br-eth2$ sudo ovs-vsctl add-port br-eth2 eth2</code></pre><p>完成後重新啟動 Neutron server 與 agent：</p><pre><code class="sh">$ sudo systemctl restart devstack@q-svc.service$ sudo systemctl restart devstack@q-agt.service</code></pre><p>建立完成後，OVS bridges 會類似如下：</p><pre><code class="sh">$ sudo ovs-vsctl show    Bridge br-int        fail_mode: secure        Port &quot;int-br-eth2&quot;            Interface &quot;int-br-eth2&quot;                type: patch                options: {peer=&quot;phy-br-eth2&quot;}        Port br-int            Interface br-int                type: internal    Bridge &quot;br-eth2&quot;        Port &quot;phy-br-eth2&quot;            Interface &quot;phy-br-eth2&quot;                type: patch                options: {peer=&quot;int-br-eth2&quot;}        Port &quot;eth2&quot;            Interface &quot;eth2&quot;        Port &quot;br-eth2&quot;            Interface &quot;br-eth2&quot;                type: internal</code></pre><p>接著建立 Neutron flat 網路來提供使用：</p><pre><code class="sh">$ neutron net-create sharednet1 \                     --shared \                     --provider:network_type flat \                     --provider:physical_network physnet1$ neutron subnet-create sharednet1 172.22.132.0/24 \                        --name sharedsubnet1 \                        --ip-version=4 --gateway=172.22.132.254 \                        --allocation-pool start=172.22.132.180,end=172.22.132.200 \                        --enable-dhcp</code></pre><blockquote><p>P.S. neutron-client 在未來會被移除，故請轉用 <a href="https://docs.openstack.org/install-guide/launch-instance-networks-provider.html" target="_blank" rel="noopener">Provider network</a>。</p></blockquote><h3 id="設定-Ironic-cleaning-network"><a href="#設定-Ironic-cleaning-network" class="headerlink" title="設定 Ironic cleaning network"></a>設定 Ironic cleaning network</h3><p>當使用到 <a href="http://docs.openstack.org/ironic/latest/admin/cleaning.html#node-cleaning" target="_blank" rel="noopener">Node cleaning</a> 時，我們必須設定<code>cleaning_network</code>選項來提供使用。首先取得 Network 資訊，透過以下指令：</p><pre><code class="sh">$ openstack network list+--------------------------------------+------------+----------------------------------------------------------------------------+| ID                                   | Name       | Subnets                                                                    |+--------------------------------------+------------+----------------------------------------------------------------------------+| 03de10a0-d4d2-43ce-83db-806a5277dd29 | private    | 2a651bfb-776d-47f4-a958-f8a418f7fcd5, 99bdbd78-7a20-41b7-afa3-7cf7bf25b95b || 349a6a5b-1e26-4e36-8444-f6a6bbbdd227 | public     | 032a516e-3d55-4623-995d-06ee033eaee4, daf733a9-492e-4ea6-8a45-6364b88a8f6f || ade096bd-6a86-4d90-9cf4-bce9921f7257 | sharednet1 | 3f9f2a47-fdd9-472b-a6a2-ce6570e490ff                                       |+--------------------------------------+------------+----------------------------------------------------------------------------+</code></pre><p>編輯<code>/etc/ironic/ironic.conf</code>修改一下內容：</p><pre><code>[neutron]cleaning_network = sharednet1</code></pre><p>完成後，重新啟動 Ironic 服務：</p><pre><code class="sh">$ sudo systemctl restart devstack@ir-api.service$ sudo systemctl restart devstack@ir-cond.service</code></pre><h3 id="建立-Deploy-與-User-映像檔"><a href="#建立-Deploy-與-User-映像檔" class="headerlink" title="建立 Deploy 與 User 映像檔"></a>建立 Deploy 與 User 映像檔</h3><p>裸機服務在配置時需要兩組映像檔，分別為 <code>Deploy</code> 與 <code>User</code> 映像檔，其功能如下：</p><ul><li><code>Deploy images</code>: 用來準備裸機服務機器以進行實際的作業系統部署，在 Cleaning 等階段會使用到。</li><li><code>User images</code>:最後安裝至裸機服務提供給使用者使用的作業系統映像檔。</li></ul><p>由於 DevStack 預設會建立一組 Deploy 映像檔，這邊只針對 User 映像檔做手動建構說明，若要建構 Deploy 映像檔可以參考 <a href="https://docs.openstack.org/ironic/latest/install/deploy-ramdisk.html#deploy-ramdisk" target="_blank" rel="noopener">Building or downloading a deploy ramdisk image</a>。</p><p>首先我們必須先安裝<code>disk-image-builder</code>工具來提供建構映像檔：</p><pre><code class="sh">$ virtualenv dib$ source dib/bin/activate(dib) $ pip install diskimage-builder</code></pre><p>接著執行以下指令來進行建構映像檔：</p><pre><code class="sh">$ cat &lt;&lt;EOF &gt; k8s.repo[kubernetes]name=Kubernetesbaseurl=http://yum.kubernetes.io/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg       https://packages.cloud.google.com/yum/doc/rpm-package-key.gpgEOF$ DIB_YUM_REPO_CONF=k8s.repo \  DIB_DEV_USER_USERNAME=kyle \  DIB_DEV_USER_PWDLESS_SUDO=yes \  DIB_DEV_USER_PASSWORD=r00tme \  disk-image-create \        centos7 \        dhcp-all-interfaces \        devuser \        yum \        epel \        baremetal \        -o k8s.qcow2 \        -p vim,docker,kubelet,kubeadm,kubectl,kubernetes-cni...Converting image using qemu-img convertImage file k8s.qcow2 created...</code></pre><p>完成後會看到以下檔案：</p><pre><code class="sh">$ lsdib  k8s.d  k8s.initrd  k8s.qcow2  k8s.repo  k8s.vmlinuz</code></pre><p>上傳至 Glance 以提供使用：</p><pre><code class="sh"># 上傳 Kernel$ openstack image create k8s.kernel \                      --public \                      --disk-format aki \                      --container-format aki &lt; k8s.vmlinuz# 上傳 Initrd$ openstack image create k8s.initrd \                      --public \                      --disk-format ari \                      --container-format ari &lt; k8s.initrd# 上傳 Qcow2$ export MY_VMLINUZ_UUID=$(openstack image list | awk &#39;/k8s.kernel/ { print $2 }&#39;)$ export MY_INITRD_UUID=$(openstack image list | awk &#39;/k8s.initrd/ { print $2 }&#39;)$ openstack image create k8s \                      --public \                      --disk-format qcow2 \                      --container-format bare \                      --property kernel_id=$MY_VMLINUZ_UUID \                      --property ramdisk_id=$MY_INITRD_UUID &lt; k8s.qcow2</code></pre><h2 id="建立-Ironic-節點"><a href="#建立-Ironic-節點" class="headerlink" title="建立 Ironic 節點"></a>建立 Ironic 節點</h2><p>在所有服務配置都完成後，這時候要註冊實體機器資訊，來提供給 Compute 服務部署時使用。首先確認 Ironic 的 Driver 是否有資源機器的 Power driver：</p><pre><code class="sh">$ ironic driver-list+---------------------+----------------+| Supported driver(s) | Active host(s) |+---------------------+----------------+| agent_ipmitool      | ironic-dev     || fake                | ironic-dev     || ipmi                | ironic-dev     || pxe_ipmitool        | ironic-dev     |+---------------------+----------------+</code></pre><blockquote><p>若有缺少的話，請參考 <a href="https://docs.openstack.org/ironic/latest/install/setup-drivers.html" target="_blank" rel="noopener">Set up the drivers for the Bare Metal service</a>。</p></blockquote><p>確認有支援後，透過以下指令來建立 Node，並進行註冊：</p><pre><code class="sh">$ export DEPLOY_VMLINUZ_UUID=$(openstack image list | awk &#39;/ipmitool.kernel/ { print $2 }&#39;)$ export DEPLOY_INITRD_UUID=$(openstack image list | awk &#39;/ipmitool.initramfs/ { print $2 }&#39;)$ ironic node-create -d agent_ipmitool \                     -n bare-node-1 \                     -i ipmi_address=172.20.3.194 \                     -i ipmi_username=maas \                     -i ipmi_password=passwd \                     -i ipmi_port=623 \                     -i deploy_kernel=$DEPLOY_VMLINUZ_UUID \                     -i deploy_ramdisk=$DEPLOY_INITRD_UUID</code></pre><blockquote><p>若使用 Console 的話，要加入<code>-i ipmi_terminal_port=9000</code>，可參考 <a href="https://docs.openstack.org/ironic/latest/admin/console.html" target="_blank" rel="noopener">Configuring Web or Serial Console</a>。</p></blockquote><p>接著更新機器資訊，這邊透過手動方式來更新資訊：</p><pre><code class="sh">$ export NODE_UUID=$(ironic node-list | awk &#39;/bare-node-1/ { print $2 }&#39;)$ ironic node-update $NODE_UUID add \                     properties/cpus=4 \                     properties/memory_mb=8192 \                     properties/local_gb=100 \                     properties/root_gb=100 \                     properties/cpu_arch=x86_64</code></pre><p>(option)也可以使用 inspector 來識別裸機機器的硬體資訊，但需要修改<code>/etc/ironic-inspector/dnsmasq.conf</code>修改一下：</p><pre><code>no-daemonport=0interface=eth1bind-interfacesdhcp-range=172.22.132.200,172.22.132.210dhcp-match=ipxe,175dhcp-boot=tag:!ipxe,undionly.kpxedhcp-boot=tag:ipxe,http://172.22.132.93:3928/ironic-inspector.ipxedhcp-sequential-ip</code></pre><blockquote><p>完成後，透過 systemctl 重新啟動背景服務<code>devstack@ironic-inspector-dhcp.service</code>與<code>devstack@ironic-inspector.service</code>。</p></blockquote><p>透過 port create 來把 Node 的所有網路資訊進行註冊：</p><pre><code class="sh">$ ironic port-create -n $NODE_UUID -a NODE_MAC_ADDRESS</code></pre><blockquote><p>這邊<code>NODE_MAC_ADDRESS</code>是指<code>bare-node-1</code>節點的 PXE(eth1)網卡 Mac Address，如 54:a0:50:85:d5:fa。</p></blockquote><p>完成後透過 validate 指令來檢查：</p><pre><code class="sh">$ ironic node-validate $NODE_UUID+------------+--------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Interface  | Result | Reason                                                                                                                                                                                                |+------------+--------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| boot       | False  | Cannot validate image information for node 8e6fd86a-8eed-4e24-a510-3f5ebb0a336a because one or more parameters are missing from its instance_info. Missing are: [&#39;ramdisk&#39;, &#39;kernel&#39;, &#39;image_source&#39;] || console    | False  | Missing &#39;ipmi_terminal_port&#39; parameter in node\&#39;s driver_info.                                                                                                                                         || deploy     | False  | Cannot validate image information for node 8e6fd86a-8eed-4e24-a510-3f5ebb0a336a because one or more parameters are missing from its instance_info. Missing are: [&#39;ramdisk&#39;, &#39;kernel&#39;, &#39;image_source&#39;] || inspect    | True   |                                                                                                                                                                                                       || management | True   |                                                                                                                                                                                                       || network    | True   |                                                                                                                                                                                                       || power      | True   |                                                                                                                                                                                                       || raid       | True   |                                                                                                                                                                                                       || storage    | True   |                                                                                                                                                                                                       |+------------+--------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</code></pre><blockquote><p>P.S. 這邊<code>boot</code>與<code>deploy</code>的錯誤若是如上所示的話，可以直接忽略，這是因為使用 Nova 來管理 baremetal 會出現的問題。</p></blockquote><p>最後利用 provision 指令來測試節點是否能夠提供服務：</p><pre><code class="sh">$ ironic --ironic-api-version 1.34 node-set-provision-state $NODE_UUID manage$ ironic --ironic-api-version 1.34 node-set-provision-state $NODE_UUID provide$ ironic node-list+--------------------------------------+--------+---------------+-------------+--------------------+-------------+| UUID                                 | Name   | Instance UUID | Power State | Provisioning State | Maintenance |+--------------------------------------+--------+---------------+-------------+--------------------+-------------+| 0c20cf7d-0a36-46f4-ac38-721ff8bfb646 | bare-0 | None          | power off   | cleaning           | False       |+--------------------------------------+--------+---------------+-------------+--------------------+-------------+</code></pre><blockquote><p>這時候機器會進行 clean 過程，經過一點時間就會完成，若順利完成則該節點就可以進行部署了。若要了解細節狀態，可以參考 <a href="https://docs.openstack.org/ironic/latest/contributor/states.html" target="_blank" rel="noopener">Ironic’s State Machine</a>。</p></blockquote><p><img src="/images/openstack/ironic-clean.png" alt=""></p><h2 id="透過-Nova-部署-baremetal-機器"><a href="#透過-Nova-部署-baremetal-機器" class="headerlink" title="透過 Nova 部署 baremetal 機器"></a>透過 Nova 部署 baremetal 機器</h2><p>最後我們要透過 Nova API 來部署裸機，在開始前要建立一個 flavor 跟上傳 keypair 來提供使用：</p><pre><code class="sh">$ ssh-keygen -t rsa$ openstack keypair create --public-key ~/.ssh/id_rsa.pub default$ openstack flavor create --vcpus 4 --ram 8192 --disk 100 baremetal.large</code></pre><p>完成後，即可透過以下指令進行部署：</p><pre><code class="sh">$ NET_ID=$(openstack network list | awk &#39;/sharednet1/ { print $2 }&#39;)$ openstack server create --flavor baremetal.large \                          --nic net-id=$NET_ID \                          --image k8s \                          --key-name default k8s-01</code></pre><p>經過一段時間後，就會看到部署完成，這時候可以透過以下指令來確認部署結果：</p><pre><code class="sh">$ openstack server list+--------------------------------------+--------+--------+---------------------------+-------+-----------------+| ID                                   | Name   | Status | Networks                  | Image | Flavor          |+--------------------------------------+--------+--------+---------------------------+-------+-----------------+| a40e5cb1-dfc6-44d5-b638-648e8c0975fb | k8s-01 | ACTIVE | sharednet1=172.22.132.187 | k8s   | baremetal.large |+--------------------------------------+--------+--------+---------------------------+-------+-----------------+$ openstack baremetal list+--------------------------------------+--------+--------------------------------------+-------------+--------------------+-------------+| UUID                                 | Name   | Instance UUID                        | Power State | Provisioning State | Maintenance |+--------------------------------------+--------+--------------------------------------+-------------+--------------------+-------------+| 0c20cf7d-0a36-46f4-ac38-721ff8bfb646 | bare-0 | a40e5cb1-dfc6-44d5-b638-648e8c0975fb | power on    | active             | False       |+--------------------------------------+--------+--------------------------------------+-------------+--------------------+-------------+</code></pre><p>最後透過 ssh 來進入部署機器來建立應用：</p><pre><code class="sh">$ ssh kyle@172.22.132.187[kyle@host-172-22-132-187 ~]$ sudo systemctl start kubelet.service[kyle@host-172-22-132-187 ~]$ sudo systemctl start docker.service[kyle@host-172-22-132-187 ~]$ sudo kubeadm init --service-cidr 10.96.0.0/12 \                                                --kubernetes-version v1.7.4 \                                                --pod-network-cidr 10.244.0.0/16 \                                                --apiserver-advertise-address 172.22.132.187 \                                                --token b0f7b8.8d1767876297d85c</code></pre><blockquote><p>整合<code>Magnum</code>有空再寫，先簡單玩玩吧。</p></blockquote><p>若是懶人可以用 Dashboard 來部署，另外本教學的 DevStack 有使用 Ironic UI，因此可以在以下頁面看到 node 資訊。<br><img src="/images/openstack/ironic-ui.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://docs.openstack.org/ironic/latest/user/index.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Ironic&lt;/a&gt; 是 OpenStack 專案之一，主要目的是提供裸機機器部署服務(Bare-metal service)。它能夠單獨或整合 OpenStack 其他服務被使用，而可整合服務包含 Keystone、Nova、Neutron、Glance 與 Swift 等核心服務。當使用 Compute 與 Network 服務對 Bare-metal 進行適當的配置時，OpenStack 可以透過 Compute API 同時部署虛擬機(Virtual machines)與裸機(Bare machines)。&lt;/p&gt;
&lt;p&gt;本篇為了精簡安裝過程，故這邊不採用手動安裝教學(會在 Gitbook 書上更新)，因此採用 &lt;a href=&quot;https://docs.openstack.org/devstack/latest/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;DevStack&lt;/a&gt; 來部署服務，再手動設定一些步驟。&lt;/p&gt;
&lt;p&gt;本環境安裝資訊：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;OpenStack Pike&lt;/li&gt;
&lt;li&gt;DevStack Pike&lt;/li&gt;
&lt;li&gt;Pike Pike Pike ….&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="OpenStack" scheme="https://kairen.github.io/categories/OpenStack/"/>
    
    
      <category term="OpenStack" scheme="https://kairen.github.io/tags/OpenStack/"/>
    
      <category term="DevStack" scheme="https://kairen.github.io/tags/DevStack/"/>
    
      <category term="Bare-metal" scheme="https://kairen.github.io/tags/Bare-metal/"/>
    
  </entry>
  
  <entry>
    <title>利用 LinuxKit 建立 Kubernetes 叢集</title>
    <link href="https://kairen.github.io/2017/07/22/kubernetes/deploy/linuxkit-k8s/"/>
    <id>https://kairen.github.io/2017/07/22/kubernetes/deploy/linuxkit-k8s/</id>
    <published>2017-07-21T16:00:00.000Z</published>
    <updated>2017-08-08T08:01:50.000Z</updated>
    
    <content type="html"><![CDATA[<p>LinuxKit 是以 Container 來建立最小、不可變的 Linux 作業系統映像檔框架，先前有簡單介紹與操作過，可以參考<a href="https://kairen.github.io/2017/04/23/container/linuxkit/">LinuxKit</a>。本篇則將利用 LinuxKit 來建立 Kubernetes 的映像檔，並部署簡單的 Kubernetes 叢集。</p><p><img src="/images/kube/moby+kubernetes.png" alt=""></p><a id="more"></a><p>本次教學會在<code>Mac OS X</code>作業系統上進行，而部署的軟體資訊如下：</p><ul><li>Kubernetes v1.7.2(2017-08-07, Update)</li><li>Etcd v3</li><li>Weave</li><li>Docker v17.06.0-ce</li></ul><h2 id="預先準備資訊"><a href="#預先準備資訊" class="headerlink" title="預先準備資訊"></a>預先準備資訊</h2><ul><li>主機已安裝與啟動<code>Docker</code>工具。</li><li>主機已安裝<code>Git</code>工具。</li><li>主機以下載 LinuxKit 專案，並建構了 Moby 與 LinuxKit 工具。</li></ul><p>建構 Moby 與 LinuxKit 方法如以下操作：</p><pre><code class="sh">$ git clone https://github.com/linuxkit/linuxkit.git$ cd linuxkit$ make$ ./bin/moby versionmoby version 0.0commit: c2b081ed8a9f690820cc0c0568238e641848f58f$ ./bin/linuxkit versionlinuxkit version 0.0commit: 0e3ca695d07d1c9870eca71fb7dd9ede31a38380</code></pre><h2 id="建構-Kubernetes-系統映像檔"><a href="#建構-Kubernetes-系統映像檔" class="headerlink" title="建構 Kubernetes 系統映像檔"></a>建構 Kubernetes 系統映像檔</h2><p>首先我們要建立一個包好 Kubernetes 的 Linux 映像檔，而官方已經有做好範例，只要利用以下方式即可建構：</p><pre><code class="sh">$ cd linuxkit/projects/kubernetes/$ make build-vm-images...Create outputs:  kube-node-kernel kube-node-initrd.img kube-node-cmdline</code></pre><h2 id="建置-Kubernetes-cluster"><a href="#建置-Kubernetes-cluster" class="headerlink" title="建置 Kubernetes cluster"></a>建置 Kubernetes cluster</h2><p>完成建構映像檔後，就可以透過以下指令來啟動 Master OS 映像檔，然後獲取節點 IP：</p><pre><code class="sh">$ ./boot.sh(ns: getty) linuxkit-025000000002:~\# ip addr show dev eth02: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000    link/ether 02:50:00:00:00:02 brd ff:ff:ff:ff:ff:ff    inet 192.168.65.3/24 brd 192.168.65.255 scope global eth0       valid_lft forever preferred_lft forever    inet6 fe80::abf0:9fa4:d0f4:8da2/64 scope link       valid_lft forever preferred_lft forever</code></pre><p>啟動後，開啟新的 Console 來 SSH 進入 Master，來利用 kubeadm 初始化 Master：</p><pre><code class="sh">$ cd linuxkit/projects/kubernetes/$ ./ssh_into_kubelet.sh 192.168.65.3linuxkit-025000000002:/\# kubeadm-init.sh...kubeadm join --token 4236d3.29f61af661c49dbf 192.168.65.3:6443</code></pre><p>一旦 kubeadm 完成後，就會看到 Token，這時請記住 Token 資訊。接著開啟新 Console，然後執行以下指令來啟動 Node：</p><pre><code class="sh">console1&gt;$ ./boot.sh 1 --token 4236d3.29f61af661c49dbf 192.168.65.3:6443</code></pre><blockquote><p>P.S. 開啟節點格式為<code>./boot.sh &lt;n&gt; [&lt;join_args&gt; ...]</code>。</p></blockquote><p>接著分別在開兩個 Console 來加入叢集：</p><pre><code class="sh">console2&gt; $ ./boot.sh 2 --token 4236d3.29f61af661c49dbf 192.168.65.3:6443console3&gt; $ ./boot.sh 3 --token 4236d3.29f61af661c49dbf 192.168.65.3:6443</code></pre><p>完成後回到 Master 節點上，執行以下指令來查看節點狀況：</p><pre><code class="sh">$ kubectl get noNAME                    STATUS    AGE       VERSIONlinuxkit-025000000002   Ready     16m       v1.7.2linuxkit-025000000003   Ready     6m        v1.7.2linuxkit-025000000004   Ready     1m        v1.7.2linuxkit-025000000005   Ready     1m        v1.7.2</code></pre><h2 id="簡單部署-Nginx-服務"><a href="#簡單部署-Nginx-服務" class="headerlink" title="簡單部署 Nginx 服務"></a>簡單部署 Nginx 服務</h2><p>Kubernetes 可以選擇使用指令直接建立應用程式與服務，或者撰寫 YAML 與 JSON 檔案來描述部署應用程式的配置，以下將建立一個簡單的 Nginx 服務：</p><pre><code class="sh">$ kubectl run nginx --image=nginx --replicas=1 --port=80$ kubectl get pods -o wideNAME                     READY     STATUS    RESTARTS   AGE       IP          NODEnginx-1423793266-v0hpb   1/1       Running   0          38s       10.42.0.1   linuxkit-025000000004</code></pre><p>完成後要接著建立 svc(Service)，來提供外部網路存取應用程式，使用以下指令建立：</p><pre><code class="sh">$ kubectl expose deploy nginx --port=80 --type=NodePort$ kubectl get svcNAME         CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGEkubernetes   10.96.0.1       &lt;none&gt;        443/TCP        19mnginx        10.108.41.230   &lt;nodes&gt;       80:31773/TCP   5s</code></pre><p>由於這邊不是使用實體機器部署，因此網路使用 Docker namespace 網路，故這邊透過<code>ubuntu-desktop-lxde-vnc</code>來瀏覽 Nginx 應用：</p><pre><code class="sh">$ docker run -it --rm -p 6080:80 dorowu/ubuntu-desktop-lxde-vnc</code></pre><blockquote><p>完成後透過瀏覽器連接 <a href="localhost:6080" target="_blank" rel="noopener">HTLM VNC</a></p></blockquote><p><img src="/images/kube/docker-desktop.png" alt=""></p><p>最後關閉節點只需要執行以下即可：</p><pre><code class="sh">$ halt[ 1503.034689] reboot: Power down</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;LinuxKit 是以 Container 來建立最小、不可變的 Linux 作業系統映像檔框架，先前有簡單介紹與操作過，可以參考&lt;a href=&quot;https://kairen.github.io/2017/04/23/container/linuxkit/&quot;&gt;LinuxKit&lt;/a&gt;。本篇則將利用 LinuxKit 來建立 Kubernetes 的映像檔，並部署簡單的 Kubernetes 叢集。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/kube/moby+kubernetes.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://kairen.github.io/categories/Kubernetes/"/>
    
    
      <category term="Docker" scheme="https://kairen.github.io/tags/Docker/"/>
    
      <category term="Kubernetes" scheme="https://kairen.github.io/tags/Kubernetes/"/>
    
      <category term="LinuxKit" scheme="https://kairen.github.io/tags/LinuxKit/"/>
    
  </entry>
  
  <entry>
    <title>智能合約(Smart contracts)</title>
    <link href="https://kairen.github.io/2017/05/28/blockchain/smart-contracts/"/>
    <id>https://kairen.github.io/2017/05/28/blockchain/smart-contracts/</id>
    <published>2017-05-28T09:08:54.000Z</published>
    <updated>2017-08-02T02:33:37.000Z</updated>
    
    <content type="html"><![CDATA[<p><code>智能合約(Smart Contracts)</code> 是在 Ethereum 區塊鏈中所屬的物件。它們包含程式碼函式以及能夠與其他合約進行互動、做出決策、儲存資料與傳送乙太幣給其他人。合約是由創建者所定義，但是它們的執行與他們所提供的服務，都是由 Ethereum 網路本身提供。它們將存在且可被執行，只要整個網路存在，並且只會因程式中有撰寫自我銷毀的功能才會消失。</p><p>我可以用合約做什麼呢？只要想像力夠豐富，要做什麼幾乎都沒問題，但以下指南只會是入門，讓我們去實現一些簡單的事情。</p><a id="more"></a><h3 id="Smart-Sponsor"><a href="#Smart-Sponsor" class="headerlink" title="Smart Sponsor"></a>Smart Sponsor</h3><p>本節將說明一智能合約範例，透過建構一個合約來允許以下賬戶持有人進行互動。</p><ul><li>一個慈善機構舉行籌款活動，我們稱之為 <strong>thebenefactor</strong>。</li><li>一個受贊助的 runner 想為慈善機構募款，我們稱之為 <strong>therunner</strong>。</li><li>其他的人想要贊助 runner，我們稱之為 <strong>thesponsor</strong>。</li><li>一個 Ethereum 節點，用來開採區塊鏈以驗證交易，我們稱之為 <strong>theminer</strong>。</li></ul><p>我們的合約(smartSponsor)：</p><ul><li>是由一位 <strong>runner</strong> 透過贊助的執行來為慈善機構募款。</li><li>當建立合約時，<strong>runner</strong> 會任命為募集錢的捐助者。</li><li><strong>runner</strong> 則邀情其他人去進行贊助。用戶透過呼叫一個在智能合約上的函式，將乙太幣從 <strong>贊助商的帳戶</strong> 轉移到 <strong>合約</strong>，並保持乙太幣於合約，直到有進一步的通知。</li><li>在合約的時限期間的所有人都能看到誰是 <strong>捐助者</strong>，有多少的乙太幣被從誰捐(雖然贊助者可以匿名，當然:p)。</li></ul><p><img src="/images/blockchain/smartSponsor-1.png" alt="flow-1"></p><p>那麼有兩件事情可能發生：</p><ul><li>執行都按計劃進行，以及 <strong>runner</strong> 指示合約轉移到所有資金的捐助者。</li></ul><p><img src="/images/blockchain/smartSponsor-2.png" alt="flow-2"></p><ul><li>執行由於謀些原因無法承擔，而 runner 指示合約將退還贊助商的抵押。</li></ul><p><img src="/images/blockchain/smartSponsor-3.png" alt="flow-3"></p><p>Ethereum 允許智能合約由撰寫 Solidity 語言來定義。Solidity 的合約是類似於 Java 的類別定義。成員變數的儲存採用區塊鍊交易與合約的方法，來詢問合約或改變的其狀態。作為區塊鏈的副本會分散到網路中的所有節點，任何人都可以詢問合約，以從中搜尋公開的訊息。</p><p>合約有以下幾種方法：</p><ul><li><strong><code>smartSponsor</code></strong>：合約的建構子。它初始化合約的狀態。合約的建立者傳入賬戶的位址，有利於合約的 drawdown。</li><li><strong><code>pledge</code></strong>：任何人都可以呼叫捐贈乙太幣贊助基金。贊助商提供支援的選擇性訊息</li><li><strong><code>getPot</code></strong>：回傳當前儲存在合約的總乙太幣。</li><li><strong><code>refund</code></strong>：把贊助商的錢退回給贊助商。只有合約的擁有者才能呼叫這個函式。</li><li><strong><code>drawdown</code></strong>：傳送合約的總價值給捐助者賬戶。只有合約的擁有者才能呼叫這個函式。</li></ul><p>這個想法是使一個合約擁有約束力。他們不能拿回任何資金，除非整個合約被退還。在這種情況下，所有資料都是被公開存取的，這意味著任何人都有存取 Ethereum 區塊鏈，來查看誰建立了合約，誰是捐助者，以及誰透過存取合約程式碼本身保證了每一筆資金。</p><p>要注意很重要的一點，任何改變合約的狀態(建立、承若、退還或 drawing down)都需要在區塊鏈上建立交易，這意味著這些交易不會被儲存，要直到這些交易的區塊被開採。操作只能讀取到一個現有合約狀態(getPot 或讀取公有成員變數)都不需要進行挖礦。這是一個很重要且微妙的點：寫入操作是很慢的(因為我們要等到採礦完成)。由於這情況合約可能永遠不會被建立到區塊鍊中，因此呼叫方需要提供一些獎勵，來促進礦工去工作。這是被稱為 gas 的 Ethereum 術語，所有的寫入操作都是需要 gas 的支出來改變區塊鍊的狀態。</p><p>幸運的是我們不需要購買真正的乙太幣，以及參與 Ethereum 網路。我們可以使用相同的軟體，但要配置它運行一個本地測試區塊鏈，以及產生自己的假乙太幣。</p><p>以下為一個 Solidity 語言的智能合約範例：</p><pre><code class="js">contract smartSponsor {  address public owner;  address public benefactor;  bool public refunded;  bool public complete;  uint public numPledges;  struct Pledge {    uint amount;    address eth_address;    bytes32 message;  }  mapping(uint =&gt; Pledge) public pledges;  // constructor  function smartSponsor(address _benefactor) {    owner = msg.sender;    numPledges = 0;    refunded = false;    complete = false;    benefactor = _benefactor;  }  // add a new pledge  function pledge(bytes32 _message) {    if (msg.value == 0 || complete || refunded) throw;    pledges[numPledges] = Pledge(msg.value, msg.sender, _message);    numPledges++;  }  function getPot() constant returns (uint) {    return this.balance;  }  // refund the backers  function refund() {    if (msg.sender != owner || complete || refunded) throw;    for (uint i = 0; i &lt; numPledges; ++i) {      pledges[i].eth_address.send(pledges[i].amount);    }    refunded = true;    complete = true;  }  // send funds to the contract benefactor  function drawdown() {    if (msg.sender != owner || complete || refunded) throw;    benefactor.send(this.balance);    complete = true;  }}</code></pre><blockquote><ul><li><p>一個<code>Pledge</code>結構模型的捐贈，儲存著贊助商的帳戶 ID、承若押金，以及一些訊息字串。</p></li><li><p>這個<code>pledges</code>陣列儲存了一個承若方的列表。</p></li><li><p>合約中的所有成員變數都是公開的，所以<code>getters</code>將自動被建立。</p></li><li><p><code>throw</code>被稱為某些函式(functions)，用以防止資料被寫入錯誤的資料到該區塊鏈中。</p></li></ul></blockquote><h2 id="參考連結"><a href="#參考連結" class="headerlink" title="參考連結"></a>參考連結</h2><ul><li><a href="https://medium.com/@kpcb_edge/our-thoughts-on-ethereum-31520b164e00#.2q1i88278" target="_blank" rel="noopener">Our thoughts on Ethereum</a></li><li><a href="https://www.ethereum.org/greeter" target="_blank" rel="noopener">Building a smart contract using the command line</a></li><li><a href="https://developer.ibm.com/clouddataservices/2016/05/19/block-chain-technology-smart-contracts-and-ethereum/" target="_blank" rel="noopener">Block chain technology, smart contracts and Ethereum</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;code&gt;智能合約(Smart Contracts)&lt;/code&gt; 是在 Ethereum 區塊鏈中所屬的物件。它們包含程式碼函式以及能夠與其他合約進行互動、做出決策、儲存資料與傳送乙太幣給其他人。合約是由創建者所定義，但是它們的執行與他們所提供的服務，都是由 Ethereum 網路本身提供。它們將存在且可被執行，只要整個網路存在，並且只會因程式中有撰寫自我銷毀的功能才會消失。&lt;/p&gt;
&lt;p&gt;我可以用合約做什麼呢？只要想像力夠豐富，要做什麼幾乎都沒問題，但以下指南只會是入門，讓我們去實現一些簡單的事情。&lt;/p&gt;
    
    </summary>
    
      <category term="Blockchain" scheme="https://kairen.github.io/categories/Blockchain/"/>
    
    
      <category term="Ethereum" scheme="https://kairen.github.io/tags/Ethereum/"/>
    
      <category term="Blockchain" scheme="https://kairen.github.io/tags/Blockchain/"/>
    
      <category term="Solidity" scheme="https://kairen.github.io/tags/Solidity/"/>
    
      <category term="Smart Contract" scheme="https://kairen.github.io/tags/Smart-Contract/"/>
    
  </entry>
  
  <entry>
    <title>利用 Browser Solidity 部署智能合約</title>
    <link href="https://kairen.github.io/2017/05/27/blockchain/browser-solidity/"/>
    <id>https://kairen.github.io/2017/05/27/blockchain/browser-solidity/</id>
    <published>2017-05-27T09:08:54.000Z</published>
    <updated>2017-08-01T14:55:34.000Z</updated>
    
    <content type="html"><![CDATA[<p>Browser Solidity 是一個 Web-based 的 Solidity 編譯器與 IDE。本節將說明如何安裝於 Linux 與 Docker 中。</p><p>這邊可以連結官方的 <a href="https://ethereum.github.io/browser-solidity" target="_blank" rel="noopener">https://ethereum.github.io/browser-solidity</a> 來使用; 該網站會是該專案的最新版本預覽。</p><a id="more"></a><h3 id="Ubuntu-Server-手動安裝"><a href="#Ubuntu-Server-手動安裝" class="headerlink" title="Ubuntu Server 手動安裝"></a>Ubuntu Server 手動安裝</h3><p>首先安裝 Browser Solidity 要使用到的相關套件：</p><pre><code class="sh">$ sudo apt-get install -y apache2 make g++ git</code></pre><p>接著安裝 node.js 平台，來建置 App：</p><pre><code class="sh">$ curl -sL https://deb.nodesource.com/setup_6.x | sudo -E bash -$ sudo apt-get install nodejs</code></pre><p>然後透過 git 將專案抓到 local 端，並進入目錄：</p><pre><code class="sh">$ git clone https://github.com/ethereum/browser-solidity.git$ cd browser-solidity</code></pre><p>安裝相依套件與建置應用程式：</p><pre><code class="sh">$ sudo npm install$ sudo npm run build</code></pre><p>完成後，將所以有目錄的資料夾與檔案搬移到 Apache HTTP Server 的網頁根目錄：</p><pre><code class="sh">$ sudo cp ./* /var/www/html/</code></pre><blockquote><p>完成後就可以開啟網頁了。</p></blockquote><h3 id="Docker-快速安裝"><a href="#Docker-快速安裝" class="headerlink" title="Docker 快速安裝"></a>Docker 快速安裝</h3><p>目前 Browser Solidity 有提供 <a href="https://hub.docker.com/r/kairen/solidity/" target="_blank" rel="noopener">Docker Image</a> 下載。這邊只需要透過以下指令就能夠建立 Browser Solidity Dashboard 環境：</p><pre><code class="sh">$ docker run -d \            -p 80:80 \            --name solidity \            kairen/solidity</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Browser Solidity 是一個 Web-based 的 Solidity 編譯器與 IDE。本節將說明如何安裝於 Linux 與 Docker 中。&lt;/p&gt;
&lt;p&gt;這邊可以連結官方的 &lt;a href=&quot;https://ethereum.github.io/browser-solidity&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://ethereum.github.io/browser-solidity&lt;/a&gt; 來使用; 該網站會是該專案的最新版本預覽。&lt;/p&gt;
    
    </summary>
    
      <category term="Blockchain" scheme="https://kairen.github.io/categories/Blockchain/"/>
    
    
      <category term="Ethereum" scheme="https://kairen.github.io/tags/Ethereum/"/>
    
      <category term="Blockchain" scheme="https://kairen.github.io/tags/Blockchain/"/>
    
      <category term="Solidity" scheme="https://kairen.github.io/tags/Solidity/"/>
    
      <category term="Smart Contract" scheme="https://kairen.github.io/tags/Smart-Contract/"/>
    
  </entry>
  
  <entry>
    <title>監控 Go Ethereum 的區塊鏈狀況</title>
    <link href="https://kairen.github.io/2017/05/26/blockchain/geth-monitoring/"/>
    <id>https://kairen.github.io/2017/05/26/blockchain/geth-monitoring/</id>
    <published>2017-05-26T09:08:54.000Z</published>
    <updated>2017-08-01T14:55:32.000Z</updated>
    
    <content type="html"><![CDATA[<p>Ethereum 提供了一個 Web-based 的監控儀表板，可以部署該儀表板，並透過 Clinet 端傳送 Ethereum 節點的資訊，來查看整個區塊鏈狀態。本節將說明如何安裝監控儀表板於 Linux 與 Docker 容器中。</p><p>這邊可以連結官方的 <a href="https://ethstats.net/" target="_blank" rel="noopener">https://ethstats.net/</a> 來查看主節點網路的狀態。</p><a id="more"></a><h3 id="Ubuntu-Server-手動安裝"><a href="#Ubuntu-Server-手動安裝" class="headerlink" title="Ubuntu Server 手動安裝"></a>Ubuntu Server 手動安裝</h3><p>本部分說明如何手動安裝 eth-netstats 服務，其中會包含以下兩個部分：</p><ul><li><a href="#monitoring-site">Monitoring site</a></li><li><a href="#client-side">Client side</a></li></ul><h4 id="Monitoring-site"><a href="#Monitoring-site" class="headerlink" title="Monitoring site"></a>Monitoring site</h4><p>首先安裝 Browser Solidity 要使用到的相關套件：</p><pre><code class="sh">$ sudo apt-get install -y make g++ git</code></pre><p>接著安裝 node.js 平台，來建置 App：</p><pre><code class="sh">$ curl -sL https://deb.nodesource.com/setup_6.x | sudo -E bash -$ sudo apt-get install nodejs</code></pre><p>然後透過 git 將專案抓到 local 端，並進入目錄：</p><pre><code class="sh">$ git clone https://github.com/cubedro/eth-netstats$ cd eth-netstats</code></pre><p>安裝相依套件與建置應用程式，並啟動服務：</p><pre><code class="sh">$ sudo npm install$ sudo npm install -g grunt-cli$ grunt$ PORT=&quot;3000&quot; WS_SECRET=&quot;admin&quot; npm start</code></pre><blockquote><p>接著就可以開啟 <a href="http://localhost:3000" target="_blank" rel="noopener">eth-netstats</a>。</p><p>在沒有任何 Clinet 節點連上情況下，會是一個空的網頁。</p></blockquote><p>撰寫一個腳本<code>eth-netstats.sh</code>放置到背景服務執行：</p><pre><code class="sh">#!/bin/bash# History:# 2016/05/22 Kyle Bai Release#export PORT=&quot;3000&quot;export WS_SECRET=&quot;admin&quot;echo &quot;Starting private eth-netstats ...&quot;screen -dmS netstats /usr/bin/npm start</code></pre><p>透過以下方式執行：</p><pre><code class="sh">$ chmod u+x eth-netstats.sh$ ./eth-netstats.shStarting private eth-netstats ...</code></pre><blockquote><p>透過<code>screen -x netstats</code>取得當前畫面。</p></blockquote><h4 id="Client-side"><a href="#Client-side" class="headerlink" title="Client side"></a>Client side</h4><p>首先安裝 Browser Solidity 要使用到的相關套件：</p><pre><code class="sh">$ sudo apt-get install -y make g++ git</code></pre><p>接著安裝 node.js 平台，來建置 App：</p><pre><code class="sh">$ curl -sL https://deb.nodesource.com/setup_6.x | sudo -E bash -$ sudo apt-get install nodejs</code></pre><p>然後透過 git 將專案抓到 local 端，並進入目錄：</p><pre><code class="sh">$ git clone https://github.com/cubedro/eth-net-intelligence-api$ cd eth-net-intelligence-api</code></pre><p>安裝相依套件與建置應用程式：</p><pre><code class="sh">$ sudo npm install &amp;&amp; sudo npm install -g pm2</code></pre><p>編輯<code>app.json</code>設定檔，並修改以下內容：</p><pre><code class="sh">[  {    &quot;name&quot;        : &quot;mynode&quot;,    &quot;cwd&quot;         : &quot;.&quot;,    &quot;script&quot;      : &quot;app.js&quot;,    &quot;log_date_format&quot;   : &quot;YYYY-MM-DD HH:mm Z&quot;,    &quot;merge_logs&quot;    : false,    &quot;watch&quot;       : false,    &quot;exec_interpreter&quot;  : &quot;node&quot;,    &quot;exec_mode&quot;     : &quot;fork_mode&quot;,    &quot;env&quot;:    {      &quot;NODE_ENV&quot;    : &quot;production&quot;,      &quot;RPC_HOST&quot;    : &quot;localhost&quot;,      &quot;RPC_PORT&quot;    : &quot;8545&quot;,      &quot;INSTANCE_NAME&quot;   : &quot;mynode-1&quot;,      &quot;WS_SERVER&quot;     : &quot;http://localhost:3000&quot;,      &quot;WS_SECRET&quot;     : &quot;admin&quot;,    }  },]</code></pre><blockquote><ul><li><p><code>RPC_HOST</code>為 ethereum 的 rpc ip address。</p></li><li><p><code>RPC_PORT</code>為 ethereum 的 rpc port。</p></li><li><p><code>INSTANCE_NAME</code>為 ethereum 的監控實例名稱。</p></li><li><p><code>WS_SERVER</code>為 eth-netstats 的 URL。</p></li><li><p><code>WS_SECRET</code>為 eth-netstats 的 secret。</p></li></ul></blockquote><p>確認完成後，即可啟動服務：</p><pre><code class="sh">$ pm2 start app.json$ sudo tail -f $HOME/.pm2/logs/mynode-out-0.log</code></pre><h3 id="Docker-快速安裝"><a href="#Docker-快速安裝" class="headerlink" title="Docker 快速安裝"></a>Docker 快速安裝</h3><p>本部分說明如何手動安裝 eth-netstats 服務，其中會包含以下兩個部分：</p><ul><li><a href="#docker-monitoring-site">Docker Monitoring site</a></li><li><a href="#docker-client-side">Docker Client side</a></li></ul><h4 id="Docker-Monitoring-site"><a href="#Docker-Monitoring-site" class="headerlink" title="Docker Monitoring site"></a>Docker Monitoring site</h4><p>自動建置的映像檔現在可以在 <a href="https://hub.docker.com/r/kairen/ethstats/" target="_blank" rel="noopener">DockerHub</a> 找到，建議直接執行以下指令來啟動 eth-netstats 容器：</p><pre><code class="sh">$ docker run -d \            -p 3000:3000 \            -e WS_SECRET=&quot;admin&quot; \            --name ethstats \            kairen/ethstats</code></pre><blockquote><p>接著就可以開啟 <a href="http://localhost:3000" target="_blank" rel="noopener">eth-netstats</a>。</p><p>在沒有任何 Clinet 節點連上情況下，會是一個空的網頁。</p></blockquote><h4 id="Docker-Client-side"><a href="#Docker-Client-side" class="headerlink" title="Docker Client side"></a>Docker Client side</h4><p>自動建置的映像檔現在可以在 <a href="https://hub.docker.com/r/kairen/ethnetintel/" target="_blank" rel="noopener">DockerHub</a> 找到，也推薦透過執行以下指令來啟動 eth-netintel 容器：</p><pre><code class="sh">$ docker run -d \            -p 30303:30303 \            -p 30303:30303/udp \            -e NAME_PREFIX=&quot;geth-1&quot; \            -e WS_SERVER=&quot;http://172.17.1.200:3000&quot; \            -e WS_SECRET=&quot;admin&quot; \            -e RPC_HOST=&quot;172.17.1.199&quot; \            -e RPC_PORT=&quot;8545&quot; \            --name geth-1 \            kairen/ethnetintel</code></pre><blockquote><p>Monitor 與 Client 需要統一<code>WS_SECRET</code>。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Ethereum 提供了一個 Web-based 的監控儀表板，可以部署該儀表板，並透過 Clinet 端傳送 Ethereum 節點的資訊，來查看整個區塊鏈狀態。本節將說明如何安裝監控儀表板於 Linux 與 Docker 容器中。&lt;/p&gt;
&lt;p&gt;這邊可以連結官方的 &lt;a href=&quot;https://ethstats.net/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://ethstats.net/&lt;/a&gt; 來查看主節點網路的狀態。&lt;/p&gt;
    
    </summary>
    
      <category term="Blockchain" scheme="https://kairen.github.io/categories/Blockchain/"/>
    
    
      <category term="Ethereum" scheme="https://kairen.github.io/tags/Ethereum/"/>
    
      <category term="Blockchain" scheme="https://kairen.github.io/tags/Blockchain/"/>
    
      <category term="Go lang" scheme="https://kairen.github.io/tags/Go-lang/"/>
    
  </entry>
  
  <entry>
    <title>建立 Go Ethereum 私有網路鏈</title>
    <link href="https://kairen.github.io/2017/05/25/blockchain/multi-node-geth/"/>
    <id>https://kairen.github.io/2017/05/25/blockchain/multi-node-geth/</id>
    <published>2017-05-25T09:08:54.000Z</published>
    <updated>2017-08-01T14:42:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>Ethereum 專案是以區塊鏈原理，並進一步增加容納值、儲存資料，並且能封裝程式碼來建立智能合約(Smart Contracts)，形成區塊鏈應用程式，來執行運算任務。類似於比特幣(Bitcoin)，Ethereum 也具有一種貨幣，它叫做<code>乙太幣(Ether)</code>。乙太幣是開採於儲存在共享一致性的區塊鏈前驗證交易節點。乙太幣可以在賬戶(公有金鑰, Pubilc keys)與智能合約(Smart Contracts)之間進行轉移。</p><center><img src="/images/blockchain/ethereum-logo.png" alt="ethereum-logo"></center><p>本節將說明如何透過 Ubuntu 部署 Go Ethereum。並利用簡單的指令來進行 Demo。</p><a id="more"></a><h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>本次會使用到兩個節點來建立 Geth Instances，其規格如下：</p><table><thead><tr><th>Role</th><th>CPUs</th><th>RAM</th><th>Disk</th></tr></thead><tbody><tr><td>geth-1</td><td>2vCPU</td><td>4 GB</td><td>40 GB</td></tr><tr><td>geth-2</td><td>2vCPU</td><td>4 GB</td><td>40 GB</td></tr></tbody></table><p>首先在每個節點安裝 Ethereum 最新版本，可以依照官方透過以下方式快速安裝：</p><pre><code class="sh">$ sudo apt-get install -y software-properties-common$ sudo add-apt-repository -y ppa:ethereum/ethereum$ sudo apt-get update &amp;&amp; sudo apt-get install ethereum</code></pre><p>在每個節點建立一個<code>private.json</code>檔案來定義起源區塊(Genesis Block)，內容如下：</p><pre><code class="json">{  &quot;coinbase&quot; : &quot;0x0000000000000000000000000000000000000000&quot;,  &quot;difficulty&quot; : &quot;0x40000&quot;,  &quot;extraData&quot; : &quot;Custem Ethereum Genesis Block&quot;,  &quot;gasLimit&quot; : &quot;0xffffffff&quot;,  &quot;nonce&quot; : &quot;0x0000000000000042&quot;,  &quot;mixhash&quot; : &quot;0x0000000000000000000000000000000000000000000000000000000000000000&quot;,  &quot;parentHash&quot; : &quot;0x0000000000000000000000000000000000000000000000000000000000000000&quot;,  &quot;timestamp&quot; : &quot;0x00&quot;,  &quot;config&quot;: {        &quot;chainId&quot;: 123,        &quot;homesteadBlock&quot;: 0,        &quot;eip155Block&quot;: 0,        &quot;eip158Block&quot;: 0    },    &quot;alloc&quot;: { }}</code></pre><p>初始化創世區塊：</p><pre><code class="sh">$ geth init --datadir=data/ private.json</code></pre><p>在每個節點新增一名稱為<code>geth-private.sh</code>的腳本程式，將用於啟動 geth，並放置背景：</p><pre><code class="sh">#!/bin/bash# Program:#       This program is a private geth runner.# History:# 2016/05/22 Kyle Bai Release#echo &quot;Starting private geth&quot;screen -dmS geth /usr/bin/geth \            --datadir data/ \            --networkid 123 \            --nodiscover \            --maxpeers 5 \            --port 30301 \            --rpc \            --rpcaddr &quot;0.0.0.0&quot; \            --rpcport &quot;8545&quot; \            --rpcapi &quot;admin,db,eth,debug,miner,net,shh,txpool,personal,web3&quot; \            --rpccorsdomain &quot;*&quot; \            -verbosity 6</code></pre><blockquote><p>更多的參數，請參考 <a href="https://github.com/ethereum/go-ethereum/wiki/Command-Line-Options" target="_blank" rel="noopener">Command-Line-Options</a>。</p></blockquote><p>建立完成後，修改執行權限：</p><pre><code class="sh">$ chmod u+x geth-private.sh</code></pre><h2 id="建立-Ethereum-環境"><a href="#建立-Ethereum-環境" class="headerlink" title="建立 Ethereum 環境"></a>建立 Ethereum 環境</h2><p>首先進入到<code>geth-1</code>節點透過以下方式來啟動：</p><pre><code class="sh">$ ./geth-private.shStarting private geth</code></pre><blockquote><p>這時候會透過 screen 執行於背景，我們可以透過<code>screen -x geth</code>來進行前景。若要回到背景則透過<code>[Ctrl-A] + [Ctrl-D]</code>來 detached。要關閉 screen 則透過 <code>[Ctrl-C]</code>。</p></blockquote><p>接著為了確認是否正確啟動，我們可以透過 geth 的 attach 指令來連接 console：</p><pre><code class="sh">$ geth attach ipc:data/geth.ipc</code></pre><blockquote><p>也可以透過 HTTP 方式 attach，<code>geth attach http://localhost:8545</code>。</p><p>若一開始建立沒有 RPC，但想要加入 RPC 可以 attach 後，輸入以下 function：</p><pre><code class="sh">admin.startRPC(&quot;0.0.0.0&quot;, 8545, &quot;*&quot;, &quot;web3,db,net,eth&quot;)</code></pre></blockquote><p>進入後透過 admin API 來取得節點的資訊：</p><pre><code class="sh">&gt; admin.nodeInfo.enode&quot;enode://e3dd0392a2971c4b0c4c43a01cd682e19f31aaa573c43a9b227685af7ffed5070217392ae5ada278968d5c4bfddd9c93547bcf4592852196a8facbcdad64d257@[::]:30301?discport=0&quot;</code></pre><blockquote><p>這邊要取代<code>[::]</code>為主機 IP，如以下：</p><pre><code>&quot;enode://e3dd0392a2971c4b0c4c43a01cd682e19f31aaa573c43a9b227685af7ffed5070217392ae5ada278968d5c4bfddd9c93547bcf4592852196a8facbcdad64d257@172.16.1.99:30301?discport=0&quot;</code></pre></blockquote><p>上面沒問題後，接著進入到<code>geth-2</code>節點，然後透過以下指令開啟 console：</p><pre><code class="sh">$ geth init --datadir=data/ private.json$ geth --datadir data/ \       --networkid 123 \       --nodiscover \       --maxpeers 5 \       --port 30301 \       --rpc \       --rpcaddr &quot;0.0.0.0&quot; \       --rpcport &quot;8545&quot; \       --rpcapi &quot;admin,db,eth,debug,miner,net,shh,txpool,personal,web3&quot; \      --rpccorsdomain &quot;*&quot; \      -verbosity 6 \       console</code></pre><blockquote><p>也可以透過上一個節點的方式將服務放到背景，在 attach。</p></blockquote><p>完成上面指令會直接進入 console，接著透過以下方式來連接<code>geth-1</code>：</p><pre><code class="sh">&gt; admin.addPeer(&quot;enode://e3dd0392a2971c4b0c4c43a01cd682e19f31aaa573c43a9b227685af7ffed5070217392ae5ada278968d5c4bfddd9c93547bcf4592852196a8facbcdad64d257@172.16.1.99:30301?discport=0&quot;)trueI0525 12:56:40.623642 eth/downloader/downloader.go:239] Registering peer e3dd0392a2971c4bI0525 12:57:10.622920 p2p/server.go:467] &lt;-taskdone: wait for dial hist expire (29.99999387s)</code></pre><p>接著透過 net API 進行查看連接狀態：</p><pre><code class="sh">&gt; net.peerCount1&gt; admin.peers[{    caps: [&quot;eth/61&quot;, &quot;eth/62&quot;, &quot;eth/63&quot;],    id: &quot;e3dd0392a2971c4b0c4c43a01cd682e19f31aaa573c43a9b227685af7ffed5070217392ae5ada278968d5c4bfddd9c93547bcf4592852196a8facbcdad64d257&quot;,    name: &quot;Geth/v1.4.5-stable/linux/go1.5.1&quot;,    network: {      localAddress: &quot;172.16.1.100:51038&quot;,      remoteAddress: &quot;172.16.1.99:30301&quot;    },    protocols: {      eth: {        difficulty: 131072,        head: &quot;882048e0d045ea48903eddb4c50825a4e3c6c1a055df6a32244e9a9239f8c5e8&quot;,        version: 63      }    }}]</code></pre><h2 id="驗證服務"><a href="#驗證服務" class="headerlink" title="驗證服務"></a>驗證服務</h2><p>這部分將透過幾個指令與流程來驗證服務，首先在<code>geth-1</code>透過 attach 進入，並建立一個賬戶與查看乙太幣：</p><pre><code class="sh">$ geth attach http://localhost:8545&gt; kairen = personal.newAccount();Passphrase:Repeat passphrase:&quot;0xcb41ad8ba28c4b8b52eee159ef3bb6da197ff60b&quot;&gt; personal.listAccounts[&quot;0xcb41ad8ba28c4b8b52eee159ef3bb6da197ff60b&quot;]&gt; web3.fromWei(eth.getBalance(kairen), &quot;ether&quot;);0</code></pre><blockquote><p>P.S. 若要移除帳號，可以刪除<code>data/keystore</code>底下的檔案。</p></blockquote><p>接著在<code>geth-2</code>透過以下指令建立一個賬戶與查看乙太幣：</p><pre><code class="sh">&gt; pingyu = personal.newAccount();Passphrase:Repeat passphrase:&quot;0xf8c70df559cb9225f6e426d0f139fd6e8752c644&quot;&gt; personal.listAccounts[&quot;0xf8c70df559cb9225f6e426d0f139fd6e8752c644&quot;]&gt; web3.fromWei(eth.getBalance(pingyu), &quot;ether&quot;);0</code></pre><p>接著回到<code>geth-1</code>來賺取一些要交易的乙太幣：</p><pre><code class="sh">&gt; miner.setEtherbase(kairen)true</code></pre><p>當賬戶設定完成後，就可以執行以下指令進行採礦：</p><pre><code class="sh">&gt; miner.start(1)true</code></pre><blockquote><p>這邊需要一點時間產生 DAG，可以開一個新的命令列透過<code>screen -x geth</code>查看。</p><p>經過一段時間後，當 DAG 完成並開始採擴時就可以<code>miner.stop()</code>。</p></blockquote><p>接著在<code>geth-1</code>查看賬戶的乙太幣：</p><pre><code class="sh">&gt; web3.fromWei(eth.getBalance(kairen), &quot;ether&quot;);40.78125</code></pre><p>當成開採區塊後，就可以查看<code>geth-1</code>共採集的 ether balance 的數值：</p><pre><code>&gt; eth.getBalance(eth.coinbase).toNumber()40781250000000000000</code></pre><blockquote><p>即為<code>40.78125</code>乙太幣。</p></blockquote><p>接著我們要在將<code>geth-1</code>的賬戶乙太幣轉移到<code>geth-2</code>上，首先在<code>geth-1</code>上建立一個變數來存<code>geth-2</code>的賬戶位址：</p><pre><code class="sh">&gt; consumer = &quot;0xf8c70df559cb9225f6e426d0f139fd6e8752c644&quot;&quot;0xf8c70df559cb9225f6e426d0f139fd6e8752c644&quot;</code></pre><p>完成上述後，首先要將賬戶解鎖：</p><pre><code class="sh">&gt; personal.unlockAccount(kairen)true</code></pre><blockquote><p>輸入當初建立賬戶的密碼。</p></blockquote><p>並透過 eth API 的交易函式還將 ether balance 數值轉移：</p><pre><code class="sh">$ eth.sendTransaction({from: kairen, to: consumer, value: web3.toWei(10, &quot;ether&quot;)})&quot;0x1aee9082a55751c59077a273e7b08acd028d5099a4986f002518b0c8919d9e36&quot;</code></pre><p>若有在每一台 geth 節點上進入 debug 模式的話，會發現該交易資訊被存到一個區塊，這邊也可以透過 txpool 與 eth API 來查看：</p><pre><code class="sh">&gt; txpool.status{  pending: 1,  queued: 0}&gt; eth.getBlock(&quot;pending&quot;, true).transactions[{    blockHash: &quot;0x0b58d0b17e02f56746b0b5b22f195b6ae71d47343bf778763c4c476386ad7db7&quot;,    blockNumber: 112,    from: &quot;0xcb41ad8ba28c4b8b52eee159ef3bb6da197ff60b&quot;,    gas: 90000,    gasPrice: 20000000000,    hash: &quot;0x1aee9082a55751c59077a273e7b08acd028d5099a4986f002518b0c8919d9e36&quot;,    input: &quot;0x&quot;,    nonce: 0,    to: &quot;0xf8c70df559cb9225f6e426d0f139fd6e8752c644&quot;,    transactionIndex: 0,    value: 10000000000000000000}]</code></pre><blockquote><p>這邊的<code>pending</code>表示目前還沒有被驗證，因此我們需要一些節點來進行採礦驗證。這邊也可以發現該交易資訊被存在區塊編號<code>112</code>，可以提供往後查詢之用。</p></blockquote><p>接著回到<code>geth-2</code>節點，查看目前的數值變化：</p><pre><code class="sh">&gt; web3.fromWei(eth.getBalance(pingyu), &quot;ether&quot;);0</code></pre><p>這邊會發現沒有任何錢進來，Why? so sad。其實是因為該區塊還沒有被採集與認證，因此該交易不會被執行。</p><p>因此我們需要在任一節點提供運算，這邊在<code>geth-1</code>執行以下指令來進行採礦，這樣就可以看到該交易被驗證與接受：</p><pre><code class="sh">&gt; miner.start(1)trueTX(1aee9082a55751c59077a273e7b08acd028d5099a4986f002518b0c8919d9e36)Contract: falseFrom:     cb41ad8ba28c4b8b52eee159ef3bb6da197ff60bTo:       f8c70df559cb9225f6e426d0f139fd6e8752c644Nonce:    0GasPrice: 20000000000GasLimit  90000Value:    10000000000000000000Data:     0xV:        0x1cR:        0x9de7d843959f55a553577dc68a887893adf1b80eccd872021dfa6b8bcf3db43S:        0x287f8e01640ccd5924308725d2d274def7edc4a18169b36ae26c95216fdf0fedHex:      f86d808504a817c80083015f9094f8c70df559cb9225f6e426d0f139fd6e8752c644888ac7230489e80000801ca009de7d843959f55a553577dc68a887893adf1b80eccd872021dfa6b8bcf3db43a0287f8e01640ccd5924308725d2d274def7edc4a18169b36ae26c95216fdf0fed</code></pre><blockquote><p>當該區塊的交易確認沒問題被執行後，就可以透過<code>miner.stop()</code>停止採礦。</p></blockquote><p>這時再回到<code>geth-2</code>節點，查看目前的數值變化，會發現增加了 10 枚乙太幣：</p><pre><code class="sh">&gt; web3.fromWei(eth.getBalance(pingyu), &quot;ether&quot;);10</code></pre><p>之後可以在任一節點透過 eth web3 的 API 來查找指定區塊的交易資訊：</p><pre><code class="sh">&gt; eth.getTransactionFromBlock(40){  blockHash: &quot;0xe839c1392657731417fc04b9aecf7a181dd339086d5f7cdea0bccc2b1483b885&quot;,  blockNumber: 112,  from: &quot;0xcb41ad8ba28c4b8b52eee159ef3bb6da197ff60b&quot;,  gas: 90000,  gasPrice: 20000000000,  hash: &quot;0x1aee9082a55751c59077a273e7b08acd028d5099a4986f002518b0c8919d9e36&quot;,  input: &quot;0x&quot;,  nonce: 0,  to: &quot;0xf8c70df559cb9225f6e426d0f139fd6e8752c644&quot;,  transactionIndex: 0,  value: 10000000000000000000}</code></pre><h2 id="簡單的-Contract"><a href="#簡單的-Contract" class="headerlink" title="簡單的 Contract"></a>簡單的 Contract</h2><p>這邊將說明如何建立一個簡單的合約(Contract)來部署於區塊鏈上，首先複製以下內容：</p><pre><code>contract SimpleStorage {    uint storedData;    function set(uint x) {        storedData = x;    }    function get() constant returns (uint retVal) {        return storedData;    }}</code></pre><p>接著將內容貼到 <a href="https://ethereum.github.io/browser-solidity" target="_blank" rel="noopener">browser-solidity</a> 進行編譯成 JavaScript。如快照畫面所示。</p><center><img src="/images/blockchain/snapshot-contract.png" alt=""></center><p>透過這個 IDE 可以將 Solidity 語言轉換成 web3 code(JavaScript)，複製 web3 code 的內容，並儲存成<code>SimpleStorage.js</code>檔案放置到<code>geth-1</code>上。接著 attach 進入 geth 執行以下指令：</p><pre><code class="sh">&gt; loadScript(&#39;SimpleStorage.js&#39;);</code></pre><p>若有自行安裝<code>browser-solidity</code>的話，則可以使用如下圖一樣的方式連接。</p><center><img src="/images/blockchain/snapshot-dash-web3-provider.png" alt=""></center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Ethereum 專案是以區塊鏈原理，並進一步增加容納值、儲存資料，並且能封裝程式碼來建立智能合約(Smart Contracts)，形成區塊鏈應用程式，來執行運算任務。類似於比特幣(Bitcoin)，Ethereum 也具有一種貨幣，它叫做&lt;code&gt;乙太幣(Ether)&lt;/code&gt;。乙太幣是開採於儲存在共享一致性的區塊鏈前驗證交易節點。乙太幣可以在賬戶(公有金鑰, Pubilc keys)與智能合約(Smart Contracts)之間進行轉移。&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;/images/blockchain/ethereum-logo.png&quot; alt=&quot;ethereum-logo&quot;&gt;&lt;/center&gt;

&lt;p&gt;本節將說明如何透過 Ubuntu 部署 Go Ethereum。並利用簡單的指令來進行 Demo。&lt;/p&gt;
    
    </summary>
    
      <category term="Blockchain" scheme="https://kairen.github.io/categories/Blockchain/"/>
    
    
      <category term="Ethereum" scheme="https://kairen.github.io/tags/Ethereum/"/>
    
      <category term="Blockchain" scheme="https://kairen.github.io/tags/Blockchain/"/>
    
      <category term="Go lang" scheme="https://kairen.github.io/tags/Go-lang/"/>
    
  </entry>
  
  <entry>
    <title>Enterprise 的 Docker registry 平台 Harbor</title>
    <link href="https://kairen.github.io/2017/05/10/container/harbor-install/"/>
    <id>https://kairen.github.io/2017/05/10/container/harbor-install/</id>
    <published>2017-05-10T09:08:54.000Z</published>
    <updated>2017-08-02T02:56:37.000Z</updated>
    
    <content type="html"><![CDATA[<p>Harbor 是一個企業級 Registry 伺服器用於儲存和分散 Docker Image 的，透過新增一些企業常用的功能，例如：安全性、身分驗證和管理等功能擴展了開源的 <a href="https://github.com/docker/distribution" target="_blank" rel="noopener">Docker Distribution</a>。作為一個企業級的私有 Registry 伺服器，Harbor 提供了更好的效能與安全性。Harbor 支援安裝多個 Registry 並將 Image 在多個 Registry 做 replicated。除此之外，Harbor 亦提供了高級的安全性功能，像是用戶管理(user managment)，存取控制(access control)和活動審核(activity auditing)。</p><p><img src="/images/docker/harbor_logo.png" alt=""><br><a id="more"></a></p><h2 id="功能特色"><a href="#功能特色" class="headerlink" title="功能特色"></a>功能特色</h2><ul><li><strong>基於角色為基礎的存取控制(Role based access control)</strong>：使用者和 Repository 透過 Project 進行組織管理，一個使用者在同一個 Project 下，對於每個 Image 可以有不同權限。</li><li><strong>基於 Policy 的 Image 複製</strong>：Image 可以在多得 Registry instance 中同步複製。適合於附載平衡、高可用性、混合雲與多雲的情境。</li><li><strong>支援 LDAP/AD</strong>：Harbor 可以整合企業已有的 LDAP/AD，來管理使用者的認證與授權。</li><li><strong>使用者的圖形化介面</strong>：使用者可以透過瀏覽器，查詢 Image 和管理 Project</li><li><strong>審核管理</strong>：所有對 Repositroy 的操作都被記錄。</li><li><strong>RESTful API</strong>：RESTful APIs 提供給管理的操作，可以輕易的整合額外的系統。</li><li><strong>快速部署</strong>：提供 Online installer 與 Offline installer。</li></ul><h2 id="安裝指南"><a href="#安裝指南" class="headerlink" title="安裝指南"></a>安裝指南</h2><p>Harbor 提供兩種方法進行安裝：</p><ol><li>Online installer<br> 這種安裝方式會從 Docker hub 下載 Harbor 所需的映像檔，因此 installer 檔案較輕量。</li><li>Offline installer<br> 當無任何網際網路連接的情況下使用此種安裝方式，預先將所需的映像檔打包，因此 installer 檔案較大。</li></ol><h3 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h3><p>Harbor 會部署數個 Docker container，所以部署的主機需要能支援 Docker 的 Linux distribution。而部署主機需要安裝以下套件：</p><ul><li>Python 版本<code>2.7+</code>。</li><li>Docker Engine 版本 <code>1.10+</code>。Docker 安裝方式，請參考：<a href="https://docs.docker.com/engine/installation/" target="_blank" rel="noopener">Install Docker</a></li><li>Docker Compose 版本 <code>1.6.0+</code>。Docker Compose 安裝方式，請參考：<a href="https://docs.docker.com/compose/install/" target="_blank" rel="noopener">Install Docker Compose</a></li></ul><blockquote><p>官方安裝指南說明是 Linux 且要支援 Docker，但 Windows 支援 Docker 部署 Harbor 還需要驗證是否可行。</p></blockquote><p>安裝步驟大致可分為以下階段：</p><ol><li>下載 installer</li><li>設定 Harbor</li><li>執行安裝腳本</li></ol><h4 id="下載-installer"><a href="#下載-installer" class="headerlink" title="下載 installer"></a>下載 installer</h4><p>installer 的二進制檔案可以從 <a href="https://github.com/vmware/harbor/releases" target="_blank" rel="noopener">release page</a> 下載，選擇您需要 Online installer 或者 Offline installer，下載完成後，使用<code>tar</code>將 package 解壓縮：</p><p>Online installer：</p><pre><code class="sh">$ tar xvf harbor-online-installer-&lt;version&gt;.tgz</code></pre><p>Offline installer：</p><pre><code class="sh">$ tar xvf harbor-offline-installer-&lt;version&gt;.tgz</code></pre><h4 id="設定-Harbor"><a href="#設定-Harbor" class="headerlink" title="設定 Harbor"></a>設定 Harbor</h4><p>Harbor 的設定與參數都在<code>harbor.cfg</code>中。</p><p><code>harbor.cfg</code>中的參數分為<strong>required parameters</strong>與<strong>optional parameters</strong></p><ul><li><strong>required parameters</strong><br>  這類的參數是必須設定的，且會影響使用者更新<code>harbor.cfg</code>後，重新執行安裝腳本來重新安裝 Harbor。</li><li><strong>optional parameters</strong><br>  這類的參數為使用者自行決定是否設定，且只會在第一次安裝時，這些參數的配置才會生效。而 Harbor 啟動後，可以透過 Web UI 進行修改。</li></ul><h5 id="Configuring-storage-backend-optional"><a href="#Configuring-storage-backend-optional" class="headerlink" title="Configuring storage backend (optional)"></a>Configuring storage backend (optional)</h5><p>預設的情況下，Harbor 會將 Docker image 儲存在本機的檔案系統上，在生產環境中，您可以考慮使用其他 storage backend 而不是本機的檔案系統，像是 S3, OpenStack Swift, Ceph 等。而僅需更改 <code>common/templates/registry/config.yml</code>。以下為一個接 OpenStack Swift 的範例：</p><pre><code class="sh">storage:  swift:    username: admin    password: ADMIN_PASS    authurl: http://keystone_addr:35357/v3/auth    tenant: admin    domain: default    region: regionOne    container: docker_images</code></pre><blockquote><p>更多 storage backend 的資訊，請參考：<a href="https://docs.docker.com/registry/configuration/" target="_blank" rel="noopener">Registry Configuration Reference</a>。<br>另外官方提供的是改 <code>common/templates/registry/config.yml</code>，感覺寫錯，需再測試其正確性。</p></blockquote><h4 id="執行安裝腳本"><a href="#執行安裝腳本" class="headerlink" title="執行安裝腳本"></a>執行安裝腳本</h4><p>一旦<code>harbor.cfg</code>與 storage backend (optional) 設定完成後，可以透過<code>install.sh</code>腳本開始安裝 Harbor。從 Harbor 1.1.0 版本之後，已經整合<code>Notary</code>，但是預設的情況下安裝是不包含<code>Notary</code>支援：</p><pre><code class="sh">$ sudo ./install.sh</code></pre><blockquote><p>Online installer 會從 Docker hub 下載 Harbor 所需的映像檔，因此會花較久的時間。</p></blockquote><p>如果安裝過程正常，您可以打開瀏覽器並輸入在<code>harbor.cfg</code>中設定的<code>hostname</code>，來存取 Harbor 的 Web UI。<br><img src="https://i.imgur.com/jBVsr49.png" alt="Harbor Web UI"></p><blockquote><p>預設的管理者帳號密碼為 <code>admin</code>/<code>Harbor12345</code>。</p></blockquote><h4 id="開始使用-Harbor"><a href="#開始使用-Harbor" class="headerlink" title="開始使用 Harbor"></a>開始使用 Harbor</h4><p>登入成功後，可以創建一個新的 Project，並使用 Docker command 進行登入，但在登入之前，需要對 Docker daemon 新增<code>--insecure-registry</code>參數。新增<code>--insecure-registry</code>參數至<code>/etc/default/docker</code>中：</p><pre><code class="sh">DOCKER_OPTS=&quot;--insecure-registry &lt;your harbor.cfg hostname&gt;&quot;</code></pre><blockquote><p>其他細節，請參考：<a href="https://docs.docker.com/registry/insecure/#deploying-a-plain-http-registry" target="_blank" rel="noopener">Test an insecure registry</a>。</p><p>若在<code>Ubuntu 16.04</code>的作業系統版本，需要修改<code>/lib/systemd/system/docker.service</code>檔案，並加入一下內容。另外在 CentOS 7.x 版本則不需要加入<code>-H fd://</code>資訊：</p><pre><code class="sh">EnvironmentFile=/etc/default/dockerExecStart=/usr/bin/dockerd -H fd:// $DOCKER_OPTS</code></pre></blockquote><p>修改完成後，重新啟動服務：</p><pre><code class="sh">$ sudo systemctl daemon-reload</code></pre><p>服務重啟成功後，透過 Docker command 進行 login：</p><pre><code class="sh">$ docker login &lt;your harbor.cfg hostname&gt;</code></pre><p>將映像檔上 tag 之後，上傳至 Harbor：</p><pre><code class="sh">$ docker tag ubuntu:&lt;your harbor.cfg hostname&gt;/&lt;your project&gt;/ubuntu:16.04$ docker push &lt;your harbor.cfg hostname&gt;/&lt;your project&gt;/ubunut:16.04</code></pre><p>從 Harbor 抓取上傳的映像檔：</p><pre><code class="sh">$ docker pull &lt;your harbor.cfg hostname&gt;/&lt;your project&gt;/ubunut:16.04</code></pre><blockquote><p>更多使用者操作，請參考：<a href="https://github.com/vmware/harbor/blob/master/docs/user_guide.md" target="_blank" rel="noopener">Harbor User Guide</a>。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Harbor 是一個企業級 Registry 伺服器用於儲存和分散 Docker Image 的，透過新增一些企業常用的功能，例如：安全性、身分驗證和管理等功能擴展了開源的 &lt;a href=&quot;https://github.com/docker/distribution&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Docker Distribution&lt;/a&gt;。作為一個企業級的私有 Registry 伺服器，Harbor 提供了更好的效能與安全性。Harbor 支援安裝多個 Registry 並將 Image 在多個 Registry 做 replicated。除此之外，Harbor 亦提供了高級的安全性功能，像是用戶管理(user managment)，存取控制(access control)和活動審核(activity auditing)。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/docker/harbor_logo.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Container" scheme="https://kairen.github.io/categories/Container/"/>
    
    
      <category term="Linux Container" scheme="https://kairen.github.io/tags/Linux-Container/"/>
    
      <category term="Docker" scheme="https://kairen.github.io/tags/Docker/"/>
    
      <category term="Docker registry" scheme="https://kairen.github.io/tags/Docker-registry/"/>
    
  </entry>
  
  <entry>
    <title>品嚐 Moby LinuxKit 的 Linux 作業系統</title>
    <link href="https://kairen.github.io/2017/04/23/container/linuxkit/"/>
    <id>https://kairen.github.io/2017/04/23/container/linuxkit/</id>
    <published>2017-04-23T09:08:54.000Z</published>
    <updated>2017-08-01T14:23:59.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/linuxkit/linuxkit" target="_blank" rel="noopener">LinuxKit</a> 是 <a href="http://www.nebulaworks.com/blog/2017/04/22/docker-captains-dockercon-2017-review/" target="_blank" rel="noopener">DockerCon 2017</a> 中推出的工具之一，其主要是以 Container 來建立最小、不可變的 Linux 作業系統映像檔框架，Docker 公司一直透過 LinuxKit 來建立相關產品，如 Docker for Mac 等。由於要最快的了解功能，因此這邊透過建立簡單的映像檔來學習。</p><center><img src="/images/docker/linux-kit.png" alt=""></center><a id="more"></a><p>在開始前需要準備完成一些事情：</p><ul><li>安裝 Git client。</li><li>安裝 Docker engine，這邊建立使用 Docker-ce 17.04.0。</li><li>安裝 GUN make 工具。</li><li>安裝 GUN tar 工具。</li></ul><h2 id="建構-Moby-工具"><a href="#建構-Moby-工具" class="headerlink" title="建構 Moby 工具"></a>建構 Moby 工具</h2><p>首先我們要建構名為 Moby 的工具，這個工具主要提供指定的 YAML 檔來執行描述的建構流程與功能，並利用 Docker 來建構出 Linux 作業系統。在本教學中，最後我們會利用 <a href="https://github.com/mist64/xhyve" target="_blank" rel="noopener">xhyve</a> 這個 OS X 的虛擬化來提供執行系統實例，當然也可以透過官方的 <a href="https://github.com/moby/hyperkit" target="_blank" rel="noopener">HyperKit</a> 來進行。</p><p>首先透過 Git 來抓取 LinuxKit repos，並進入建構 Moby：</p><pre><code class="sh">$ git clone https://github.com/linuxkit/linuxkit.git$ cd linuxkit$ make &amp;&amp; sudo make install$ moby versionmoby version 0.0commit: 34d508562d7821cb812dd7b9caf4d9fbcdbc9fef</code></pre><h3 id="建立-Linux-映像檔"><a href="#建立-Linux-映像檔" class="headerlink" title="建立 Linux 映像檔"></a>建立 Linux 映像檔</h3><p>當完成建構 Moby 工具後，就可以透過撰寫 YAML 檔來描述 Linux 的建構功能與流程了，這邊建立一個 Docker + SSH 的 Linux 映像檔。首先建立檔名為<code>docker-sshd.yml</code>的檔案，然後加入以下內容：</p><pre><code class="yaml">kernel:  image: &quot;linuxkit/kernel:4.9.x&quot;  cmdline: &quot;console=ttyS0 console=tty0 page_poison=1&quot;init:  - linuxkit/init:63eed9ca7a09d2ce4c0c5e7238ac005fa44f564b  - linuxkit/runc:b0fb122e10dbb7e4e45115177a61a3f8d68c19a9  - linuxkit/containerd:18eaf72f3f4f9a9f29ca1951f66df701f873060b  - linuxkit/ca-certificates:e091a05fbf7c5e16f18b23602febd45dd690ba2fonboot:  - name: sysctl    image: &quot;linuxkit/sysctl:1f5ec5d5e6f7a7a1b3d2ff9dd9e36fd6fb14756a&quot;    net: host    pid: host    ipc: host    capabilities:     - CAP_SYS_ADMIN    readonly: true  - name: sysfs    image: linuxkit/sysfs:6c1d06f28ddd9681799d3950cddf044b930b221c  - name: binfmt    image: &quot;linuxkit/binfmt:c7e69ebd918a237dd086a5c58dd888df772746bd&quot;    binds:     - /proc/sys/fs/binfmt_misc:/binfmt_misc    readonly: true  - name: format    image: &quot;linuxkit/format:53748000acf515549d398e6ae68545c26c0f3a2e&quot;    binds:     - /dev:/dev    capabilities:     - CAP_SYS_ADMIN     - CAP_MKNOD  - name: mount    image: &quot;linuxkit/mount:d2669e7c8ddda99fa0618a414d44261eba6e299a&quot;    binds:     - /dev:/dev     - /var:/var:rshared,rbind    capabilities:     - CAP_SYS_ADMIN    rootfsPropagation: shared    command: [&quot;/mount.sh&quot;, &quot;/var/lib/docker&quot;]services:  - name: rngd    image: &quot;linuxkit/rngd:c42fd499690b2cb6e4e6cb99e41dfafca1cf5b14&quot;    capabilities:     - CAP_SYS_ADMIN    oomScoreAdj: -800    readonly: true  - name: dhcpcd    image: &quot;linuxkit/dhcpcd:57a8ef29d3a910645b2b24c124f9ce9ef53ce703&quot;    binds:     - /var:/var     - /tmp/etc:/etc    capabilities:     - CAP_NET_ADMIN     - CAP_NET_BIND_SERVICE     - CAP_NET_RAW    net: host    oomScoreAdj: -800  - name: ntpd    image: &quot;linuxkit/openntpd:a570316d7fc49ca1daa29bd945499f4963d227af&quot;    capabilities:      - CAP_SYS_TIME      - CAP_SYS_NICE      - CAP_SYS_CHROOT      - CAP_SETUID      - CAP_SETGID    net: host  - name: docker    image: &quot;linuxkit/docker-ce:741bf21513328f674e0cdcaa55492b0b75974e08&quot;    capabilities:     - all    net: host    mounts:     - type: cgroup       options: [&quot;rw&quot;,&quot;nosuid&quot;,&quot;noexec&quot;,&quot;nodev&quot;,&quot;relatime&quot;]    binds:     - /var/lib/docker:/var/lib/docker     - /lib/modules:/lib/modules  - name: sshd    image: &quot;linuxkit/sshd:e108d208adf692c8a0954f602743e0eec445364e&quot;    capabilities:    - all    net: host    pid: host    binds:      - /root/.ssh:/root/.ssh      - /etc/resolv.conf:/etc/resolv.conf  - name: test-docker-bench    image: &quot;linuxkit/test-docker-bench:2f941429d874c5dcf05e38005affb4f10192e1a8&quot;    ipc: host    pid: host    net: host    binds:    - /run:/var/run    capabilities:    - allfiles:  - path: etc/docker/daemon.json    contents: &#39;{&quot;debug&quot;: true}&#39;  - path: root/.ssh/authorized_keys    contents: &#39;SSH_KEY&#39;trust:  image:    - linuxkit/kernel    - linuxkit/binfmt    - linuxkit/rngdoutputs:  - format: kernel+initrd  - format: iso-bios</code></pre><blockquote><p><code>P.S.</code>請修改<code>SSH_KEY</code>內容為你的系統 ssh public key。</p></blockquote><p>這邊說明幾個 YAML 格式意義：</p><ul><li><strong>kernel</strong>: 指定 Docker 映像檔的核心版本，會包含一個 Linux 核心與檔案系統的 tar 檔，會將核心建構在<code>/kernel</code>目錄中。</li><li><strong>init</strong>: 是一個 Docker Container 的 init 行程基礎，裡面包含<code>init</code>、<code>containerd</code>、<code>runC</code>與其他等工具。</li><li><strong>onboot</strong>: 指定要建構的系統層級工具，會依據定義順序來執行，該類型如: dhcpd 與 ntpd 等。</li><li><strong>services</strong>: 指定要建構服務，通常會是系統開啟後執行，如 ngnix、apache2。</li><li><strong>files</strong>:要複製到該 Linux 系統映像檔中的檔案。</li><li><strong>outputs</strong>:輸出的映像檔格式。</li></ul><blockquote><p>更多 YAML 格式說明可以參考官方 <a href="https://github.com/linuxkit/linuxkit/blob/master/docs/yaml.md" target="_blank" rel="noopener">LinuxKit YAML</a>。目前 LinuxKit 的映像檔來源可以參考 <a href="https://hub.docker.com/u/linuxkit/" target="_blank" rel="noopener">Docker Hub</a></p></blockquote><p>撰寫完後，就可以透過 Moby 工具進行建構 Linux 映像檔了：</p><pre><code class="sh">$ moby build docker-sshd.ymlExtract kernel image: linuxkit/kernel:4.9.xPull image: linuxkit/kernel:4.9.x...Create outputs:  docker-sshd-kernel docker-sshd-initrd.img docker-sshd-cmdline  docker-sshd.iso</code></pre><p>完成後會看到以下幾個檔案：</p><ul><li>docker-sshd-kernel: 為 RAW Kernel 映像檔.</li><li>docker-sshd-initrd.img: 為初始化 RAW Disk 檔案.</li><li>docker-sshd-cmdline: Command line options 檔案.</li><li>docker-sshd.iso: Docker SSHD ISO 格式映像檔.</li></ul><h3 id="測試映像檔"><a href="#測試映像檔" class="headerlink" title="測試映像檔"></a>測試映像檔</h3><p>當完成建構映像檔後，就可以透過一些工具來進行測試，這邊採用 <a href="https://github.com/mist64/xhyve" target="_blank" rel="noopener">xhyve</a> 來執行實例，首先透過 Git 取得 xhyve repos，並建構與安裝：</p><pre><code class="sh">$ git clone https://github.com/mist64/xhyve$ cd xhyve$ make &amp;&amp; cp build/xhyve /usr/local/bin/$ xhyveUsage: xhyve [-behuwxMACHPWY] [-c vcpus] [-g &lt;gdb port&gt;] [-l &lt;lpc&gt;]             [-m mem] [-p vcpu:hostcpu] [-s &lt;pci&gt;] [-U uuid] -f &lt;fw&gt;</code></pre><blockquote><p>xhyve 是 FreeBSD 虛擬化技術 bhyve 的 OS X 版本，是以  <a href="https://developer.apple.com/library/mac/documentation/DriversKernelHardware/Reference/Hypervisor/index.html" target="_blank" rel="noopener">Hypervisor.framework</a> 為基底的上層工具，這是除了 VirtualBox 與 VMwar 的另外選擇，並且該工具非常的輕巧，只有幾 KB 的容量。</p></blockquote><p>接著撰寫 xhyve 腳本來啟動映像檔：</p><pre><code class="sh">#!/bin/shKERNEL=&quot;docker-sshd-kernel&quot;INITRD=&quot;docker-sshd-initrd.img&quot;CMDLINE=&quot;console=ttyS0 console=tty0 page_poison=1&quot;MEM=&quot;-m 1G&quot;PCI_DEV=&quot;-s 0:0,hostbridge -s 31,lpc&quot;LPC_DEV=&quot;-l com1,stdio&quot;ACPI=&quot;-A&quot;#SMP=&quot;-c 2&quot;# sudo if you want networking enabledNET=&quot;-s 2:0,virtio-net&quot;xhyve $ACPI $MEM $SMP $PCI_DEV $LPC_DEV $NET -f kexec,$KERNEL,$INITRD,&quot;$CMDLINE&quot;</code></pre><blockquote><p>修改<code>KERNEL</code>與<code>INITRD</code>為 docker-sshd 的映像檔。</p></blockquote><p>完成後就可以進行啟動測試：</p><pre><code>$ chmod u+x run.sh$ sudo ./run.shWelcome to LinuxKit                        ##         .                  ## ## ##        ==               ## ## ## ## ##    ===           /&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;&quot;\___/ ===      ~~~ {~~ ~~~~ ~~~ ~~~~ ~~~ ~ /  ===- ~~~           \______ o           __/             \    \         __/              \____\_______/.../ # lsbin         etc         lib         root        srv         usrcontainers  home        media       run         sys         vardev         init        proc        sbin        tmp/ # ip...4: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000    inet 192.168.64.4/24 brd 192.168.64.255 scope global eth0       valid_lft forever preferred_lft forever14: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN    inet 172.17.0.1/16 scope global docker0       valid_lft forever preferred_lft forever</code></pre><h3 id="驗證映像檔服務"><a href="#驗證映像檔服務" class="headerlink" title="驗證映像檔服務"></a>驗證映像檔服務</h3><p>當看到上述結果後，表示作業系統開啟無誤，這時候我們要測試系統服務是否正常，首先透過 SSH 來進行測試，在剛剛新增的 ssh public key 主機上執行以下：</p><pre><code>$ ssh root@192.168.64.4moby-aa16c789d03b:~# uname -r4.9.25-linuxkitmoby-aa16c789d03b:~# exit</code></pre><p>查看 Docker 是否啟動：</p><pre><code>moby-aa16c789d03b:~# netstat -xpActive UNIX domain sockets (w/o servers)Proto RefCnt Flags       Type       State         I-Node PID/Program name    Pathunix  2      [ ]         DGRAM                     33822 606/dhcpcdunix  3      [ ]         STREAM     CONNECTED      33965 748/ntpd: dns enginunix  3      [ ]         STREAM     CONNECTED      33960 747/ntpd: ntp enginunix  3      [ ]         STREAM     CONNECTED      33964 747/ntpd: ntp enginunix  3      [ ]         STREAM     CONNECTED      33959 642/ntpdunix  3      [ ]         STREAM     CONNECTED      34141 739/dockerdunix  3      [ ]         STREAM     CONNECTED      34142 751/docker-containe /var/run/docker/libcontainerd/docker-containerd.sock</code></pre><p>最後關閉虛擬機可以透過以下指令完成：</p><pre><code>moby-aa16c789d03b:~# haltTerminated</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://github.com/linuxkit/linuxkit&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;LinuxKit&lt;/a&gt; 是 &lt;a href=&quot;http://www.nebulaworks.com/blog/2017/04/22/docker-captains-dockercon-2017-review/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;DockerCon 2017&lt;/a&gt; 中推出的工具之一，其主要是以 Container 來建立最小、不可變的 Linux 作業系統映像檔框架，Docker 公司一直透過 LinuxKit 來建立相關產品，如 Docker for Mac 等。由於要最快的了解功能，因此這邊透過建立簡單的映像檔來學習。&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;/images/docker/linux-kit.png&quot; alt=&quot;&quot;&gt;&lt;/center&gt;
    
    </summary>
    
      <category term="Container" scheme="https://kairen.github.io/categories/Container/"/>
    
    
      <category term="Docker" scheme="https://kairen.github.io/tags/Docker/"/>
    
      <category term="Linux" scheme="https://kairen.github.io/tags/Linux/"/>
    
      <category term="Moby" scheme="https://kairen.github.io/tags/Moby/"/>
    
      <category term="Microkernel" scheme="https://kairen.github.io/tags/Microkernel/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 基本使用與分散式概念</title>
    <link href="https://kairen.github.io/2017/04/10/tensorflow/intro/"/>
    <id>https://kairen.github.io/2017/04/10/tensorflow/intro/</id>
    <published>2017-04-10T08:23:01.000Z</published>
    <updated>2017-08-01T14:23:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>TensorFlow™ 是利用資料流圖(Data Flow Graphs)來表達數值運算的開放式原始碼函式庫。資料流圖中的節點(Nodes)被用來表示數學運算，而邊(Edges)則用來表示在節點之間互相聯繫的多維資料陣列，即張量(Tensors)。它靈活的架構讓你能夠在不同平台上執行運算，例如 PC 中的一個或多的 CPU(或GPU)、智慧手持裝置與伺服器等。TensorFlow 最初是 Google 機器智能研究所的研究員和工程師開發而成，主要用於機器學習與深度神經網路方面研究。</p><a id="more"></a><p>TensorFlow 其實在意思上是要用兩個部分來解釋，Tensor 與 Flow：</p><ul><li><strong>Tensor</strong>：是中文翻譯是<code>張量</code>，其實就是一個<code>n</code>維度的陣列或列表。如一維 Tensor 就是向量，二維 Tensor 就是矩陣等等.</li><li><strong>Flow</strong>：是指 Graph 運算過程中的資料流.</li></ul><p><img src="https://lh3.googleusercontent.com/hIViPosdbSGUpLmPnP2WqL9EmvoVOXW7dy6nztmY5NZ9_u5lumMz4sQjjsBZ2QxjyZZCIPgucD2rhdL5uR7K0vLi09CEJYY=s688" alt=""></p><h2 id="Data-Flow-Graphs"><a href="#Data-Flow-Graphs" class="headerlink" title="Data Flow Graphs"></a>Data Flow Graphs</h2><p>資料流圖(Data Flow Graphs)是一種有向圖的節點(Node)與邊(Edge)來描述計算過程。圖中的節點表示數學操作，亦表示資料 I/O 端點; 而邊則表示節點之間的關析，用來傳遞操作之間互相使用的多維陣列(Tensors)，而 Tensor 是在圖中流動的資料表示。一旦節點相連的邊傳來資料流，這時節點就會被分配到運算裝置上異步(節點之間)或同步(節點之內)的執行。</p><center><img src="https://www.tensorflow.org/images/tensors_flowing.gif" alt=""></center><h2 id="TensorFlow-基本使用"><a href="#TensorFlow-基本使用" class="headerlink" title="TensorFlow 基本使用"></a>TensorFlow 基本使用</h2><p>在開始進行 TensorFlow 之前，需要了解幾個觀念：</p><ul><li>使用 <a href="https://www.tensorflow.org/api_docs/python/tf/Graph" target="_blank" rel="noopener">tf.Graph</a> 來表示計算任務.</li><li>採用<code>tensorflow::Session</code>的上下文(Context)來執行圖.</li><li>以 Tensor 來表示所有資料，可看成擁有靜態資料類型，但有動態大小的多維陣列與列表，如 Boolean 或 String 轉成數值類型.</li><li>透過<code>tf.Variable</code>來維護狀態.</li><li>透過 feed 與 fetch 來任意操作(Arbitrary operation)給予值或從中取得資料.</li></ul><p>TensorFlow 的圖中的節點被稱為 <a href="https://www.tensorflow.org/api_docs/python/tf/Operation" target="_blank" rel="noopener">op(operation)</a>。一個<code>op</code>會有 0 至多個 Tensor，而每個 Tensor 是一種類別化的多維陣列，例如把一個圖集合表示成四維浮點陣列，分別為<code>[batch, height, width, channels]</code>。</p><p><img src="http://upload-images.jianshu.io/upload_images/2630831-5da81623d4661886.jpg?imageMogr2/auto-orient/strip" alt=""></p><p>利用三種不同稱呼來描述 Tensor 的維度，Shape、Rank 與 Dimension。可參考 <a href="https://www.tensorflow.org/programmers_guide/dims_types" target="_blank" rel="noopener">Rank, Shape, 和 Type</a>。</p><p><img src="http://upload-images.jianshu.io/upload_images/2630831-3625a021343b5da3.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>一般只有 shape 能夠直接被 print，而 Tensor 則需要 Session 來提供，一般需要三個操作步驟：</p><ol><li>建立 Tensor.</li><li>新增 op.</li><li>建立 Session(包含一個 Graph)來執行運算.</li></ol><p>以下是一個簡單範例，說明如何建立運算：</p><pre><code class="py"># coding=utf-8import tensorflow as tfa = tf.constant(1)b = tf.constant(2)c = tf.constant(3)d = tf.constant(4)add1 = tf.add(a, b)mul1 = tf.multiply(b, c)add2 = tf.add(c, d)output = tf.add(add1, mul1)with tf.Session() as sess:    print sess.run(output)</code></pre><p>執行流程如下圖：<br><img src="https://github.com/lienhua34/notes/raw/master/tensorflow/asserts/graph_compute_flow.jpg?_=5998853" alt=""></p><p>以下是一個簡單範例，說明如何建立多個 Graph：</p><pre><code class="python="># coding=utf-8import tensorflow as tflogs_path = &#39;./basic_tmp&#39;# 建立一個 graph，並建立兩個常數 op ，這些 op 稱為節點g1 = tf.Graph()with g1.as_default():    a = tf.constant([1.5, 6.0])    b = tf.constant([1.5, 3.2])    c = a * bwith tf.Graph().as_default() as g2:    # 建立一個 1x2 矩陣與 2x1 矩陣 op    m1 = tf.constant([[1., 0., 2.], [-1., 3., 1.]])    m2 = tf.constant([[3., 1.], [2., 1.], [1., 0.]])    m3 = tf.matmul(m1, m2) # 矩陣相乘# 在 session 執行 graph，並進行資料數據操作 `c`。# 然後指派給 cpu 做運算with tf.Session(graph=g1) as sess_cpu:  with tf.device(&quot;/cpu:0&quot;):      writer = tf.summary.FileWriter(logs_path, graph=g1)      print(sess_cpu.run(c))with tf.Session(graph=g2) as sess_gpu:  with tf.device(&quot;/gpu:0&quot;):      result = sess_gpu.run(m3)      print(result)# 使用 tf.InteractiveSession 方式來印出內容(不會實際執行)it_sess = tf.InteractiveSession()x = tf.Variable([1.0, 2.0])a = tf.constant([3.0, 3.0])# 使用初始器 initializer op 的 run() 方法初始化 &#39;x&#39;x.initializer.run()sub = tf.subtract(x, a)print sub.eval()it_sess.close()</code></pre><blockquote><ul><li>範例來至 <a href="https://www.tensorflow.org/versions/r0.10/get_started/basic_usage" target="_blank" rel="noopener">Basic Usage</a>。</li><li>指定 Device 可以看這邊 <a href="https://www.tensorflow.org/versions/r0.10/how_tos/using_gpu/" target="_blank" rel="noopener">Using GPU</a>.</li></ul></blockquote><p>上面範例可以看到建立了一個 Graph 的計算過程<code>c</code>，而當直接執行到<code>c</code>時，並不會真的執行運算，而是在<code>sess</code>會話建立後，並透過<code>sess</code>執行分配給 CPU 或 GPU 之類設備進行運算後，才會回傳一個節點的 Tensor，在 Python 中 Tensor 是一個 Numpy 的 ndarry 物件。</p><p>TensorFlow 也可以透過變數來維護 Graph 的執行過程狀態，這邊提供一個簡單的累加器：</p><pre><code class="python="># coding=utf-8import tensorflow as tf# 建立一個變數 counter，並初始化為 0state = tf.Variable(0, name=&quot;counter&quot;)# 建立一個常數 op 為 1，並用來累加 stateone = tf.constant(1)new_value = tf.add(state, one)update = tf.assign(state, new_value)# 啟動 Graph 前，變數必須先被初始化(init) opinit_op = tf.global_variables_initializer()# 啟動 Graph 來執行 opwith tf.Session() as sess:  sess.run(init_op)  print sess.run(state)  # 執行 op 並更新 state  for _ in range(3):    sess.run(update)    print sess.run(state)</code></pre><blockquote><p>更多細節可以查看 <a href="https://www.tensorflow.org/programmers_guide/variables" target="_blank" rel="noopener">Variables</a>。</p></blockquote><p>另外可以利用 Fetch 方式來一次取得多個節點的 Tensor，範例如下：</p><pre><code class="python="># coding=utf-8import tensorflow as tfinput1 = tf.constant(3.0)input2 = tf.constant(2.0)input3 = tf.constant(5.0)intermed = tf.add(input2, input3)mul = tf.multiply(input1, intermed)with tf.Session() as sess:  # 一次取得多個 Tensor  result = sess.run([mul, intermed])  print result</code></pre><p>而當我們想要在執行 Session 時，臨時替換 Tensor 內容的話，就可以利用 TensorFlow 內建的 Feed 方法來解決：</p><pre><code class="python="># coding=utf-8import tensorflow as tfinput1 = tf.placeholder(tf.float32)input2 = tf.placeholder(tf.float32)output = tf.multiply(input1, input2)with tf.Session() as sess:  # 透過 feed 來更改 op 內容，這只會在執行時有效  print sess.run([output], feed_dict={input1:[7.], input2:[2.]})  print sess.run([output])</code></pre><h2 id="TensorFlow-分散式運算"><a href="#TensorFlow-分散式運算" class="headerlink" title="TensorFlow 分散式運算"></a>TensorFlow 分散式運算</h2><p>本節將以 TensorFlow 分散式深度學習為例。</p><h3 id="gRPC"><a href="#gRPC" class="headerlink" title="gRPC"></a>gRPC</h3><p>gRPC(google Remote Procedure Call) 是 Google 開發的基於 HTTP/2 和 Protocol Buffer 3 的 RPC 框架，該框架有各種常見語言的實作，如 C、Java 與 Go 等語言，提供輕鬆跨語言的呼叫。</p><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>說明客戶端(Client)、叢集(Cluster)、工作(Job)、任務(Task)、TensorFlow 伺服器、Master 與 Worker 服務。</p><p><img src="http://www.pittnuts.com/wp-content/uploads/2016/08/TFramework.png" alt=""></p><p>如圖所示，幾個流程說明如下：</p><ul><li>整個系统映射到 TensorFlow 叢集.</li><li>參數伺服器映射到一個 Job.</li><li>每個模型(Model)副本映射到一個 Job.</li><li>每台實體運算節點映射到其 Job 中的 Task.</li><li>每個 Task 都有一個 TF Server，並利用 Master 服務來進行溝通與協調工作，而 Worker 服務則透過本地裝置(CPU 或 GPU)進行 TF graph 運算.</li></ul><p>TensorFlow 叢集裡包含了一個或多個工作(Job)，每個工作又可以拆分成一個或多個任務(Task)，簡單說 Cluster 是 Job 的集合，而 Job 是 Task 的集合。叢集概念主要用在一個特定層次對象，如訓練神經網路、平行操作多台機器等，一個叢集物件可以透過<code>tf.train.ClusterSpec</code>來定義。</p><p>如上所述，TensorFlow 的叢集就是一組工作任務，每個任務是一個服務，而服務又分成<code>Master</code>與<code>Worker</code>這兩種，並提供給<code>Client</code>進行操作。</p><ul><li><strong>Client</strong>：是用於建立 TensorFlow 計算 Graph，並建立與叢集進行互動的<code>tensorflow::Session</code>行程，一般由 Python 或 C++ 實作，單一客戶端可以同時連接多個 TF 伺服器連接，同時也能被多個 TF 伺服器連接.</li><li><strong>Master Service</strong>：是一個 RPC 服務行程，用來遠端連線一系列分散式裝置，主要提供<code>tensorflow::Session</code>介面，並負責透過 Worker Service 與工作的任務進行溝通.</li><li><strong>Worker Service</strong>：是一個可以使用本地裝置(CPU 或 GPU)對部分 Graph 進行運算的 RPC 邏輯，透過<code>worker_service.proto</code>介面來實作，所有 TensorFlow 伺服器均包含了 Worker Service 邏輯.</li></ul><blockquote><p><strong>TensorFlow 伺服器</strong>是運行<code>tf.train.Server</code>實例的行程，其為叢集一員，並有 Master 與 Worker 之分。</p></blockquote><p>而 TensorFlow 的工作(Job)可拆成多個相同功能的任務(Task)，這些工作又分成<code>Parameter server</code>與<code>Worker</code>，兩者功能說明如下：</p><p><img src="https://img.tipelse.com/uploads/B/6A/B6A07C1923.jpeg" alt=""></p><ul><li><strong>Parameter server(ps)</strong>:是分散式系統縮放至工業大小機器學習的問題，它提供工作節點與伺服器節點之間的非同步與零拷貝 key-value 的溝通，並支援資料的一致性模型的分散式儲存。在 TensorFlow 中主要根據梯度更新變數，並儲存於<code>tf.Variable</code>，可理解成只儲存 TF Model 的變數，並存放 Variable 副本.</li></ul><p><img src="http://arimo.com/wp-content/uploads/2016/03/TF_Image_0.png" alt=""></p><ul><li><strong>Worker</strong>:通常稱為計算節點，一般管理無狀態(Stateless)，且執行密集型的 Graph 運算資源，並根據變數運算梯度。存放 Graph 副本.</li></ul><p><img src="http://arimo.com/wp-content/uploads/2016/03/TF_Image_1.png" alt=""></p><blockquote><ul><li><a href="http://blog.csdn.net/cyh_24/article/details/50545780" target="_blank" rel="noopener">Parameter Server 詳解</a></li></ul></blockquote><p>一般對於<code>小型規模訓練</code>，這種資料與參數量不多時，可以用一個 CPU 來同時執行兩種任務。而<code>中型規模訓練</code>，資料量較大，但參數量不多時，計算梯度的工作負載較高，而參數更新負載較低，所以計算梯度交給若干個 CPU 或 GPU 去執行，而更新參數則交給一個 CPU 即可。對於<code>大型規模訓練</code>，資料與參數量多時，不僅計算梯度需要部署多個 CPU 或 GPU，連更新參數也要不說到多個 CPU 中。</p><p>然而單一節點能夠裝載的 CPU 與 GPU 是有限的，所以在大量訓練時就需要多台機器來提供運算能力的擴展。</p><h3 id="分散式變數伺服器-Parameter-Server"><a href="#分散式變數伺服器-Parameter-Server" class="headerlink" title="分散式變數伺服器(Parameter Server)"></a>分散式變數伺服器(Parameter Server)</h3><p>當在較大規模的訓練時，隨著模型的變數越來越多，很可能造成單一節點因為效能問題，而無法負荷模型變數儲存與更新時，這時候就需要將變數分開到不同機器來做儲存與更新。而 TensorFlow 提供了變數伺服器的邏輯實現，並可以用多台機器來組成叢集，類似分散式儲存結構，主要用來解決變數的儲存與更新效能問題。</p><h3 id="撰寫分散式程式注意概念"><a href="#撰寫分散式程式注意概念" class="headerlink" title="撰寫分散式程式注意概念"></a>撰寫分散式程式注意概念</h3><p>當我們在寫分散式程式時，需要知道使用的副本與訓練模式。</p><p><img src="https://camo.githubusercontent.com/0b7a1232bd3f8861dfbccab568a30591588384dc/68747470733a2f2f7777772e74656e736f72666c6f772e6f72672f696d616765732f74656e736f72666c6f775f666967757265372e706e67" alt=""></p><h4 id="In-graph-與-Between-graph-副本模式"><a href="#In-graph-與-Between-graph-副本模式" class="headerlink" title="In-graph 與 Between-graph 副本模式"></a>In-graph 與 Between-graph 副本模式</h4><p>下圖顯示兩者差異，而這邊也在進行描述。</p><ul><li><strong>In-graph</strong>：只有一個 Clinet(主要呼叫<code>tf::Session</code>行程)，並將裡面變數與 op 指定給對應的 Job 完成，因此資料分發只由一個 Client 完成。這種方式設定簡單，其他節點只需要 join 操作，並提供一個 gRPC 位址來等待任務。但是訓練資料只在單一節點，因此要把資料分發到不同機器時，會影響平行訓練效能。可理解成所有 op 都在同一個 Graph 中，伺服器只需要做<code>join()</code>功能.</li><li><strong>Between-graph</strong>：多個獨立 Client 建立相同 Graph(包含變數)，並透過<code>tf.train.replica_device_setter</code>將這些參數映射到 ps 上，即訓練的變數儲存在 Parameter Server，而資料不用分發，資料分片(Shards)會存在個計算節點，因此個節點自己算自己的，算完後，把要更新變數告知 Parameter Server 進行更新。適合在 TB 級別的資料量使用，節省大量資料傳輸時間，也是深度學習推薦模式。</li></ul><h4 id="同步-Synchronous-訓練與非同步-Asynchronous-訓鍊"><a href="#同步-Synchronous-訓練與非同步-Asynchronous-訓鍊" class="headerlink" title="同步(Synchronous)訓練與非同步(Asynchronous)訓鍊"></a>同步(Synchronous)訓練與非同步(Asynchronous)訓鍊</h4><p>TensorFlow 的副本擁有 in-graph 和 between-graph 模式，這兩者都支援了同步與非同步更新。本節將說明同步與非同步兩者的差異為何。</p><ul><li><strong>Synchronous</strong>：每個 Graph 的副本讀取相同 Parameter 的值，然後平行計算梯度(gradients)，將所有計算完的梯度放在一起處理，當每次更新梯度時，需要等所以分發的資料計算完成，並回傳結果來把梯度累加計算平均，在進行更新變數。好處在於使用 loss 的下降時比較穩定，壞處就是要等最慢的分片計算時間。</li></ul><blockquote><p>可以利用<code>tf.train.SyncReplicasOptimizer</code>來解決這個問題(在 Between-graph 情況下)，而在 In-graph 則將所有梯度平均即可。</p></blockquote><ul><li><strong>Asynchronous</strong>：自己計算完梯度後，就去更新 paramenter，不同副本之前不會進行協調進度，因此計算資源被充分的利用。缺點是 loss 的下降不穩定。</li></ul><p><img src="http://img.blog.csdn.net/20161114005141032" alt=""></p><p>一般在資料量小，且各節點計算能力平均下，適合使用同步模式; 反之在資料量大與各節點效能差異不同時，適合用非同步。</p><h3 id="簡單分散式訓練程式"><a href="#簡單分散式訓練程式" class="headerlink" title="簡單分散式訓練程式"></a>簡單分散式訓練程式</h3><p>TensorFlow 提供建立 Server 函式來進行測試使用，以下是建立一個分散式訓練 Server 程式<code>server.py</code>：</p><pre><code class="python="># coding=utf-8import tensorflow as tf# 定義 Clustercluster = tf.train.ClusterSpec({&quot;worker&quot;: [&quot;localhost:2222&quot;]})# 建立 Worker serverserver = tf.train.Server(cluster,job_name=&quot;worker&quot;,task_index=0)server.join()</code></pre><blockquote><p>也可以透過<code>tf.train.Server.create_local_server()</code> 來建立 Local Server</p></blockquote><p>當確認程式沒有任何問題後，就可以透過以下方式啟動：</p><pre><code class="shell=">$ python server.py2017-04-10 18:19:41.953448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 650, pci bus id: 0000:01:00.0)2017-04-10 18:19:41.983913: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:200] Initialize GrpcChannelCache for job local -&gt; {0 -&gt; localhost:2222}2017-04-10 18:19:41.984946: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:240] Started server with target: grpc://localhost:2222</code></pre><p>接著我們要撰寫 Client 端來進行定義 Graph 運算的程式<code>client.py</code>：</p><pre><code class="python="># coding=utf-8import tensorflow as tf# 執行目標 Sessionserver_target = &quot;grpc://localhost:2222&quot;logs_path = &#39;./basic_tmp&#39;# 指定 worker task 0 使用 CPU 運算with tf.device(&quot;/job:worker/task:0&quot;):    with tf.device(&quot;/cpu:0&quot;):        a = tf.constant([1.5, 6.0], name=&#39;a&#39;)        b = tf.Variable([1.5, 3.2], name=&#39;b&#39;)        c = (a * b) + (a / b)        d = c * a        y = tf.assign(b, d)# 啟動 Sessionwith tf.Session(server_target) as sess:    sess.run(tf.global_variables_initializer())    writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())    print(sess.run(y))</code></pre><p>完成後即可透過以下指令測試：</p><pre><code class="python=">$ python client.py[   4.875       126.45000458]</code></pre><h3 id="線性迴歸訓練程式"><a href="#線性迴歸訓練程式" class="headerlink" title="線性迴歸訓練程式"></a>線性迴歸訓練程式</h3><p>上面範例提供了很簡單的 Client 與 Server 運算操作。而這邊建立一個 Between-graph 執行程式<code>bg_dist.py</code>：</p><pre><code class="python="># coding=utf-8import tensorflow as tfimport numpy as npparameter_servers = [&quot;localhost:2222&quot;]workers = [&quot;localhost:2223&quot;, &quot;localhost:2224&quot;]tf.app.flags.DEFINE_string(&quot;job_name&quot;, &quot;&quot;, &quot;輸入 &#39;ps&#39; 或是 &#39;worker&#39;&quot;)tf.app.flags.DEFINE_integer(&quot;task_index&quot;, 0, &quot;Job 的任務 index&quot;)FLAGS = tf.app.flags.FLAGSdef main(_):    cluster = tf.train.ClusterSpec({&quot;ps&quot;: parameter_servers, &quot;worker&quot;: workers})    server = tf.train.Server(cluster,job_name=FLAGS.job_name,task_index=FLAGS.task_index)    if FLAGS.job_name == &quot;ps&quot;:        server.join()    elif FLAGS.job_name == &quot;worker&quot;:        train_X = np.linspace(-1.0, 1.0, 100)        train_Y = 2.0 * train_X + np.random.randn(*train_X.shape) * 0.33 + 10.0        X = tf.placeholder(&quot;float&quot;)        Y = tf.placeholder(&quot;float&quot;)        # Assigns ops to the local worker by default.        with tf.device(tf.train.replica_device_setter(                worker_device=&quot;/job:worker/task:%d&quot; % FLAGS.task_index,                cluster=cluster)):            w = tf.Variable(0.0, name=&quot;weight&quot;)            b = tf.Variable(0.0, name=&quot;bias&quot;)            # 損失函式，用於描述模型預測值與真實值的差距大小，常見為`均方差(Mean Squared Error)`            loss = tf.square(Y - tf.multiply(X, w) - b)            global_step = tf.Variable(0)            train_op = tf.train.AdagradOptimizer(0.01).minimize(                loss, global_step=global_step)            saver = tf.train.Saver()            summary_op = tf.summary.merge_all()            init_op = tf.global_variables_initializer()        # 建立 &quot;Supervisor&quot; 來負責監督訓練過程        sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),                                 logdir=&quot;/tmp/train_logs&quot;,                                 init_op=init_op,                                 summary_op=summary_op,                                 saver=saver,                                 global_step=global_step,                                 save_model_secs=600)        with sv.managed_session(server.target) as sess:            loss_value = 100            while not sv.should_stop() and loss_value &gt; 70.0:                # 執行一個非同步 training 步驟.                # 若要執行同步可利用`tf.train.SyncReplicasOptimizer` 來進行                for (x, y) in zip(train_X, train_Y):                    _, step = sess.run([train_op, global_step],                                       feed_dict={X: x, Y: y})                loss_value = sess.run(loss, feed_dict={X: x, Y: y})                print(&quot;步驟: {}, loss: {}&quot;.format(step, loss_value))        sv.stop()if __name__ == &quot;__main__&quot;:    tf.app.run()</code></pre><blockquote><p><code>tf.train.replica_device_setter(ps_tasks=0, ps_device=&#39;/job:ps&#39;, worker_device=&#39;/job:worker&#39;, merge_devices=True, cluster=None, ps_ops=None)</code> 指定方式。</p></blockquote><p>撰寫完成後，透過以下指令來進行測試：</p><pre><code class="shell=">$ python liner_dist.py --job_name=ps --task_index=0$ python liner_dist.py --job_name=worker --task_index=0$ python liner_dist.py --job_name=worker --task_index=1</code></pre><h2 id="Tensorboard-視覺化工具"><a href="#Tensorboard-視覺化工具" class="headerlink" title="Tensorboard 視覺化工具"></a>Tensorboard 視覺化工具</h2><p>Tensorboard 是 TensorFlow 內建的視覺化工具，我們可以透過讀取事件紀錄結構化的資料，來顯示以下幾個項目來提供視覺化：</p><ul><li><strong>Event</strong>：訓練過程中統計資料(平均值等)變化狀態.</li><li><strong>Image</strong>：訓練過程中紀錄的 Graph.</li><li><strong>Audio</strong>：訓練過程中紀錄的 Audio.</li><li><strong>Histogram</strong>：順練過程中紀錄的資料分散圖</li></ul><p>一個範例程式如下所示：</p><pre><code class="python"># coding=utf-8import tensorflow as tflogs_path = &#39;./tmp/1&#39;# 建立一個 graph，並建立兩個常數 op ，這些 op 稱為節點g1 = tf.Graph()with g1.as_default():    a = tf.constant([1.5, 6.0], name=&#39;a&#39;)    b = tf.Variable([1.5, 3.2], name=&#39;b&#39;)    c = (a * b) + (a / b)    d = c * a    y = tf.assign(b, d)# 在 session 執行 graph，並進行資料數據操作 `c`。# 然後指派給 cpu 做運算with tf.Session(graph=g1) as sess_cpu:  with tf.device(&quot;/cpu:0&quot;):      sess_cpu.run(tf.global_variables_initializer())      writer = tf.summary.FileWriter(logs_path, graph=g1)      print(sess_cpu.run(y))</code></pre><p>執行後會看到當前目錄產生<code>tmp_mnist</code> logs 檔案，這時候就可以透過 thensorboard 來視覺化訓練結果：</p><pre><code class="shell=">$ tensorboard --logdir=run1:./tmp/1 --port=6006</code></pre><blockquote><p>run1 是當有多次 log 被載入時做為區別用。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;TensorFlow™ 是利用資料流圖(Data Flow Graphs)來表達數值運算的開放式原始碼函式庫。資料流圖中的節點(Nodes)被用來表示數學運算，而邊(Edges)則用來表示在節點之間互相聯繫的多維資料陣列，即張量(Tensors)。它靈活的架構讓你能夠在不同平台上執行運算，例如 PC 中的一個或多的 CPU(或GPU)、智慧手持裝置與伺服器等。TensorFlow 最初是 Google 機器智能研究所的研究員和工程師開發而成，主要用於機器學習與深度神經網路方面研究。&lt;/p&gt;
    
    </summary>
    
      <category term="TensorFlow" scheme="https://kairen.github.io/categories/TensorFlow/"/>
    
    
      <category term="TensorFlow" scheme="https://kairen.github.io/tags/TensorFlow/"/>
    
      <category term="Machine Learning" scheme="https://kairen.github.io/tags/Machine-Learning/"/>
    
      <category term="Ubuntu" scheme="https://kairen.github.io/tags/Ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>Kuberentes Helm 介紹</title>
    <link href="https://kairen.github.io/2017/03/25/kubernetes/helm-quickstart/"/>
    <id>https://kairen.github.io/2017/03/25/kubernetes/helm-quickstart/</id>
    <published>2017-03-25T09:08:54.000Z</published>
    <updated>2017-08-01T14:23:59.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/kubernetes/helm" target="_blank" rel="noopener">Helm</a> 是 Kubernetes Chart 的管理工具，Kubernetes Chart 是一套預先組態的 Kubernetes 資源套件。使用 Helm 有以下幾個好處：</p><ul><li>查詢與使用熱門的 <a href="https://github.com/kubernetes/charts" target="_blank" rel="noopener">Kubernetes Chart</a> 軟體套件。</li><li>以 Kuberntes Chart 來分享自己的應用程式。</li><li>可利用 Chart 來重複建立應用程式。</li><li>智能地管理 Kubernetes manifest 檔案。</li><li>管理釋出的 Helm 版本。</li></ul><a id="more"></a><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>Helm 有三個觀念需要我們去了解，分別為 Chart、Release 與 Repository，其細節如下：</p><ul><li><strong>Chart</strong>：主要定義要被執行的應用程式中，所需要的工具、資源、服務等資訊，有點類似 Homebrew 的 Formula 或是 APT 的 dpkg 檔案。</li><li><strong>Release</strong>：一個被執行於 Kubernetes 的 Chart 實例。Chart 能夠在一個叢集中擁有多個 Release，例如 MySQL Chart，可以在叢集建立基於該 Chart 的兩個資料庫實例，其中每個 Release 都會有獨立的名稱。</li><li><strong>Repository</strong>：主要用來存放 Chart 的倉庫，如 <a href="https://kubeapps.com/" target="_blank" rel="noopener">KubeApps</a>。</li></ul><p>可以理解 Helm 主要目標就是從 Chart Repository 中，查找部署者需要的應用程式 Chart，然後以 Release 形式來部署到 Kubernetes 中進行管理。</p><h2 id="Helm-系統元件"><a href="#Helm-系統元件" class="headerlink" title="Helm 系統元件"></a>Helm 系統元件</h2><p>Helm 主要分為兩種元件，Helm Client 與 Tiller Server，兩者功能如下：</p><ul><li><strong>Helm Client</strong>：一個安裝 Helm CLI 的機器，該機器透過 gRPC 連接 Tiller Server 來對 Repository、Chart 與 Release 等進行管理與操作，如建立、刪除與升級等操作，細節可以查看 <a href="https://github.com/kubernetes/helm/blob/master/docs/index.md" target="_blank" rel="noopener">Helm Documentation</a>。</li><li><strong>Tiller Server</strong>：主要負責接收來至 Client 的指令，並透過 kube-apiserver 與 Kubernetes 叢集做溝通，根據 Chart 定義的內容，來產生與管理各種對應 API 物件的 Kubernetes 部署檔案(又稱為 <code>Release</code>)。</li></ul><p>兩者溝通架構圖如下所示：</p><center><img src="/images/kube/helm-peer.png" alt=""></center><h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>安裝前需要確認環境滿足以下幾膽：</p><ul><li>已部署 Kubernetes 叢集。</li><li>操作端安裝 kubectl 工具。</li><li>操作端可以透過 kubectl 工具管理到 Kubernetes（可用的 kubectl config）。</li></ul><h2 id="安裝-Helm"><a href="#安裝-Helm" class="headerlink" title="安裝 Helm"></a>安裝 Helm</h2><p>Helm 有許多種安裝方式，這邊個人比較喜歡用 binary 檔案來進行安裝：</p><pre><code class="sh">$ wget -qO- https://kubernetes-helm.storage.googleapis.com/helm-v2.4.1-linux-amd64.tar.gz | tar -zxf$ sudo mv linux-amd64/helm /usr/local/bin/$ helm version</code></pre><blockquote><p>OS X 為下載 <code>helm-v2.4.1-darwin-amd64.tar.gz</code>。</p></blockquote><h2 id="初始化-Helm"><a href="#初始化-Helm" class="headerlink" title="初始化 Helm"></a>初始化 Helm</h2><p>在開始使用 Helm 之前，我們需要建置 Tiller Server 來對 Kubernetes 的管理，而 Helm CLI 內建也提供了快速初始化指令，如下：</p><pre><code class="sh">$ helm init$HELM_HOME has been configured at /root/.helm.Tiller (the helm server side component) has been installed into your Kubernetes Cluster.Happy Helming!</code></pre><blockquote><p>若之前只用舊版想要更新可以透過以下指令<code>helm init --upgrade</code>來達到效果。</p></blockquote><p>完成後，就可以透過 kubectl 來查看 Tiller Server 是否被建立：</p><pre><code class="sh">$ kubectl get po,svc -n kube-system -l app=helmNAME                                READY     STATUS    RESTARTS   AGEpo/tiller-deploy-1651596238-5lsdw   1/1       Running   0          3mNAME                CLUSTER-IP        EXTERNAL-IP   PORT(S)     AGEsvc/tiller-deploy   192.162.204.144   &lt;none&gt;        44134/TCP   3m</code></pre><p>接著透過 helm ctl 來查看資訊：</p><pre><code class="sh">$ export KUBECONFIG=/etc/kubernetes/admin.conf$ export HELM_HOST=$(kubectl describe svc/tiller-deploy -n kube-system | awk &#39;/Endpoints/{print $2}&#39;)$ helm versionClient: &amp;version.Version{SemVer:&quot;v2.4.2&quot;, GitCommit:&quot;82d8e9498d96535cc6787a6a9194a76161d29b4c&quot;, GitTreeState:&quot;clean&quot;}Server: &amp;version.Version{SemVer:&quot;v2.4.2&quot;, GitCommit:&quot;82d8e9498d96535cc6787a6a9194a76161d29b4c&quot;, GitTreeState:&quot;clean&quot;}</code></pre><h2 id="部署-Chart-Release-實例"><a href="#部署-Chart-Release-實例" class="headerlink" title="部署 Chart Release 實例"></a>部署 Chart Release 實例</h2><p>當完成初始化後，就可以透過 helm ctl 來管理與部署 Chart Release，我們可以到 <a href="https://kubeapps.com/" target="_blank" rel="noopener">KubeApps</a> 查找想要部署的 Chart，如以下快速部屬 Jenkins　範例，首先先透過搜尋來查看目前應用程式版本：</p><pre><code class="sh">$ helm search jenkinsNAME              VERSION    DESCRIPTIONstable/jenkins    0.6.3      Open source continuous integration server. It s...</code></pre><p>接著透過<code>inspect</code>指令查看該 Chart 的參數資訊：</p><pre><code class="sh">$ helm inspect stable/jenkins...Persistence:  Enabled: true</code></pre><blockquote><p>從中我們會發現需要建立一個 PVC 來提供持久性儲存。</p></blockquote><p>因此需要建立一個 PVC 提供給 Jenkins Chart 來儲存使用，這邊我們自己手動建立<code>jenkins-pv-pvc.yml</code>檔案：</p><pre><code class="yaml">apiVersion: v1kind: PersistentVolumemetadata:  name: jenkins-pv  labels:    app: jenkinsspec:  capacity:    storage: 10Gi  accessModes:  - ReadWriteOnce  persistentVolumeReclaimPolicy: Recycle  nfs:    path: /var/nfs/jenkins    server: 172.20.3.91---apiVersion: v1kind: PersistentVolumeClaimmetadata:  name: jenkins-pvc  labels:    app: jenkinsspec:  accessModes:  - ReadWriteOnce  resources:    requests:      storage: 10Gi</code></pre><p>接著透過 kubectl 來建立：</p><pre><code class="sh">$ kubectl create -f jenkins-pv-pvc.ymlpersistentvolumeclaim &quot;jenkins-pvc&quot; createdpersistentvolume &quot;jenkins-pv&quot; created$ kubectl get pv,pvcNAME            CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS    CLAIM                 STORAGECLASS   REASON    AGEpv/jenkins-pv   10Gi       RWO           Recycle         Bound     default/jenkins-pvc                            20sNAME              STATUS    VOLUME       CAPACITY   ACCESSMODES   STORAGECLASS   AGEpvc/jenkins-pvc   Bound     jenkins-pv   10Gi       RWO                          20s</code></pre><p>當 PVC 建立完成後，就可以開始透過 Helm 來建立 Jenkins Release：</p><pre><code class="sh">$ export PVC_NAME=$(kubectl get pvc -l app=jenkins --output=template --template=&quot;{{with index .items 0}}{{.metadata.name}}{{end}}&quot;)$ helm install --name demo --set Persistence.ExistingClaim=${PVC_NAME} stable/jenkinsNAME:   demoLAST DEPLOYED: Thu May 25 17:53:50 2017NAMESPACE: defaultSTATUS: DEPLOYEDRESOURCES:==&gt; v1beta1/DeploymentNAME          DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGEdemo-jenkins  1        1        1           0          1s==&gt; v1/SecretNAME          TYPE    DATA  AGEdemo-jenkins  Opaque  2     1s==&gt; v1/ConfigMapNAME                DATA  AGEdemo-jenkins-tests  1     1sdemo-jenkins        3     1s==&gt; v1/ServiceNAME          CLUSTER-IP       EXTERNAL-IP  PORT(S)                         AGEdemo-jenkins  192.169.143.140  &lt;pending&gt;    8080:30152/TCP,50000:31806/TCP  1s...</code></pre><blockquote><p>P.S. <code>install</code> 指令可以安裝來至<code>Chart repository</code>、<code>壓縮檔 Chart</code>、<code>一個 Chart 目錄</code>與<code>Chart URL</code>。</p><p>這邊 install 可以額外透過以下兩種方式來覆寫參數，在這之前可以先透過<code>helm inspect values &lt;chart&gt;</code>來取得使用的變數。</p><ul><li><strong>–values</strong>：指定一個 YAML 檔案來覆寫設定。</li></ul><pre><code class="sh">$ echo -e &#39;Master:\n  AdminPassword: r00tme&#39; &gt; config.yaml$ helm install -f config.yaml stable/jenkins</code></pre><ul><li><strong>–sets</strong>：指定一對 Key/value 指令來覆寫。</li></ul><pre><code class="sh">$ helm install --set Master.AdminPassword=r00tme stable/jenkins</code></pre></blockquote><p>完成後就可以透過 helm 與 kubectl 來查看建立狀態：</p><pre><code class="sh">$ helm lsNAME    REVISION    UPDATED                     STATUS      CHART            NAMESPACEdemo    1           Thu May 25 17:53:50 2017    DEPLOYED    jenkins-0.6.3    default$ kubectl get po,svcNAME                               READY     STATUS    RESTARTS   AGEpo/demo-jenkins-3139496662-c0lzk   1/1       Running   0          1mNAME               CLUSTER-IP        EXTERNAL-IP   PORT(S)                          AGEsvc/demo-jenkins   192.169.143.140   &lt;pending&gt;     8080:30152/TCP,50000:31806/TCP   1m</code></pre><p>由於預設只使用 LoadBalancerSourceRanges 來定義存取策略，但沒有指定任何外部 IP，因此要手動加入以下內容：</p><pre><code class="sh">$ kubectl edit svc demo-jenkinsspec:  externalIPs:  - 172.20.3.90</code></pre><p>完成後再次查看 Service 資訊：</p><pre><code class="sh">$ kubectl get svcNAME           CLUSTER-IP        EXTERNAL-IP    PORT(S)                          AGEdemo-jenkins   192.169.143.140   ,172.20.3.90   8080:30152/TCP,50000:31806/TCP   10m</code></pre><blockquote><p>這時候就可以透過 <a href="http://172.20.3.90:8080" target="_blank" rel="noopener">http://172.20.3.90:8080</a> 連進去 Jenkins 了，其預設帳號為 <code>admin</code>。</p></blockquote><p>透過以下指令來取得 Jenkins admin 密碼：</p><pre><code class="sh">$ printf $(kubectl get secret --namespace default demo-jenkins -o jsonpath=&quot;{.data.jenkins-admin-password}&quot; | base64 --decode);echobuQ1ik2Q7x</code></pre><blockquote><p>該 Chart 會產生亂數密碼存放到 secret 中。</p></blockquote><p><img src="/images/kube/helm-jenkins.png" alt=""></p><p>最後我們也可以透過<code>upgrade</code>指令來更新已經 Release 的 Chart：</p><pre><code class="sh">$ helm upgrade --set Master.AdminPassword=r00tme --set Persistence.ExistingClaim=jenkins-pvc demo stable/jenkinsRelease &quot;demo&quot; has been upgraded. Happy Helming!$ helm get values demoMaster:  AdminPassword: r00tmePersistence:  ExistingClaim: jenkins-pvc$ helm lsNAME    REVISION        UPDATED                         STATUS          CHART           NAMESPACEdemo    2               Tue May 30 21:18:43 2017        DEPLOYED        jenkins-0.6.3   default</code></pre><blockquote><p>這邊會看到<code>REVISION</code>會 +1，這可以用來做 rollback 的版本號使用。</p></blockquote><h2 id="刪除-Release"><a href="#刪除-Release" class="headerlink" title="刪除 Release"></a>刪除 Release</h2><p>Helm 除了基本的建立功能外，其還包含了整個 Release 的生命週期管理功能，如我們不需要該 Release 時，就可以透過以下方式刪除：</p><pre><code class="sh">$ helm del demo$ helm status demo | grep STATUSSTATUS: DELETED</code></pre><p>當刪除後，該 Release 並沒有真的被刪除，我們可以透過 helm ls 來查看被刪除的 Release：</p><pre><code class="sh">$ helm ls --allNAME    REVISION        UPDATED                         STATUS  CHART           NAMESPACEdemo    2               Tue May 30 21:18:43 2017        DELETED jenkins-0.6.3   default</code></pre><blockquote><p>當執行 <code>helm ls</code> 指令為加入 <code>--all</code> 時，表示只列出<code>DEPLOYED</code>狀態的 Release。</p></blockquote><p>而當 Release 處於 <code>DELETED</code> 狀態時，我們可以進行一些操作，如 Roll back 或完全刪除 Release：</p><pre><code class="sh">$ helm rollback demo 1Rollback was a success! Happy Helming!$ printf $(kubectl get secret --namespace default demo-jenkins -o jsonpath=&quot;{.data.jenkins-admin-password}&quot; | base64 --decode);echoBIsLlQTN9l$ helm del demo --purgerelease &quot;demo&quot; deleted# 這時執行以下指令就不會再看到已刪除的 Release.$ helm ls --all</code></pre><h2 id="建立簡單-Chart-結構"><a href="#建立簡單-Chart-結構" class="headerlink" title="建立簡單 Chart 結構"></a>建立簡單 Chart 結構</h2><p>Helm 提供了 create 指令來建立一個 Chart 基本結構：</p><pre><code class="sh">$ helm create example$ tree example/example/├── charts├── Chart.yaml├── templates│   ├── deployment.yaml│   ├── _helpers.tpl│   ├── ingress.yaml│   ├── NOTES.txt│   └── service.yaml└── values.yaml</code></pre><p>當我們設定完 Chart 後，就可以透過 helm 指令來打包：</p><pre><code class="sh">$ helm package example/example-0.1.0.tgz</code></pre><p>最後可以用 helm 來安裝：</p><pre><code class="sh">$ helm install ./example-0.1.0.tgz</code></pre><h2 id="自己建立-Repository"><a href="#自己建立-Repository" class="headerlink" title="自己建立 Repository"></a>自己建立 Repository</h2><p>Helm 指令除了可以建立 Chart 基本結構外，很幸運的也提供了建立 Helm Repository 的功能，建立方式如下：</p><pre><code class="sh">$ helm serve --repo-path example-0.1.0.tgz$ helm repo add example http://repo-url</code></pre><blockquote><p>另外 helm repo 也可以加入來至於 Github 與 HTTP 伺服器的網址來提供服務。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://github.com/kubernetes/helm&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Helm&lt;/a&gt; 是 Kubernetes Chart 的管理工具，Kubernetes Chart 是一套預先組態的 Kubernetes 資源套件。使用 Helm 有以下幾個好處：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;查詢與使用熱門的 &lt;a href=&quot;https://github.com/kubernetes/charts&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Kubernetes Chart&lt;/a&gt; 軟體套件。&lt;/li&gt;
&lt;li&gt;以 Kuberntes Chart 來分享自己的應用程式。&lt;/li&gt;
&lt;li&gt;可利用 Chart 來重複建立應用程式。&lt;/li&gt;
&lt;li&gt;智能地管理 Kubernetes manifest 檔案。&lt;/li&gt;
&lt;li&gt;管理釋出的 Helm 版本。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://kairen.github.io/categories/Kubernetes/"/>
    
    
      <category term="Docker" scheme="https://kairen.github.io/tags/Docker/"/>
    
      <category term="Helm" scheme="https://kairen.github.io/tags/Helm/"/>
    
      <category term="Kubernetes" scheme="https://kairen.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubespray 部署實體機 Kubernetes v1.6 叢集</title>
    <link href="https://kairen.github.io/2017/03/17/kubernetes/deploy/kubespray-baremetal/"/>
    <id>https://kairen.github.io/2017/03/17/kubernetes/deploy/kubespray-baremetal/</id>
    <published>2017-03-17T09:08:54.000Z</published>
    <updated>2017-09-28T06:00:31.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/kubernetes-incubator/kubespray" target="_blank" rel="noopener">Kubespray</a> 是 Kubernetes incubator 中的專案，目標是提供 Production Ready Kubernetes 部署方案，該專案基礎是透過 Ansible Playbook 來定義系統與 Kubernetes 叢集部署的任務，目前 Kubespray 有以下幾個特點：</p><ul><li>可以部署在 AWS, GCE, Azure, OpenStack 或者 Baremetal.</li><li>部署 High Available Kubernetes 叢集.</li><li>可組合性(Composable)，可自行選擇 Network Plugin (flannel, calico, canal, weave) 來部署.</li><li>支援多種 Linux distributions(CoreOS, Debian Jessie, Ubuntu 16.04, CentOS/RHEL7).</li></ul><p>本篇將說明如何透過 Kubespray 部署 Kubernetes 至實體機器節點，安裝版本如下所示：</p><ul><li>Kubernetes v1.6.4</li><li>Etcd v3.1.6</li><li>Flannel v0.7.1</li><li>Docker v17.04.0-ce</li></ul><a id="more"></a><h2 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h2><p>本次安裝測試環境的作業系統採用<code>Ubuntu 16.04 Server</code>，其他細節內容如下：</p><table><thead><tr><th>IP Address</th><th>Role</th><th>CPU</th><th>Memory</th></tr></thead><tbody><tr><td>192.168.121.179</td><td>master1 + deploy</td><td>2</td><td>4G</td></tr><tr><td>192.168.121.106</td><td>node1</td><td>2</td><td>4G</td></tr><tr><td>192.168.121.197</td><td>node2</td><td>2</td><td>4G</td></tr><tr><td>192.168.121.123</td><td>node3</td><td>2</td><td>4G</td></tr></tbody></table><blockquote><p>這邊 master 為主要控制節點，node 為應用程式工作節點。</p></blockquote><h2 id="預先準備資訊"><a href="#預先準備資訊" class="headerlink" title="預先準備資訊"></a>預先準備資訊</h2><ul><li>所有節點的網路之間可以互相溝通。</li><li><code>部署節點(這邊為 master1)</code>對其他節點不需要 SSH 密碼即可登入。</li><li>所有節點都擁有 Sudoer 權限，並且不需要輸入密碼。</li><li>所有節點需要安裝 <code>Python</code>。</li><li><p>所有節點需要設定<code>/etc/host</code>解析到所有主機。</p></li><li><p>修改所有節點<code>/etc/resolv.conf</code>：</p></li></ul><pre><code class="sh">$ echo &quot;nameserver 8.8.8.8&quot; | sudo tee /etc/resolv.conf</code></pre><ul><li><code>部署節點(這邊為 master1)</code>需要安裝 Ansible &gt; 2.3.0。</li></ul><p>Ubuntu 16.04 安裝最新版 Ansible:</p><pre><code class="sh">$ sudo sed -i &#39;s/us.archive.ubuntu.com/tw.archive.ubuntu.com/g&#39; /etc/apt/sources.list$ sudo apt-get install -y software-properties-common$ sudo apt-add-repository -y ppa:ansible/ansible$ sudo apt-get update &amp;&amp; sudo apt-get install -y ansible git cowsay python-pip python-netaddr libssl-dev</code></pre><h2 id="安裝-Kubespray-與準備部署資訊"><a href="#安裝-Kubespray-與準備部署資訊" class="headerlink" title="安裝 Kubespray 與準備部署資訊"></a>安裝 Kubespray 與準備部署資訊</h2><p>首先透過 pypi 安裝 kubespray-cli，雖然官方說已經改成 Go 語言版本的工具，但是根本沒在更新，所以目前暫時用 pypi 版本：</p><pre><code class="sh">$ sudo pip install -U kubespray</code></pre><p>安裝完成後，新增設定檔<code>~/.kubespray.yml</code>，並加入以下內容：</p><pre><code class="sh">$ mkdir /etc/kubespray$ cat &lt;&lt;EOF &gt; ~/.kubespray.ymlkubespray_git_repo: &quot;https://github.com/kubernetes-incubator/kubespray.git&quot;# Logging optionsloglevel: &quot;info&quot;EOF</code></pre><p>接著用 kubespray cli 來產生 inventory 檔案：</p><pre><code class="sh">$ kubespray prepare --masters master1 --etcds master1 --nodes node1 node2 node3$ cat ~/.kubespray/inventory/inventory.cfg</code></pre><blockquote><p>也可以自己建立<code>inventory</code>來描述部署節點。</p></blockquote><p>完成後就可以透過以下指令進行部署 Kubernetes 叢集：</p><pre><code class="sh">$ time kubespray deploy --verbose -u root -k .ssh/id_rsa -n flannelRun kubernetes cluster deployment with the above command ? [Y/n]y...master1                    : ok=368  changed=89   unreachable=0    failed=0node1                      : ok=305  changed=73   unreachable=0    failed=0node2                      : ok=276  changed=62   unreachable=0    failed=0node3                      : ok=276  changed=62   unreachable=0    failed=0Kubernetes deployed successfuly</code></pre><blockquote><p>其中<code>-n</code>為部署的網路插件類型，目前支援 calico、flannel、weave 與 canal。</p></blockquote><h2 id="驗證叢集"><a href="#驗證叢集" class="headerlink" title="驗證叢集"></a>驗證叢集</h2><p>當 Ansible 執行完成後，若沒發生錯誤就可以開始進行操作 Kubernetes，如取得版本資訊：</p><pre><code class="sh">$ kubectl versionClient Version: version.Info{Major:&quot;1&quot;, Minor:&quot;6&quot;, GitVersion:&quot;v1.6.4+coreos.0&quot;, GitCommit:&quot;9212f77ed8c169a0afa02e58dce87913c6387b3e&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2017-04-04T00:32:53Z&quot;, GoVersion:&quot;go1.7.5&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;}Server Version: version.Info{Major:&quot;1&quot;, Minor:&quot;6&quot;, GitVersion:&quot;v1.6.4+coreos.0&quot;, GitCommit:&quot;9212f77ed8c169a0afa02e58dce87913c6387b3e&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2017-04-04T00:32:53Z&quot;, GoVersion:&quot;go1.7.5&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;}</code></pre><p>取得目前節點狀態：</p><pre><code class="sh">$ kubectl get nodeNAME      STATUS                     AGE       VERSIONmaster1   Ready,SchedulingDisabled   11m       v1.6.4+coreos.0node1     Ready                      11m       v1.6.4+coreos.0node2     Ready                      11m       v1.6.4+coreos.0node3     Ready                      11m       v1.6.4+coreos.0</code></pre><p>查看目前系統 Pod 狀態：</p><pre><code class="sh">$ kubectl get po -n kube-systemNAME                                  READY     STATUS    RESTARTS   AGEdnsmasq-975202658-6jj3n               1/1       Running   0          14mdnsmasq-975202658-h4rn9               1/1       Running   0          14mdnsmasq-autoscaler-2349860636-kfpx0   1/1       Running   0          14mflannel-master1                       1/1       Running   1          14mflannel-node1                         1/1       Running   1          14mflannel-node2                         1/1       Running   1          14mflannel-node3                         1/1       Running   1          14mkube-apiserver-master1                1/1       Running   0          15mkube-controller-manager-master1       1/1       Running   0          15mkube-proxy-master1                    1/1       Running   1          14mkube-proxy-node1                      1/1       Running   1          14mkube-proxy-node2                      1/1       Running   1          14mkube-proxy-node3                      1/1       Running   1          14mkube-scheduler-master1                1/1       Running   0          15mkubedns-1519522227-thmrh              3/3       Running   0          14mkubedns-autoscaler-2999057513-tx14j   1/1       Running   0          14mnginx-proxy-node1                     1/1       Running   1          14mnginx-proxy-node2                     1/1       Running   1          14mnginx-proxy-node3                     1/1       Running   1          14m</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://github.com/kubernetes-incubator/kubespray&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Kubespray&lt;/a&gt; 是 Kubernetes incubator 中的專案，目標是提供 Production Ready Kubernetes 部署方案，該專案基礎是透過 Ansible Playbook 來定義系統與 Kubernetes 叢集部署的任務，目前 Kubespray 有以下幾個特點：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可以部署在 AWS, GCE, Azure, OpenStack 或者 Baremetal.&lt;/li&gt;
&lt;li&gt;部署 High Available Kubernetes 叢集.&lt;/li&gt;
&lt;li&gt;可組合性(Composable)，可自行選擇 Network Plugin (flannel, calico, canal, weave) 來部署.&lt;/li&gt;
&lt;li&gt;支援多種 Linux distributions(CoreOS, Debian Jessie, Ubuntu 16.04, CentOS/RHEL7).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本篇將說明如何透過 Kubespray 部署 Kubernetes 至實體機器節點，安裝版本如下所示：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes v1.6.4&lt;/li&gt;
&lt;li&gt;Etcd v3.1.6&lt;/li&gt;
&lt;li&gt;Flannel v0.7.1&lt;/li&gt;
&lt;li&gt;Docker v17.04.0-ce&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://kairen.github.io/categories/Kubernetes/"/>
    
    
      <category term="Docker" scheme="https://kairen.github.io/tags/Docker/"/>
    
      <category term="Kubernetes" scheme="https://kairen.github.io/tags/Kubernetes/"/>
    
      <category term="Ansible" scheme="https://kairen.github.io/tags/Ansible/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu 16.04 安裝 TensorFlow GPU GTX 1060</title>
    <link href="https://kairen.github.io/2017/03/12/tensorflow/install-source/"/>
    <id>https://kairen.github.io/2017/03/12/tensorflow/install-source/</id>
    <published>2017-03-12T08:23:01.000Z</published>
    <updated>2017-08-01T14:23:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>本篇主要因為自己買了一片<code>Nvidia GTX 1060 6G</code>顯卡，但是購買至今只用來玩過一個遊戲，因此才拿來試跑 TensorFlow。</p><a id="more"></a><p>本次安裝硬體與規格如下：</p><ul><li>作業系統: Ubuntu 16.04 Desktop</li><li>GPU: GeForce® GTX 1060 6G</li><li>NVIDIA Driver: nvidia-367</li><li>Python: 2.7+</li><li>TensorFlow: r1.0.1</li><li>CUDA: v8.0</li><li>cuDNN: v5.1</li></ul><h1 id="環境部署"><a href="#環境部署" class="headerlink" title="環境部署"></a>環境部署</h1><p>如果要安裝 TensorFlow with GPU support 的話，需要滿足以下幾點：</p><ul><li>Nvidia Driver.</li><li>已安裝 CUDA® Toolkit 8.0.</li><li>已安裝 cuDNN v5.1.</li><li>GPU card with CUDA Compute Capability 6.1(GTX 10-series).</li><li>libcupti-dev 函式庫.</li></ul><h2 id="Nvidia-Driver-安裝"><a href="#Nvidia-Driver-安裝" class="headerlink" title="Nvidia Driver 安裝"></a>Nvidia Driver 安裝</h2><p>由於預設 Ubuntu 的 Nvidia 版本比較舊，或者並沒有安裝相關驅動，因此這邊需要安裝顯卡對應的版本才能夠正常使用，可以透過以下方式進行：</p><pre><code class="sh">$ sudo add-apt-repository -y ppa:graphics-drivers/ppa$ sudo apt-get update$ sudo apt-get install -y nvidia-367</code></pre><p>完成後，需重新啟動機器。</p><h2 id="CUDA-Toolkit-8-0-安裝"><a href="#CUDA-Toolkit-8-0-安裝" class="headerlink" title="CUDA Toolkit 8.0 安裝"></a>CUDA Toolkit 8.0 安裝</h2><p>由於 TensorFlow 支援 GPU 運算時，會需要使用到 CUDA Toolkit 相關功能，可以到 <a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener">CUDA Toolkit</a> 頁面下載，這邊會下載 Ubuntu Run file 檔案，來進行安裝：</p><pre><code class="sh">$ wget &quot;https://developer.nvidia.com/compute/cuda/8.0/Prod2/local_installers/cuda_8.0.61_375.26_linux-run&quot;$ sudo chmod u+x cuda_8.0.61_375.26_linux-run$ ./cuda_8.0.61_375.26_linux-runDo you accept the previously read EULA?accept/decline/quit: acceptInstall NVIDIA Accelerated Graphics Driver for Linux-x86_64 361.77?(y)es/(n)o/(q)uit: nInstall the CUDA 8.0 Toolkit?(y)es/(n)o/(q)uit: yEnter Toolkit Location[ default is /usr/local/cuda-8.0]: enterDo you want to install a symbolic link at /usr/local/cuda?(y)es/(n)o/(q)uit:yInstall the CUDA 8.0 Samples?(y)es/(n)o/(q)uit:yEnter CUDA Samples Location[ defualt is /home/kylebai ]: enter</code></pre><blockquote><p>這邊<code>enter</code>為鍵盤直接按壓，而不是輸入 enter。</p></blockquote><p>安裝完成後，編輯 Home 目錄底下的<code>.bashrc</code>檔案加入以下內容：</p><pre><code class="sh">export PATH=${PATH}:/usr/local/cuda-8.0/binexport LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64</code></pre><p>最後 Source Bash 檔案與測試 CUDA Toolkit：</p><pre><code class="sh">$ source .bashrc$ sudo nvidia-smi+-----------------------------------------------------------------------------+| NVIDIA-SMI 375.39                 Driver Version: 375.39                    ||-------------------------------+----------------------+----------------------+| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC || Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. ||===============================+======================+======================||   0  GeForce GTX 106...  Off  | 0000:01:00.0      On |                  N/A || 28%   29C    P8     6W / 120W |    130MiB /  6069MiB |      0%      Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes:                                                       GPU Memory ||  GPU       PID  Type  Process name                               Usage      ||=============================================================================||    0      1165    G   /usr/lib/xorg/Xorg                              98MiB ||    0      1764    G   compiz                                          29MiB |+-----------------------------------------------------------------------------+</code></pre><h2 id="cuDNN-5-1-安裝"><a href="#cuDNN-5-1-安裝" class="headerlink" title="cuDNN 5.1 安裝"></a>cuDNN 5.1 安裝</h2><p><a href="https://developer.nvidia.com/rdp/cudnn-download" target="_blank" rel="noopener">NVIDIA cuDNN</a> 是一個深度神經網路運算的 GPU 加速原函式庫，這邊需要點選前面的連結，下載<code>cuDNN v5.1 Library for Linux</code>檔案：</p><pre><code class="sh">$ tar xvf cudnn-8.0-linux-x64-v5.1.tgz$ sudo cp cuda/include/cudnn.h /usr/local/cuda/include/$ sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/</code></pre><h2 id="TensorFlow-GPU-套件建構"><a href="#TensorFlow-GPU-套件建構" class="headerlink" title="TensorFlow GPU 套件建構"></a>TensorFlow GPU 套件建構</h2><p>本次教學將透過 <a href="https://github.com/tensorflow/tensorflow" target="_blank" rel="noopener">Source code</a> 建構安裝檔，再進行安裝 TensorFlow，首先安裝相依套件：</p><pre><code class="sh">$ sudo add-apt-repository -y ppa:webupd8team/java$ sudo apt-get update$ sudo apt-get install -y libcupti-dev python-numpy python-dev python-setuptools python-pip python-wheel git oracle-java8-installer$ echo &quot;deb [arch=amd64] http://storage.googleapis.com/bazel-apt stable jdk1.8&quot; | sudo tee /etc/apt/sources.list.d/bazel.list$ curl -s &quot;https://storage.googleapis.com/bazel-apt/doc/apt-key.pub.gpg&quot; | sudo apt-key add -$ sudo apt-get update &amp;&amp; sudo apt-get -y install bazel$ sudo apt-get upgrade -y bazel</code></pre><p>接著取得 TensorFlow 專案原始碼，然後進入到 TensorFlow 專案目錄進行 bazel 設定：</p><pre><code class="sh">$ git clone &quot;https://github.com/tensorflow/tensorflow&quot;$ cd tensorflow$ ./configure...Do you wish to build TensorFlow with CUDA support? [y/N] yPlease specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0Please specify the cuDNN version you want to use. [Leave empty to use system default]: 5Please note that each additional compute capability significantly increases your build time and binary size.[Default is: &quot;3.5,5.2&quot;]: 6.1...Configuration finished</code></pre><blockquote><p><code>6.1</code>為 GTX 10-series 系列顯卡，其他可以查看 <a href="https://developer.nvidia.com/cuda-gpus" target="_blank" rel="noopener">CUDA GPUS</a>。這邊除了上述特定要輸入外，其餘都是直接鍵盤<code>enter</code>。</p></blockquote><p>當完成組態後，即可透過 bazel 進行建構 pip 套件腳本：</p><pre><code class="sh">$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package</code></pre><p>當腳本建構完成後，即可透過以下指令來建構 .whl 檔案：</p><pre><code class="sh">$ bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tf_pkg</code></pre><p>完成後，可以在<code>/tmp/tf_pkg</code>目錄底下找到安裝檔<code>tensorflow-1.0.1-py2-none-any.whl</code>，最後就可以透過 pip 來進行安裝了：</p><pre><code class="sh">$ sudo pip install /tmp/tf_pkg/tensorflow-1.0.1-cp27-cp27mu-linux_x86_64.whl</code></pre><h2 id="測試安裝結果"><a href="#測試安裝結果" class="headerlink" title="測試安裝結果"></a>測試安裝結果</h2><p>最後透過簡單程式來驗證安裝是否成功：</p><pre><code class="sh">$ cat &lt;&lt;EOF &gt; simple.pyimport tensorflow as tfhello = tf.constant(&#39;Hello, TensorFlow!&#39;)sess = tf.Session()print(sess.run(hello))EOF$ python simple.py...roperties:name: GeForce GTX 1060 6GBmajor: 6 minor: 1 memoryClockRate (GHz) 1.7845pciBusID 0000:01:00.0Total memory: 5.93GiBFree memory: 5.74GiB2017-03-12 21:43:56.477084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 02017-03-12 21:43:56.477092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y2017-03-12 21:43:56.503464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0)Hello, TensorFlow!</code></pre><blockquote><p><code>...</code>部分會顯示一些 GPU 使用狀態。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇主要因為自己買了一片&lt;code&gt;Nvidia GTX 1060 6G&lt;/code&gt;顯卡，但是購買至今只用來玩過一個遊戲，因此才拿來試跑 TensorFlow。&lt;/p&gt;
    
    </summary>
    
      <category term="TensorFlow" scheme="https://kairen.github.io/categories/TensorFlow/"/>
    
    
      <category term="TensorFlow" scheme="https://kairen.github.io/tags/TensorFlow/"/>
    
      <category term="Machine Learning" scheme="https://kairen.github.io/tags/Machine-Learning/"/>
    
      <category term="Ubuntu" scheme="https://kairen.github.io/tags/Ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>kube-ansible 快速部署實體機 HA 叢集</title>
    <link href="https://kairen.github.io/2017/02/19/kubernetes/deploy/kube-ansible-metal/"/>
    <id>https://kairen.github.io/2017/02/19/kubernetes/deploy/kube-ansible-metal/</id>
    <published>2017-02-19T09:08:54.000Z</published>
    <updated>2017-12-01T11:09:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>本篇說明如何透過 <a href="https://github.com/kairen/kube-ansible" target="_blank" rel="noopener">kube-ansible</a> 部署多節點實體機 Kubernetes 叢集。</p><p>本安裝各軟體版本如下：</p><ul><li>Kubernetes v1.8.3</li><li>Etcd v3.2.9</li><li>Docker v1.13.0+</li></ul><a id="more"></a><p>而在 OS 部分以支援<code>Ubuntu 16.x</code>及<code>CentOS 7.x</code>的虛擬機與實體機部署。</p><h2 id="節點資訊"><a href="#節點資訊" class="headerlink" title="節點資訊"></a>節點資訊</h2><p>本次安裝作業系統採用<code>Ubuntu 16.04 Server</code>，測試環境為實體主機：</p><table><thead><tr><th>IP Address</th><th>Role</th><th>CPU</th><th>Memory</th></tr></thead><tbody><tr><td>172.20.3.90</td><td>VIP</td><td></td><td></td></tr><tr><td>172.20.3.91</td><td>master1</td><td>2</td><td>4G</td></tr><tr><td>172.20.3.92</td><td>master2</td><td>2</td><td>4G</td></tr><tr><td>172.20.3.93</td><td>master3</td><td>2</td><td>4G</td></tr><tr><td>172.20.3.94</td><td>node1</td><td>4</td><td>8G</td></tr><tr><td>172.20.3.95</td><td>node2</td><td>4</td><td>8G</td></tr><tr><td>172.20.3.96</td><td>node3</td><td>4</td><td>8G</td></tr><tr><td>172.20.3.97</td><td>node4</td><td>4</td><td>8G</td></tr><tr><td>172.20.3.98</td><td>node5</td><td>4</td><td>8G</td></tr></tbody></table><h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>安裝前需要確認以下幾個項目：</p><ul><li>所有節點的網路之間可以互相溝通。</li><li><code>部署節點(這邊為 master1)</code>對其他節點不需要 SSH 密碼即可登入。</li><li>所有節點都擁有 Sudoer 權限，並且不需要輸入密碼。</li><li>所有節點需要安裝 <code>Python</code>。</li><li>所有節點需要設定<code>/etc/host</code>解析到所有主機。</li><li><code>部署節點(這邊為 master1)</code>需要安裝 Ansible。</li></ul><p>Ubuntu 16.04 安裝 Ansible:</p><pre><code class="sh">$ sudo apt-get install -y software-properties-common git cowsay$ sudo apt-add-repository -y ppa:ansible/ansible$ sudo apt-get update &amp;&amp; sudo apt-get install -y ansible</code></pre><p>CentOS 7 安裝 Ansible：</p><pre><code class="sh">$ sudo yum install -y epel-release$ sudo yum -y install ansible cowsay</code></pre><h2 id="部署-Kubernetes-叢集"><a href="#部署-Kubernetes-叢集" class="headerlink" title="部署 Kubernetes 叢集"></a>部署 Kubernetes 叢集</h2><p>首先透過 Git 取得 HA Kubernetes Ansible 的專案：</p><pre><code class="sh">$ git clone &quot;https://github.com/kairen/kube-ansible.git&quot;$ cd kube-ansible</code></pre><p>然後編輯<code>inventory</code>檔案，來加入要部署的節點角色：</p><pre><code>[etcds]172.20.3.[91:93][masters]172.20.3.[91:93][nodes]172.20.3.[94:98][kube-cluster:children]mastersnodes[kube-addon:children]masters</code></pre><p>完成後接著編輯<code>group_vars/all.yml</code>，來根據需求設定參數，範例如下：</p><pre><code class="yml"># Kubenrtes version, only support 1.8.0+.kube_version: 1.8.3# CRI plugin,# Supported runtime: docker, containerd.cri_plugin: docker# CNI plugin,# Supported network: flannel, calico, canal, weave or router.network: calicopod_network_cidr: 10.244.0.0/16# Kubernetes cluster networkcluster_subnet: 10.96.0kubernetes_service_ip: &quot;{{ cluster_subnet }}.1&quot;service_ip_range: &quot;{{ cluster_subnet }}.0/12&quot;service_node_port_range: 30000-32767# apiserver lb 與 viplb_vip_address: 172.20.3.90lb_secure_port: 6443lb_api_url: &quot;https://{{ lb_vip_address }}:{{ lb_secure_port }}&quot;# 若有內部 registry 則需要設定insecure_registrys:# - &quot;gcr.io&quot;# Core addons (Strongly recommend)kube_dns: truedns_name: cluster.local # cluster dns namedns_ip: &quot;{{ cluster_subnet }}.10&quot;kube_proxy: truekube_proxy_mode: iptables # &quot;ipvs(1.8+)&quot;, &quot;iptables&quot; or &quot;userspace&quot;.# Extra addonskube_dashboard: true # Kubenetes dasobhard console.kube_logging: false # EFK stack for Kuberneteskube_monitoring: true # Grafana + Infuxdb + Heapster monitoring# Ingress controlleringress: trueingress_type: nginx # &#39;nginx&#39;, &#39;haproxy&#39;, &#39;traefik&#39;</code></pre><p>確認<code>group_vars/all.yml</code>完成後，透過 ansible ping 來檢查叢集狀態：</p><pre><code class="sh">$ ansible all -m ping172.20.3.91 | SUCCESS =&gt; {    &quot;changed&quot;: false,    &quot;ping&quot;: &quot;pong&quot;}...</code></pre><p>接著就可以透過以下指令進行部署叢集：</p><pre><code class="sh">$ ansible-playbook cluster.yml</code></pre><p>執行後需要等一點時間，當完成後就可以進入任何一台 Master 進行操作：</p><pre><code class="sh">$ kubectl get nodeNAME      STATUS            AGEmaster1   Ready,master      3mmaster2   Ready,master      3mmaster3   Ready,master      3mnode1     Ready             1mnode2     Ready             1mnode3     Ready             1mnode4     Ready             1mnode5     Ready             1m</code></pre><p>接著就可以部署 Addons 了，透過以下方式進行：</p><pre><code class="sh">$ ansible-playbook addons.yml</code></pre><h2 id="驗證叢集"><a href="#驗證叢集" class="headerlink" title="驗證叢集"></a>驗證叢集</h2><p>當完成上述步驟後，就可以在任一台<code>master</code>節點進行操作 Kubernetes：</p><pre><code class="sh">$ kubectl get po -n kube-systemNAME                                    READY     STATUS    RESTARTS   AGE...kubernetes-dashboard-1765530275-rxbkw   1/1       Running   0          1m</code></pre><blockquote><p>確認都是<code>Running</code>後，就可以進入 <a href="https://172.20.3.90:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/" target="_blank" rel="noopener">Dashboard</a>。</p></blockquote><p>接著透過 Etcd 來查看目前 Leader 狀態：</p><pre><code class="sh">$ export CA=&quot;/etc/etcd/ssl&quot;$ ETCDCTL_API=3 etcdctl \    --cacert=${CA}/etcd-ca.pem \    --cert=${CA}/etcd.pem \    --key=${CA}/etcd-key.pem \    --endpoints=&quot;https://172.20.3.91:2379&quot; \    etcdctl member list2de3b0eee054a36f: name=master1 peerURLs=http://172.20.3.91:2380 clientURLs=http://172.20.3.91:2379 isLeader=false75809e2ee8d8d4b4: name=master2 peerURLs=http://172.20.3.92:2380 clientURLs=http://172.20.3.92:2379 isLeader=falseaf31edd02fc70872: name=master3 peerURLs=http://172.20.3.93:2380 clientURLs=http://172.20.3.93:2379 isLeader=true</code></pre><h2 id="重置叢集"><a href="#重置叢集" class="headerlink" title="重置叢集"></a>重置叢集</h2><p>若想要將整個叢集進行重置的話，可以使用以下方式：</p><pre><code class="sh">$ ansible-playbook reset.yml</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇說明如何透過 &lt;a href=&quot;https://github.com/kairen/kube-ansible&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;kube-ansible&lt;/a&gt; 部署多節點實體機 Kubernetes 叢集。&lt;/p&gt;
&lt;p&gt;本安裝各軟體版本如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes v1.8.3&lt;/li&gt;
&lt;li&gt;Etcd v3.2.9&lt;/li&gt;
&lt;li&gt;Docker v1.13.0+&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://kairen.github.io/categories/Kubernetes/"/>
    
    
      <category term="Docker" scheme="https://kairen.github.io/tags/Docker/"/>
    
      <category term="Kubernetes" scheme="https://kairen.github.io/tags/Kubernetes/"/>
    
      <category term="Ansible" scheme="https://kairen.github.io/tags/Ansible/"/>
    
  </entry>
  
  <entry>
    <title>kube-ansible 快速部署 HA 測試環境</title>
    <link href="https://kairen.github.io/2017/02/17/kubernetes/deploy/kube-ansible-test/"/>
    <id>https://kairen.github.io/2017/02/17/kubernetes/deploy/kube-ansible-test/</id>
    <published>2017-02-17T09:08:54.000Z</published>
    <updated>2017-12-01T11:13:39.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/kairen/kube-ansible" target="_blank" rel="noopener">kube-ansible</a> 提供自動化部署 Kubernetes High Availability 叢集於虛擬機與實體機上，並且支援部署 Ceph 叢集於 Kubernetes 中提供共享式儲存系統給 Pod 應用程式使用。該專案最主要是想要快速建立測試環境來進行 Kubernetes 練習與驗證。</p><p>kube-ansible 提供了以下幾項功能：</p><ul><li>Kubernetes 1.8.0+.</li><li>Ceph on Kubernetes cluster.</li><li>Common addons.</li></ul><a id="more"></a><p>而在 OS 部分以支援<code>Ubuntu 16.x</code>及<code>CentOS 7.x</code>的虛擬機與實體機部署。未來會以 Python 工具形式來提供使用。</p><h2 id="快速開始"><a href="#快速開始" class="headerlink" title="快速開始"></a>快速開始</h2><p>kube-ansible 支援了 Vagrant 腳本來快速提供 VirtualBox 環境，若想單一主機模擬 Kubernetes 叢集的話，主機需要安裝以下軟體工具：</p><ul><li><a href="https://www.vagrantup.com/downloads.html" target="_blank" rel="noopener">Vagrant</a> &gt;= 1.7.0</li><li><a href="https://www.virtualbox.org/wiki/Downloads" target="_blank" rel="noopener">VirtualBox</a> &gt;= 5.0.0</li></ul><p>當主機確認安裝完成後，即可透過 Git 下載最新版本程式，並使用<code>setup-vagrant</code>腳本：</p><pre><code class="sh">$ git clone &quot;https://github.com/kairen/kube-ansible.git&quot;$ cd kube-ansible$ ./tools/setup -hUsage : setup-vagrant [options] -b|--boss         Number of master. -w|--worker       Number of worker. -c|--cpu          Number of cores per vm. -m|--memory       Memory size per vm. -p|--provider     Virtual machine provider(virtualbox, libvirt). -o|--os-image     Virtual machine operation system(ubuntu16, centos7). -i|--interface    Network bind interface. -n|--network      Container Network plugin. -f|--force        Force deployment. --combine-master  Combine number of worker into masters. --combine-etcd    Combine number of worker into etcds.</code></pre><p>這邊執行以下指令來建立三台 Master 與三台 Node 的環境：</p><pre><code class="sh">$ ./tools/setup -m 2048 -n calico -i eth1Cluster Size: 1 master, 2 worker.     VM Size: 1 vCPU, 2048 MB     VM Info: ubuntu16, virtualbox         CNI: calico, Binding iface: eth1Start deploying?(y): y</code></pre><p>執行後需要等一點時間，當完成後就可以進入任何一台 Master 進行操作：</p><pre><code class="sh">$ kubectl -n kube-system get poNAME                                        READY     STATUS    RESTARTS   AGEcalico-node-657hv                           2/2       Running   0          57scalico-node-gmd8b                           2/2       Running   0          57scalico-node-w7nj8                           2/2       Running   0          57scalico-policy-controller-55dfcd9c69-t8s8z   1/1       Running   0          57shaproxy-master1                             1/1       Running   0          22shaproxy-node2                               1/1       Running   0          1mkeepalived-master1                          1/1       Running   0          30skeepalived-node2                            1/1       Running   0          1mkube-apiserver-master1                      1/1       Running   0          23skube-apiserver-node2                        1/1       Running   0          1mkube-controller-manager-master1             1/1       Running   0          17skube-controller-manager-node2               1/1       Running   0          1mkube-dns-6cb549f55f-8mgsd                   3/3       Running   0          46skube-proxy-l54d7                            1/1       Running   0          1mkube-proxy-rm4nn                            1/1       Running   0          1mkube-proxy-tvfs7                            1/1       Running   0          1mkube-scheduler-master1                      1/1       Running   0          39skube-scheduler-node2                        1/1       Running   0          1m</code></pre><p>這樣一個 HA 叢集就部署完成了，可以試著將一台 Master 關閉來驗證可靠性，若 Master 是三台的話，即表示可容忍最多一台故障。</p><h2 id="簡單部署-Nginx-服務"><a href="#簡單部署-Nginx-服務" class="headerlink" title="簡單部署 Nginx 服務"></a>簡單部署 Nginx 服務</h2><p>當完成部署後，可以透過簡單的應用程式部署來驗證系統是否正常運作：</p><pre><code class="sh">$ cat &lt;&lt;EOF &gt; deploy.ymlapiVersion: extensions/v1beta1kind: Deploymentmetadata:  name: nginxspec:  replicas: 1  template:    metadata:      labels:        app: nginx    spec:      containers:      - name: nginx        image: nginx:1.7.9        ports:        - containerPort: 80EOF$ kubectl create -f deploy.yml$ kubectl get poNAME                     READY     STATUS    RESTARTS   AGEnginx-4087004473-g6635   1/1       Running   0          15s</code></pre><p>然後透過建置 Service 來提供外部存取 Nginx HTTP 伺服器服務：</p><pre><code class="sh">$ cat &lt;&lt;EOF &gt; service.ymlapiVersion: v1kind: Servicemetadata:  name: nginx-servicespec:  type: NodePort  ports:  - port: 80    targetPort: 80    protocol: TCP    nodePort: 30000  selector:    app: nginxEOF$ kubectl create -f service.yml$ kubectl get svcNAME            CLUSTER-IP        EXTERNAL-IP   PORT(S)        AGEkubernetes      192.160.0.1       &lt;none&gt;        443/TCP        15mnginx-service   192.173.165.220   &lt;nodes&gt;       80:30000/TCP   11s</code></pre><p>由於範例使用 NodePort 的類型，所以任何一台節點都可以透過 TCP 30000 Port 來存取服務，包含 VIP 172.16.35.9 也可以存取。</p><p>最後，我們可以關閉 master1 來測試是否有 HA 效果。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://github.com/kairen/kube-ansible&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;kube-ansible&lt;/a&gt; 提供自動化部署 Kubernetes High Availability 叢集於虛擬機與實體機上，並且支援部署 Ceph 叢集於 Kubernetes 中提供共享式儲存系統給 Pod 應用程式使用。該專案最主要是想要快速建立測試環境來進行 Kubernetes 練習與驗證。&lt;/p&gt;
&lt;p&gt;kube-ansible 提供了以下幾項功能：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes 1.8.0+.&lt;/li&gt;
&lt;li&gt;Ceph on Kubernetes cluster.&lt;/li&gt;
&lt;li&gt;Common addons.&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://kairen.github.io/categories/Kubernetes/"/>
    
    
      <category term="Docker" scheme="https://kairen.github.io/tags/Docker/"/>
    
      <category term="Kubernetes" scheme="https://kairen.github.io/tags/Kubernetes/"/>
    
      <category term="Ansible" scheme="https://kairen.github.io/tags/Ansible/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes v1.6.x 全手動苦工安裝教學</title>
    <link href="https://kairen.github.io/2016/12/16/kubernetes/deploy/manual-v1.6/"/>
    <id>https://kairen.github.io/2016/12/16/kubernetes/deploy/manual-v1.6/</id>
    <published>2016-12-16T09:08:54.000Z</published>
    <updated>2017-10-31T06:41:11.000Z</updated>
    
    <content type="html"><![CDATA[<p>Kubernetes 提供了許多雲端平台與作業系統的安裝方式，本章將以<code>全手動安裝方式</code>來部署，主要是學習與了解 Kubernetes 建置流程。若想要瞭解更多平台的部署可以參考 <a href="https://kubernetes.io/docs/getting-started-guides/" target="_blank" rel="noopener">Picking the Right Solution</a>來選擇自己最喜歡的方式。</p><p>本次安裝版本為：</p><ul><li>Kubernetes v1.6.4</li><li>Etcd v3.1.6</li><li>Flannel v0.7.1</li><li>Docker v17.05.0-ce</li></ul><a id="more"></a><h2 id="預先準備資訊"><a href="#預先準備資訊" class="headerlink" title="預先準備資訊"></a>預先準備資訊</h2><p>本教學將以下列節點數與規格來進行部署 Kubernetes 叢集，作業系統可採用<code>Ubuntu 16.x</code>與<code>CentOS 7.x</code>：</p><table><thead><tr><th>IP Address</th><th>Role</th><th>CPU</th><th>Memory</th></tr></thead><tbody><tr><td>172.16.35.12</td><td>master</td><td>1</td><td>2G</td></tr><tr><td>172.16.35.10</td><td>node1</td><td>1</td><td>2G</td></tr><tr><td>172.16.35.11</td><td>node2</td><td>1</td><td>2G</td></tr></tbody></table><blockquote><p>這邊 master 為主要控制節點，node 為應用程式工作節點。</p></blockquote><p>首先安裝前要確認以下幾項都已將準備完成：</p><ul><li>所有節點彼此網路互通，並且不需要 SSH 密碼即可登入。</li><li>所有防火牆與 SELinux 已關閉。如 CentOS：</li></ul><pre><code class="sh">$ systemctl stop firewalld &amp;&amp; systemctl disable firewalld$ setenforce 0</code></pre><ul><li>所有節點需要設定<code>/etc/host</code>解析到所有主機。</li><li>所有節點需要安裝<code>Docker</code>或<code>rtk</code>引擎。這邊採用<code>Docker</code>來當作容器引擎，安裝方式如下：</li></ul><pre><code class="sh">$ curl -fsSL &quot;https://get.docker.com/&quot; | sh</code></pre><blockquote><p>不管是在 <code>Ubuntu</code> 或 <code>CentOS</code> 都只需要執行該指令就會自動安裝最新版 Docker。<br>CentOS 安裝完成後，需要再執行以下指令：</p><pre><code class="sh">$ systemctl enable docker &amp;&amp; systemctl start docker</code></pre></blockquote><h2 id="Etcd-安裝與設定"><a href="#Etcd-安裝與設定" class="headerlink" title="Etcd 安裝與設定"></a>Etcd 安裝與設定</h2><p>在開始安裝 Kubernetes 之前，需要先將一些必要系統建置完成，其中 Etcd 就是 Kubernetes 最為需要的一環，Kubernetes 會將部分資訊儲存於 Etcd 上，來提供給其他節點索取，以確保整個叢集的狀態。</p><p>首先在<code>master</code>節點下載 Etcd，並解壓縮放到 /opt 底下與安裝：</p><pre><code class="sh">$ cd /opt$ wget -qO- &quot;https://github.com/coreos/etcd/releases/download/v3.1.6/etcd-v3.1.6-linux-amd64.tar.gz&quot; | tar -zx$ mv etcd-v3.1.6-linux-amd64 etcd$ cd etcd/ &amp;&amp; ln etcd /usr/bin/ &amp;&amp; ln etcdctl /usr/bin/</code></pre><p>完成後新建 Etcd Group 與 User，並建立 Etcd 設定檔目錄：</p><pre><code class="sh">$ groupadd etcd$ useradd -c &quot;Etcd user&quot; -g etcd -s /sbin/nologin -r etcd$ mkdir /etc/etcd</code></pre><p>新增<code>/etc/etcd/etcd.conf</code>檔案，加入以下內容：</p><pre><code class="sh">$ cat &lt;&lt;EOF &gt; /etc/etcd/etcd.confETCD_NAME=masterETCD_DATA_DIR=/var/lib/etcdETCD_INITIAL_ADVERTISE_PEER_URLS=http://172.16.35.12:2380ETCD_INITIAL_CLUSTER=master=http://172.16.35.12:2380ETCD_INITIAL_CLUSTER_STATE=newETCD_INITIAL_CLUSTER_TOKEN=etcd-k8s-clusterETCD_LISTEN_PEER_URLS=http://0.0.0.0:2380ETCD_ADVERTISE_CLIENT_URLS=http://172.16.35.12:2379ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379ETCD_PROXY=offEOF</code></pre><blockquote><p>P.S. 若與該教學 IP 不同的話，請用自己 IP 取代<code>172.16.35.12</code>。</p></blockquote><p>新增<code>/lib/systemd/system/etcd.service</code>來管理 Etcd，並加入以下內容：</p><pre><code class="sh">$ cat &lt;&lt;EOF &gt; /lib/systemd/system/etcd.service[Unit]Description=Etcd ServiceAfter=network.target[Service]Environment=ETCD_DATA_DIR=/var/lib/etcd/defaultEnvironmentFile=-/etc/etcd/etcd.confType=notifyUser=etcdPermissionsStartOnly=trueExecStart=/usr/bin/etcdRestart=alwaysRestartSec=10LimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF</code></pre><p>建立 var 存放資訊，然後啟動 Etcd 服務:</p><pre><code class="sh">$ mkdir -p /var/lib/etcd &amp;&amp; chown etcd:etcd -R /var/lib/etcd$ systemctl enable etcd.service &amp;&amp; systemctl start etcd.service</code></pre><p>透過簡單指令驗證：</p><pre><code class="sh">$ etcdctl cluster-healthmember 95b428c288413b46 is healthy: got healthy result from http://172.16.35.12:2379cluster is healthy</code></pre><p>接著回到<code>master</code>節點，新增一個<code>/tmp/flannel-config.json</code>檔，並加入以下內容：</p><pre><code class="sh">$ cat &lt;&lt;EOF &gt; /tmp/flannel-config.json{ &quot;Network&quot;: &quot;10.244.0.0/16&quot;, &quot;SubnetLen&quot;: 24, &quot;Backend&quot;: { &quot;Type&quot;: &quot;vxlan&quot; } }EOF</code></pre><p>然後將 Flannel 網路設定儲存到 etcd 中：</p><pre><code class="sh">$ etcdctl --no-sync set /atomic.io/network/config &lt; /tmp/flannel-config.json$ etcdctl ls /atomic.io/network//atomic.io/network/config</code></pre><h2 id="Flannel-安裝與設定"><a href="#Flannel-安裝與設定" class="headerlink" title="Flannel 安裝與設定"></a>Flannel 安裝與設定</h2><p>Flannel 是 CoreOS 團隊針對 Kubernetes 設計的一個<code>覆蓋網絡(Overlay Network)</code>工具，其目的在於幫助每一個使用 Kuberentes 的主機擁有一個完整的子網路。</p><p>首先在<code>所有</code>節點下載 Flannel，並執行以下步驟。首先解壓縮放到 /opt 底下與安裝：</p><pre><code class="sh">$ cd /opt &amp;&amp; mkdir flannel$ wget -qO- &quot;https://github.com/coreos/flannel/releases/download/v0.7.1/flannel-v0.7.1-linux-amd64.tar.gz&quot; | tar -zxC flannel/$ cd flannel/ &amp;&amp; ln flanneld /usr/bin/ &amp;&amp; ln mk-docker-opts.sh /usr/bin/</code></pre><p>建立 Docker Drop-in 目錄，並新增<code>flannel.conf</code>檔案：</p><pre><code class="sh">$ mkdir -p /etc/systemd/system/docker.service.d$ cat &lt;&lt;EOF &gt; /etc/systemd/system/docker.service.d/flannel.conf[Service]EnvironmentFile=-/run/flannel/dockerEOF</code></pre><p>新增<code>/etc/default/flanneld</code>檔案，加入以下內容：</p><pre><code class="sh">$ cat &lt;&lt;EOF &gt; /etc/default/flanneldFLANNEL_ETCD_ENDPOINTS=&quot;http://172.16.35.12:2379&quot;FLANNEL_ETCD_PREFIX=&quot;/atomic.io/network&quot;FLANNEL_OPTIONS=&quot;--iface=enp0s8&quot;EOF</code></pre><blockquote><p><code>FLANNEL_ETCD_ENDPOINTS</code> 請修改成自己的 <code>master</code> IP。<br><code>FLANNEL_OPTIONS</code>可以依據需求加入，這邊主要指定 flannel 使用的網卡。</p></blockquote><p>新增<code>/lib/systemd/system/flanneld.service</code>來管理 Flannel：</p><pre><code class="sh">$ cat &lt;&lt;EOF &gt; /lib/systemd/system/flanneld.service[Unit]Description=Flanneld ServiceAfter=network-online.targetWants=network-online.targetAfter=etcd.serviceBefore=docker.service[Service]Type=notifyEnvironmentFile=/etc/default/flanneldExecStart=/usr/bin/flanneld -etcd-endpoints=\${FLANNEL_ETCD_ENDPOINTS} -etcd-prefix=\${FLANNEL_ETCD_PREFIX} \${FLANNEL_OPTIONS}ExecStartPost=/usr/bin/mk-docker-opts.sh -d /run/flannel/dockerRestart=always[Install]WantedBy=multi-user.targetRequiredBy=docker.serviceEOF</code></pre><p>之後到<code>每台</code>節點啟動 Flannel:</p><pre><code class="sh">$ systemctl enable flanneld.service &amp;&amp; systemctl start flanneld.service</code></pre><p>完成後透過以下指令簡單驗證：</p><pre><code class="sh">$ ip -4 addr show flannel.15: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default    inet 10.244.11.0/32 scope global flannel.1       valid_lft forever preferred_lft forever</code></pre><p>確認有網路後，修改<code>/lib/systemd/system/docker.service</code>檔案以下內容：</p><pre><code>ExecStartPost=/sbin/iptables -I FORWARD -s 0.0.0.0/0 -j ACCEPTExecStart=/usr/bin/dockerd -H fd:// $DOCKER_OPTS</code></pre><blockquote><p>若是 CentOS 7 則不需要加入 <code>-H fd://</code>。</p></blockquote><p>重新啟動 Docker 來使用 Flannel：</p><pre><code class="sh">$ systemctl daemon-reload &amp;&amp; systemctl restart docker$ ip -4 a show docker04: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default    inet 10.244.11.1/24 scope global docker0       valid_lft forever preferred_lft forever</code></pre><p>最後在任一台節點去 Ping 其他節點的 docker0 網路，若 Ping 的到表示部署沒問題。</p><h2 id="Kubernetes-Master-安裝與設定"><a href="#Kubernetes-Master-安裝與設定" class="headerlink" title="Kubernetes Master 安裝與設定"></a>Kubernetes Master 安裝與設定</h2><p>Master 是 Kubernetes 的大總管，主要建置<code>API Server</code>、<code>Controller Manager Server</code>與<code>Scheduler</code>來元件管理所有 Node。首先加入取得 Packages 來源並安裝：</p><pre><code class="sh">$ curl -s &quot;https://packages.cloud.google.com/apt/doc/apt-key.gpg&quot; | apt-key add -$ echo &quot;deb http://apt.kubernetes.io/ kubernetes-xenial main&quot; &gt; /etc/apt/sources.list.d/kubernetes.list$ apt-get update &amp;&amp; apt-get install -y kubectl kubelet kubernetes-cni</code></pre><blockquote><p>CentOS 7 則使用以下指令安裝：</p><pre><code class="sh">$ cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://yum.kubernetes.io/repos/kubernetes-el7-x86_64enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg       https://packages.cloud.google.com/yum/doc/rpm-package-key.gpgEOF$ yum install -y kubelet kubectl kubernetes-cni</code></pre></blockquote><p>然後準備 OpenSSL 的設定檔資訊：</p><pre><code class="sh">$ mkdir -p /etc/kubernetes/pki$ DIR=/etc/kubernetes/pki$ cat &lt;&lt;EOF &gt; ${DIR}/openssl.conf[req]req_extensions = v3_reqdistinguished_name = req_distinguished_name[req_distinguished_name][ v3_req ]basicConstraints = CA:FALSEkeyUsage = nonRepudiation, digitalSignature, keyEnciphermentsubjectAltName = @alt_names[alt_names]DNS.1 = kubernetesDNS.2 = kubernetes.defaultDNS.3 = kubernetes.default.svcDNS.4 = kubernetes.default.svc.cluster.localIP.1 = 192.160.0.1IP.2 = 172.16.35.12EOF</code></pre><blockquote><p><code>IP.2</code> 請修改成自己的<code>master</code> IP。<br>細節請參考<a href="https://coreos.com/kubernetes/docs/latest/openssl.html" target="_blank" rel="noopener">Cluster TLS using OpenSSL</a>。</p></blockquote><p>建立 OpenSSL Keypairs 與 Certificate：</p><pre><code class="sh">DIR=/etc/kubernetes/pkiopenssl genrsa -out ${DIR}/ca-key.pem 2048openssl req -x509 -new -nodes -key ${DIR}/ca-key.pem -days 1000 -out ${DIR}/ca.pem -subj &#39;/CN=kube-ca&#39;openssl genrsa -out ${DIR}/admin-key.pem 2048openssl req -new -key ${DIR}/admin-key.pem -out ${DIR}/admin.csr -subj &#39;/CN=kube-admin&#39;openssl x509 -req -in ${DIR}/admin.csr -CA ${DIR}/ca.pem -CAkey ${DIR}/ca-key.pem -CAcreateserial -out ${DIR}/admin.pem -days 1000openssl genrsa -out ${DIR}/apiserver-key.pem 2048openssl req -new -key ${DIR}/apiserver-key.pem -out ${DIR}/apiserver.csr -subj &#39;/CN=kube-apiserver&#39; -config ${DIR}/openssl.confopenssl x509 -req -in ${DIR}/apiserver.csr -CA ${DIR}/ca.pem -CAkey ${DIR}/ca-key.pem -CAcreateserial -out ${DIR}/apiserver.pem -days 1000 -extensions v3_req -extfile ${DIR}/openssl.conf</code></pre><blockquote><p>細節請參考 <a href="https://coreos.com/kubernetes/docs/latest/openssl.html" target="_blank" rel="noopener">Cluster TLS using OpenSSL</a>。</p></blockquote><p>接著下載 Kubernetes 相關檔案至<code>/etc/kubernetes</code>：</p><pre><code class="sh">cd /etc/kubernetes/URL=&quot;https://kairen.github.io/files/manual/master&quot;wget ${URL}/kube-apiserver.conf -O manifests/kube-apiserver.ymlwget ${URL}/kube-controller-manager.conf -O manifests/kube-controller-manager.ymlwget ${URL}/kube-scheduler.conf -O manifests/kube-scheduler.ymlwget ${URL}/admin.conf -O admin.confwget ${URL}/kubelet.conf -O kubeletcat &lt;&lt;EOF &gt; /etc/kubernetes/user.csvp@ssw0rd,admin,adminEOF</code></pre><blockquote><p>若<code>IP</code>與教學設定不同的話，請記得修改<code>kube-apiserver.yml</code>、<code>kube-controller-manager.yml</code>、<code>kube-scheduler.yml</code>與<code>admin.yml</code>。</p></blockquote><p>新增<code>/lib/systemd/system/kubelet.service</code>來管理 kubelet：</p><pre><code class="sh">$ cat &lt;&lt;EOF &gt; /lib/systemd/system/kubelet.service[Unit]Description=Kubernetes Kubelet ServerAfter=docker.serviceRequires=docker.service[Service]WorkingDirectory=/var/lib/kubeletEnvironmentFile=-/etc/kubernetes/kubeletExecStart=/usr/bin/kubelet \$KUBELET_ADDRESS \$KUBELET_POD_INFRA_CONTAINER \\$KUBELET_ARGS \$KUBE_NODE_LABEL \$KUBE_LOGTOSTDERR \\$KUBE_ALLOW_PRIV \$KUBELET_NETWORK_ARGS \\$KUBELET_DNS_ARGSRestart=always[Install]WantedBy=multi-user.targetEOF</code></pre><blockquote><p><code>/etc/systemd/system/kubelet.service</code>為<code>CentOS 7</code>使用的路徑。</p></blockquote><p>最後建立 var 存放資訊，然後啟動 kubelet 服務:</p><pre><code class="sh">$ mkdir -p /var/lib/kubelet$ systemctl daemon-reload &amp;&amp; systemctl restart kubelet.service</code></pre><p>完成後會需要一段時間來下載與啟動元件，可以利用該指令來監看：</p><pre><code class="sh">$ watch -n 1 netstat -ntlptcp   0  0 127.0.0.1:10248  0.0.0.0:*  LISTEN  20613/kubelettcp   0  0 127.0.0.1:10251  0.0.0.0:*  LISTEN  19968/kube-scheduletcp   0  0 127.0.0.1:10252  0.0.0.0:*  LISTEN  20815/kube-controlltcp6  0  0 :::8080          :::*       LISTEN  20333/kube-apiserve</code></pre><blockquote><p>若看到以上已經被 binding 後，就可以透過瀏覽器存取 <a href="https://172.16.35.12:6443/" target="_blank" rel="noopener">API Service</a>，並輸入帳號<code>admin</code>與密碼<code>p@ssw0rd</code>。</p></blockquote><p>透過簡單指令驗證：</p><pre><code class="sh">$ kubectl get nodeNAME      STATUS         AGEmaster   Ready,master   1m$ kubectl get po --all-namespacesNAMESPACE     NAME                              READY     STATUS    RESTARTS   AGEkube-system   kube-apiserver-master1            1/1       Running   0          3mkube-system   kube-controller-manager-master1   1/1       Running   0          2mkube-system   kube-scheduler-master1            1/1       Running   0          2m</code></pre><h2 id="Kubernetes-Node-安裝與設定"><a href="#Kubernetes-Node-安裝與設定" class="headerlink" title="Kubernetes Node 安裝與設定"></a>Kubernetes Node 安裝與設定</h2><p>Node 是主要的工作節點，上面將運行許多容器應用。到所有<code>node</code>節點加入取得 Packages 來源，並安裝：</p><pre><code class="sh">$ curl -s &quot;https://packages.cloud.google.com/apt/doc/apt-key.gpg&quot; | apt-key add -$ echo &quot;deb http://apt.kubernetes.io/ kubernetes-xenial main&quot; &gt; /etc/apt/sources.list.d/kubernetes.list$ apt-get update &amp;&amp; apt-get install -y kubelet kubernetes-cni</code></pre><blockquote><p>CentOS 7 則使用以下指令安裝：</p><pre><code class="sh">$ cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://yum.kubernetes.io/repos/kubernetes-el7-x86_64enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg       https://packages.cloud.google.com/yum/doc/rpm-package-key.gpgEOF$ yum install -y kubelet kubernetes-cni</code></pre></blockquote><p>然後準備 OpenSSL 的設定檔資訊：</p><pre><code class="sh">$ mkdir -p /etc/kubernetes/pki$ DIR=/etc/kubernetes/pki$ cat &lt;&lt;EOF &gt; ${DIR}/openssl.conf[req]req_extensions = v3_reqdistinguished_name = req_distinguished_name[req_distinguished_name][ v3_req ]basicConstraints = CA:FALSEkeyUsage = nonRepudiation, digitalSignature, keyEnciphermentsubjectAltName = @alt_names[alt_names]IP.1=172.16.35.10DNS.1=node1EOF</code></pre><blockquote><p>P.S. 這邊<code>IP.1</code>與<code>DNS.1</code>需要隨機器不同設定。細節請參考 <a href="https://coreos.com/kubernetes/docs/latest/openssl.html" target="_blank" rel="noopener">Cluster TLS using OpenSSL</a>。</p></blockquote><p>將<code>master1</code>上的 OpenSSL key 複製到<code>/etc/kubernetes/pki</code>：</p><pre><code class="sh">for file in ca-key.pem ca.pem admin.pem admin-key.pem; do  scp /etc/kubernetes/pki/${file} &lt;NODE&gt;:/etc/kubernetes/pki/done</code></pre><blockquote><p>P.S. 該操作在<code>master1</code>執行。並記得修改<code>&lt;NODE&gt;</code>為所有工作節點。</p></blockquote><p>建立 OpenSSL Keypairs 與 Certificate：</p><pre><code class="sh">DIR=/etc/kubernetes/pkiopenssl genrsa -out ${DIR}/node-key.pem 2048openssl req -new -key ${DIR}/node-key.pem -out ${DIR}/node.csr -subj &#39;/CN=kube-node&#39; -config ${DIR}/openssl.confopenssl x509 -req -in ${DIR}/node.csr -CA ${DIR}/ca.pem -CAkey ${DIR}/ca-key.pem -CAcreateserial -out ${DIR}/node.pem -days 1000 -extensions v3_req -extfile ${DIR}/openssl.conf</code></pre><blockquote><p>細節請參考 <a href="https://coreos.com/kubernetes/docs/latest/openssl.html" target="_blank" rel="noopener">Cluster TLS using OpenSSL</a>。</p></blockquote><p>接著下載 Kubernetes 相關檔案至<code>/etc/kubernetes/</code>：</p><pre><code class="sh">cd /etc/kubernetes/URL=&quot;https://kairen.github.io/files/manual/node&quot;wget ${URL}/kubelet-user.conf -O kubelet-user.confwget ${URL}/admin.conf -O admin.confwget ${URL}/kubelet.conf -O kubelet</code></pre><blockquote><p>若<code>IP</code>與教學設定不同的話，請記得修改<code>kubelet-user.conf</code>與<code>admin.conf</code>。</p></blockquote><p>新增<code>/lib/systemd/system/kubelet.service</code>來管理 kubelet：</p><pre><code class="sh">$ cat &lt;&lt;EOF &gt; /lib/systemd/system/kubelet.service[Unit]Description=Kubernetes Kubelet ServerAfter=docker.serviceRequires=docker.service[Service]WorkingDirectory=/var/lib/kubeletEnvironmentFile=-/etc/kubernetes/kubeletExecStart=/usr/bin/kubelet \$KUBELET_ADDRESS \$KUBELET_POD_INFRA_CONTAINER \\$KUBELET_ARGS \$KUBE_LOGTOSTDERR \\$KUBE_ALLOW_PRIV \$KUBELET_NETWORK_ARGS \\$KUBELET_DNS_ARGSRestart=always[Install]WantedBy=multi-user.targetEOF</code></pre><blockquote><p><code>/etc/systemd/system/kubelet.service</code>為<code>CentOS 7</code>使用的路徑。</p></blockquote><p>最後建立 var 存放資訊，然後啟動 kubelet 服務:</p><pre><code class="sh">$ mkdir -p /var/lib/kubelet$ systemctl daemon-reload &amp;&amp; systemctl restart kubelet.service</code></pre><p>當所有節點都完成後，回到<code>master</code>透過簡單指令驗證：</p><pre><code class="sh">$ kubectl get nodeNAME      STATUS         AGE       VERSIONmaster1   Ready,master   17m       v1.6.4node1     Ready          20s       v1.6.4node2     Ready          18s       v1.6.4</code></pre><h2 id="Kubernetes-Addons-部署"><a href="#Kubernetes-Addons-部署" class="headerlink" title="Kubernetes Addons 部署"></a>Kubernetes Addons 部署</h2><p>當環境都建置完成後，就可以進行部署附加元件，首先到<code>master1</code>，並進入<code>/etc/kubernetes/</code>目錄下載 Addon 檔案：</p><pre><code class="sh">cd /etc/kubernetes/ &amp;&amp; mkdir addonURL=&quot;https://kairen.github.io/files/manual/addon&quot;wget ${URL}/kube-proxy.conf -O addon/kube-proxy.ymlwget ${URL}/kube-dns.conf -O addon/kube-dns.ymlwget ${URL}/kube-dash.conf -O addon/kube-dash.ymlwget ${URL}/kube-monitor.conf -O addon/kube-monitor.yml</code></pre><blockquote><p>若<code>IP</code>與教學設定不同的話，請記得修改<code>&lt;YOUR_MASTER_IP&gt;</code>。</p><pre><code class="sh">$ sed -i &#39;s/172.16.35.12/&lt;YOUR_MASTER_IP&gt;/g&#39; addon/kube-monitor.yml$ sed -i &#39;s/172.16.35.12/&lt;YOUR_MASTER_IP&gt;/g&#39; addon/kube-proxy.yml</code></pre></blockquote><p>接著透過 kubectl 來指定檔案建立附加元件：</p><pre><code class="sh">$ kubectl apply -f addon/</code></pre><blockquote><p>若想要刪除則將<code>apply</code>改成<code>delete</code>即可。</p></blockquote><p>透過以下指令來驗證部署是否有效：</p><pre><code class="sh">$ kubectl get po -n kube-systemNAME                                   READY     STATUS    RESTARTS   AGEheapster-v1.2.0-1753406648-wsb3z       1/1       Running   0          2minfluxdb-grafana-42195489-vtmnl        2/2       Running   0          2mkube-apiserver-master1                 1/1       Running   0          33mkube-controller-manager-master1        1/1       Running   0          33mkube-dns-3701766129-0p28b              3/3       Running   0          2mkube-proxy-amd64-44rft                 1/1       Running   0          2mkube-proxy-amd64-fz77b                 1/1       Running   0          2mkube-proxy-amd64-gqq2p                 1/1       Running   0          2mkube-scheduler-master1                 1/1       Running   0          33mkubernetes-dashboard-210558060-zw814   1/1       Running   2          2m</code></pre><p>確定都啟動後，可以開啟 <a href="https://172.16.35.12:6443/ui" target="_blank" rel="noopener">https://172.16.35.12:6443/ui</a> 來查看。<br><img src="/images/kube/dash-preview.png" alt=""></p><h2 id="簡單部署-Nginx-服務"><a href="#簡單部署-Nginx-服務" class="headerlink" title="簡單部署 Nginx 服務"></a>簡單部署 Nginx 服務</h2><p>Kubernetes 可以選擇使用指令直接建立應用程式與服務，或者撰寫 YAML 與 JSON 檔案來描述部署應用程式的配置，以下將建立一個簡單的 Nginx 服務：</p><pre><code class="sh">$ kubectl run nginx --image=nginx --replicas=1 --port=80$ kubectl get pods -o wideNAME                    READY     STATUS    RESTARTS   AGE       IP            NODEnginx-158599303-k7cbt   1/1       Running   0          14s       10.244.24.3   node1</code></pre><p>完成後要接著建立 svc(Service)，來提供外部網路存取應用程式，使用以下指令建立：</p><pre><code class="sh">$ kubectl expose deploy nginx --port=80 --type=LoadBalancer --external-ip=172.16.35.12$ kubectl get svcNAME             CLUSTER-IP       EXTERNAL-IP     PORT(S)        AGEsvc/kubernetes   192.160.0.1      &lt;none&gt;          443/TCP        2hsvc/nginx        192.160.57.181   ,172.16.35.12   80:32054/TCP   21s</code></pre><blockquote><p>這邊<code>type</code>可以選擇 NodePort 與 LoadBalancer。另外需隨機器 IP 不同而修改 <code>external-ip</code>。</p></blockquote><p>確認沒問題後即可在瀏覽器存取 <a href="http://172.16.35.12/。" target="_blank" rel="noopener">http://172.16.35.12/。</a></p><h3 id="擴展服務數量"><a href="#擴展服務數量" class="headerlink" title="擴展服務數量"></a>擴展服務數量</h3><p>若叢集<code>node</code>節點增加了，而想讓 Nginx 服務提供可靠性的話，可以透過以下方式來擴展服務的副本：</p><pre><code class="sh">$ kubectl scale deploy nginx --replicas=2$ kubectl get pods -o wideNAME                    READY     STATUS    RESTARTS   AGE       IP             NODEnginx-158599303-0h9lr   1/1       Running   0          25s       10.244.100.5   node2nginx-158599303-k7cbt   1/1       Running   0          1m        10.244.24.3    node1</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Kubernetes 提供了許多雲端平台與作業系統的安裝方式，本章將以&lt;code&gt;全手動安裝方式&lt;/code&gt;來部署，主要是學習與了解 Kubernetes 建置流程。若想要瞭解更多平台的部署可以參考 &lt;a href=&quot;https://kubernetes.io/docs/getting-started-guides/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Picking the Right Solution&lt;/a&gt;來選擇自己最喜歡的方式。&lt;/p&gt;
&lt;p&gt;本次安裝版本為：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes v1.6.4&lt;/li&gt;
&lt;li&gt;Etcd v3.1.6&lt;/li&gt;
&lt;li&gt;Flannel v0.7.1&lt;/li&gt;
&lt;li&gt;Docker v17.05.0-ce&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://kairen.github.io/categories/Kubernetes/"/>
    
    
      <category term="Docker" scheme="https://kairen.github.io/tags/Docker/"/>
    
      <category term="Kubernetes" scheme="https://kairen.github.io/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Ceph 使用 SPDK 加速 NVMe SSD</title>
    <link href="https://kairen.github.io/2016/12/03/ceph/ceph-spdk/"/>
    <id>https://kairen.github.io/2016/12/03/ceph/ceph-spdk/</id>
    <published>2016-12-03T09:08:54.000Z</published>
    <updated>2017-08-01T14:23:59.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/spdk/spdk" target="_blank" rel="noopener">SPDK(Storage Performance Development Kit)</a> 是 Intel 釋出的儲存效能開發工具，主要提供一套撰寫高效能、可擴展與 User-mode 的儲存應用程式工具與函式庫，而中國公司 XSKY 藉由該開發套件來加速 Ceph 在 NVMe SSD 的效能。</p><a id="more"></a><p>首先進入 root，並 clone 專案到 local：</p><pre><code class="shell=">$ sudo su -$ git clone http://github.com/ceph/ceph$ cd ceph</code></pre><p>編輯<code>CMakeLists.txt</code>檔案，修改以下內容：</p><pre><code class="shell=">option(WITH_SPDK &quot;Enable SPDK&quot; ON)</code></pre><p>接著安裝一些相依套件與函式庫：</p><pre><code class="shell=">$ ./install-deps.sh$ sudo apt-get install -y libpciaccess-dev</code></pre><p>接著需要在環境安裝 DPDK 開發套件，首先進入 src 底下的 dpdk 目錄，編輯<code>config/common_linuxapp</code>檔案修改以下內容：</p><pre><code class="shell=">CONFIG_RTE_BUILD_SHARED_LIB=</code></pre><p>完成後建置與安裝 DPDK：</p><pre><code class="shell=">$ make config T=x86_64-native-linuxapp-gcc$ make &amp;&amp; make install</code></pre><p>接著回到 ceph root 目錄進行建構 Ceph 準備，透過以下指令進行：</p><pre><code class="shell=">$ ./do_cmake.sh....-- Configuring done-- Generating done-- Build files have been written to: /root/ceph/build+ cat+ echo 40000+ echo done.done.</code></pre><p>確認上面無誤後就可以進行 compile 包含 SPDK 的 Ceph：</p><pre><code class="shell=">$ cd build$ make -j2</code></pre><p>完成後就可以執行 test cluster，首先建構 vstart 程式：</p><pre><code class="shell=">$ make vstart$ ../src/vstart.sh -d -n -x -l$ ./bin/ceph -s</code></pre><blockquote><p>若要關閉則使用以下方式：</p><pre><code class="shell=">$ ../src/stop.sh</code></pre></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://github.com/spdk/spdk&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;SPDK(Storage Performance Development Kit)&lt;/a&gt; 是 Intel 釋出的儲存效能開發工具，主要提供一套撰寫高效能、可擴展與 User-mode 的儲存應用程式工具與函式庫，而中國公司 XSKY 藉由該開發套件來加速 Ceph 在 NVMe SSD 的效能。&lt;/p&gt;
    
    </summary>
    
      <category term="Ceph" scheme="https://kairen.github.io/categories/Ceph/"/>
    
    
      <category term="Ceph" scheme="https://kairen.github.io/tags/Ceph/"/>
    
      <category term="Storage" scheme="https://kairen.github.io/tags/Storage/"/>
    
      <category term="Distribution System" scheme="https://kairen.github.io/tags/Distribution-System/"/>
    
      <category term="SPDK" scheme="https://kairen.github.io/tags/SPDK/"/>
    
  </entry>
  
  <entry>
    <title>Using bluestore in Kraken</title>
    <link href="https://kairen.github.io/2016/11/28/ceph/deploy/ceph-deploy-bluestore/"/>
    <id>https://kairen.github.io/2016/11/28/ceph/deploy/ceph-deploy-bluestore/</id>
    <published>2016-11-28T09:08:54.000Z</published>
    <updated>2017-08-01T14:23:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>本篇說明如何安裝 Kraken 版本的 Ceph，並將 objectstore backend 修改成 Bluestore，過程包含建立 RBD 等操作。</p><a id="more"></a><h2 id="硬體規格說明"><a href="#硬體規格說明" class="headerlink" title="硬體規格說明"></a>硬體規格說明</h2><p>本安裝由於實體機器數量受到限制，故只進行一台 MON 與兩台 OSD，而 OSD 數量則總共兩顆，硬體規格如下所示：</p><table><thead><tr><th>Role</th><th>RAM</th><th>CPUs</th><th>Disk</th><th>IP Address</th></tr></thead><tbody><tr><td>mon1(deploy)</td><td>4 GB</td><td>4 core</td><td>500 GB</td><td>172.16.1.200</td></tr><tr><td>osd1</td><td>16 GB</td><td>8 core</td><td>2 TB</td><td>172.16.1.201</td></tr><tr><td>osd2</td><td>16 GB</td><td>8 core</td><td>2 TB</td><td>172.16.1.202</td></tr><tr><td>osd3</td><td>16 GB</td><td>8 core</td><td>2 TB</td><td>172.16.1.203</td></tr></tbody></table><p>作業系統採用<code>Ubuntu 16.04 LTS Server</code>，Kernel 版本為<code>Linux 4.4.0-31-generic</code>。</p><h2 id="事前準備"><a href="#事前準備" class="headerlink" title="事前準備"></a>事前準備</h2><p>在開始部署 Ceph 叢集之前，我們需要在每個節點做一些基本的準備，來確保叢集安裝的過程是流暢的，本次安裝會擁有四台節點。</p><p>首先在每一台節點新增以下內容到<code>/etc/hosts</code>：</p><pre><code>127.0.0.1    localhost172.16.1.200 mon1172.16.1.201 osd1172.16.1.202 osd2172.16.1.203 osd3</code></pre><p>然後設定各節點 sudo 指令的權限，使之不用輸入密碼(若使用 root 則忽略)：</p><pre><code class="sh">$ echo &quot;ubuntu ALL = (root) NOPASSWD:ALL&quot; | \sudo tee /etc/sudoers.d/ubuntu &amp;&amp; sudo chmod 440 /etc/sudoers.d/ubuntu</code></pre><p>接著在設定<code>deploy</code>節點能夠以無密碼方式進行 SSH 登入其他節點，請依照以下執行：</p><pre><code class="sh">$ ssh-keygen -t rsa$ ssh-copy-id mon1$ ssh-copy-id osd1...</code></pre><blockquote><p>若不同節點之間使用不同 User 進行 SSH 部署的話，可以設定 ~/.ssh/config</p></blockquote><p>之後在<code>deploy</code>節點安裝部署工具，首先使用 apt-get 來進行安裝基本相依套件，再透過 pypi 進行安裝 ceph-deploy 工具：</p><pre><code class="sh">$ sudo apt-get install -y python-pip$ sudo pip install -U ceph-deploy</code></pre><h2 id="節點部署"><a href="#節點部署" class="headerlink" title="節點部署"></a>節點部署</h2><p>首先建立一個名稱為 local 的目錄，並進到目錄底下：</p><pre><code class="sh">$ sudo mkdir local &amp;&amp; cd local</code></pre><p>接著透過 ceph-deploy 在各節點安裝 ceph：</p><pre><code class="sh">$ ceph-deploy install --release kraken mon1 osd1 osd2 osd3</code></pre><p>完成後建立 Monitor 節點資訊到 ceph.conf 中：</p><pre><code class="sh">$ ceph-deploy new mon1 &lt;other_mons&gt;</code></pre><p>接著編輯目錄底下的 ceph.conf，並加入以下內容：</p><pre><code class="sh">[global]...rbd_default_features = 3osd pool default size = 3osd pool default min size = 1public network = 172.16.1.0/24cluster network = 172.16.1.0/24filestore_xattr_use_omap = trueenable experimental unrecoverable data corrupting features = bluestore rocksdbbluestore fsck on mount = truebluestore block db size = 134217728bluestore block wal size = 268435456bluestore block size = 322122547200osd objectstore = bluestore[osd]bluestore = true</code></pre><p>若確認沒問題，即可透過以下指令初始化 mon：</p><pre><code class="sh">$ ceph-deploy mon create-initial</code></pre><p>上述沒有問題後，就可以開始部署實際作為儲存的 OSD 節點，我們可以透過以下指令進行：</p><pre><code class="sh">$ ceph-deploy osd prepare --bluestore osd1:&lt;device&gt;</code></pre><h2 id="系統驗證"><a href="#系統驗證" class="headerlink" title="系統驗證"></a>系統驗證</h2><h3 id="叢集檢查"><a href="#叢集檢查" class="headerlink" title="叢集檢查"></a>叢集檢查</h3><p>首先要驗證環境是否有部署成功，可以透過 ceph 提供的基本指令做檢查：</p><pre><code class="sh">$ ceph -vceph version v11.0.2 (697fe64f9f106252c49a2c4fe4d79aea29363be7)$ ceph -s    cluster 6da24ae5-755f-4077-bfa0-78681dfc6bde     health HEALTH_OK     monmap e1: 1 mons at {r-mon00=172.16.1.200:6789/0}            election epoch 7, quorum 0 mon1        mgr no daemons active     osdmap e256: 3 osds: 3 up, 3 in            flags sortbitwise,require_jewel_osds      pgmap v920162: 128 pgs, 1 pools, 6091 MB data, 1580 objects            12194 MB used, 588 GB / 600 GB avail                 128 active+clean</code></pre><p>另外也可以用 osd 指令來查看部屬的 osd 資訊：</p><pre><code class="sh">$ ceph osd treeID WEIGHT  TYPE NAME        UP/DOWN REWEIGHT PRIMARY-AFFINITY-1 0.58618 root default-2 0.29309     host osd1 0 0.29309         osd.0         up  1.00000          1.00000-3 0.29309     host osd2 1 0.29309         osd.1         up  1.00000          1.00000-4 0.29309     host osd3 1 0.29309         osd.2         up  1.00000          1.00000</code></pre><h3 id="RBD-建立"><a href="#RBD-建立" class="headerlink" title="RBD 建立"></a>RBD 建立</h3><p>本節說明在 Kraken 版本建立 RBD 來進行使用，在預設部署起來的叢集下會存在一個儲存池 rbd，因此可以省略建立新的儲存池。</p><p>首先透過以下指令建立一個區塊裝置映像檔：</p><pre><code class="sh">$ rbd create rbd/bd -s 50G</code></pre><p>接著透過 info 指令查看區塊裝置映像檔資訊：</p><pre><code class="sh">$ rbd info rbd/bdrbd image &#39;bd&#39;:    size 51200 MB in 12800 objects    order 22 (4096 kB objects)    block_name_prefix: rbd_data.102d474b0dc51    format: 2    features: layering, striping    flags:    stripe unit: 4096 kB    stripe count: 1</code></pre><blockquote><p>P.S. 這邊由於 Kernel 版本問題有些特性無法支援，因此在 conf 檔只設定使用 layering, striping。</p><p>P.S. 若預設未修改 feature 設定的話，可以透過以下指令修改:</p><pre><code class="sh">$ rbd feature disable rbd/bd &lt;feature_name&gt;</code></pre><p>以下為目前支援的特性：</p><table><thead><tr><th>屬性名稱</th><th>說明</th><th>Bit Code</th></tr></thead><tbody><tr><td>layering</td><td>支援分層</td><td>1</td></tr><tr><td>striping</td><td>支援串連(v2)</td><td>2</td></tr><tr><td>exclusive-lock</td><td>支援互斥鎖定</td><td>4</td></tr><tr><td>object-map</td><td>支援物件映射(相依於 exclusive-lock )</td><td>8</td></tr><tr><td>fast-diff</td><td>支援快速計算差異(相依於 object-map )</td><td>16</td></tr><tr><td>deep-flatten</td><td>支援快照扁平化操作</td><td>32</td></tr><tr><td>journaling</td><td>支援紀錄 I/O 操作(相依於 exclusive-lock )</td><td>64</td></tr></tbody></table></blockquote><p>接著就可以透過 Linux mkfs 指令來格式化 rbd：</p><pre><code class="sh">$ sudo mkfs.ext4 /dev/rbd0$ sudo mount /dev/rbd0 /mnt</code></pre><p>最後透過 dd 指令測試 rbd 寫入效能：</p><pre><code class="sh">$ dd if=/dev/zero of=/mnt/test bs=4096 count=40000004000000+0 records in4000000+0 records out16384000000 bytes (16 GB) copied, 119.947 s, 137 MB/s</code></pre><p>另外有些需求為了測試 feature，卻又礙於 Kernel 不支援等問題，而造成無法 Map 時，可以透過 rbd-nbd 來進行 Map，安裝跟使用方式如下：</p><pre><code class="sh">$ sudo apt-get install -y rbd-nbd$ sudo rbd-nbd map rbd/bd/dev/nbd0</code></pre><blockquote><p>P.S. 在新版的 ceph 已經有內建 rbd nbd，參考 <a href="http://docs.ceph.com/docs/jewel/man/8/rbd/#commands" target="_blank" rel="noopener">rbd - manage command</a>。</p></blockquote><p>最後透過 dd 指令測試 nbd 寫入效能：</p><pre><code class="sh">$ dd if=/dev/zero of=./mnt-nbd/test bs=4096 count=40000004000000+0 records in4000000+0 records out16384000000 bytes (16 GB) copied, 168.201 s, 97.4 MB/s</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本篇說明如何安裝 Kraken 版本的 Ceph，並將 objectstore backend 修改成 Bluestore，過程包含建立 RBD 等操作。&lt;/p&gt;
    
    </summary>
    
      <category term="Ceph" scheme="https://kairen.github.io/categories/Ceph/"/>
    
    
      <category term="Ceph" scheme="https://kairen.github.io/tags/Ceph/"/>
    
      <category term="Storage" scheme="https://kairen.github.io/tags/Storage/"/>
    
      <category term="BlueStore" scheme="https://kairen.github.io/tags/BlueStore/"/>
    
  </entry>
  
</feed>
